1
00:00:31,114 --> 00:00:34,659
Why don't you go ahead?  Sit down and see if you can get comfy.
好 你直接开始吧？ ‎好 坐下 能否舒适地坐着

2
00:00:37,579 --> 00:00:39,789
You good?   All right.   Yeah.
还好吗？  好了  ‎是

3
00:00:43,043 --> 00:00:44,794
Take one, marker.
‎第一镜 打板

4
00:00:46,796 --> 00:00:48,798
Wanna start by introducing yourself?
要我先从自我介绍开始吗？

5
00:00:50,467 --> 00:00:53,344
Hello, world. Bailey. Take three.
‎世界你好 贝利 第三镜

6
00:00:53,970 --> 00:00:56,347
You good?    This is the worst part, man.
可以了吗？ ‎  这是我最讨厌的部分 兄弟

7
00:00:56,890 --> 00:00:59,517
I don't like this.
‎我不喜欢这样

8
00:00:59,851 --> 00:01:02,228
I worked at Facebook in 2011 and 2012.
‎我2011年到2012年间在脸书工作

9
00:01:02,312 --> 00:01:05,190
I was one of the really early employees at Instagram.
‎我是Instagram非常早期的员工

10
00:01:05,273 --> 00:01:08,693
I worked at, uh, Google, uh, YouTube.
‎我曾任职于谷歌、YouTube

11
00:01:08,777 --> 00:01:11,696
Apple, Google, Twitter, Palm.
‎苹果、谷歌、推特、Palm

12
00:01:12,739 --> 00:01:15,533
I helped start Mozilla Labs and switched over to the Firefox side.
‎我帮助创始了Mozilla Labs ‎后来更多工作于火狐这边

13
00:01:15,617 --> 00:01:18,119
Are we rolling?  Everybody?
‎在拍吗？大家…

14
00:01:18,203 --> 00:01:19,162
Great.
‎很好

15
00:01:21,206 --> 00:01:22,624
I worked at Twitter.
‎我曾在推特工作

16
00:01:23,041 --> 00:01:23,917
My last job there
我在推特的 ‎

17
00:01:24,000 --> 00:01:26,169
was the senior vice president of engineering.
最后一份岗位 是高级副总工程师

18
00:01:27,337 --> 00:01:29,255
I was the president of Pinterest.
‎我曾是Pinterest的总经理

19
00:01:29,339 --> 00:01:32,717
Before that, um, I was the... the director of monetization
‎在那之前 我在脸书做了五年的

20
00:01:32,801 --> 00:01:34,260
at Facebook for five years.
‎盈利总监

21
00:01:34,344 --> 00:01:37,972
While at Twitter, I spent a number of years running their developer platform,
‎在推特期间 我用了几年时间 ‎运营他们的开发者平台

22
00:01:38,056 --> 00:01:40,225
and then I became head of consumer product.
‎然后我做了消费者产品部长

23
00:01:40,308 --> 00:01:44,270
I was the  coinventor of Google Drive, Gmail Chat,
‎我曾合作发明 ‎谷歌硬盘、谷歌邮箱聊天

24
00:01:44,354 --> 00:01:46,689
Facebook Pages, and the Facebook like button.
‎脸书网页和脸书“赞”按钮

25
00:01:47,440 --> 00:01:50,777
Yeah. This is... This is why I spent, like, eight months
‎对 所以我才花了八个月的时间

26
00:01:50,860 --> 00:01:52,779
talking back and forth with lawyers.
‎和律师反复周旋

27
00:01:54,072 --> 00:01:55,406
This freaks me out.
‎真的让我很崩溃

28
00:01:58,409 --> 00:01:59,702
When I was there,
‎我在那里任职的时候

29
00:01:59,786 --> 00:02:02,914
I always felt like, fundamentally, it was a force for good.
一直觉得 ‎总体上 这是积极的力量

30
00:02:03,414 --> 00:02:05,375
I don't know if I feel that way anymore.
‎我现在不确定 自己是否还这样想了

31
00:02:05,458 --> 00:02:10,588
I left Google in June 2017, uh, due to ethical concerns.
‎出于对道德伦理的担心 ‎我于2017年6月离开谷歌

32
00:02:10,672 --> 00:02:14,134
And... And not just at Google but within the industry at large.
‎不仅是担心谷歌的道德伦理 ‎而是对整个产业

33
00:02:14,217 --> 00:02:15,385
I'm very concerned.
‎我很担心

34
00:02:16,636 --> 00:02:17,679
I'm very concerned.
‎非常担心

35
00:02:19,097 --> 00:02:21,808
It's easy today to lose sight of the fact
‎如今 人们很容易忽略一个事实

36
00:02:21,891 --> 00:02:27,814
that these tools actually have created some wonderful things in the world.
‎这些工具 ‎其实为世界创造了一些美好的东西

37
00:02:27,897 --> 00:02:31,943
They've reunited lost family members. They've found organ donors.
‎它们让失去联系的家人重聚 ‎找器官捐赠者

38
00:02:32,026 --> 00:02:36,573
I mean, there were meaningful, systemic changes happening
‎它们为整个世界带来了

39
00:02:36,656 --> 00:02:39,159
around the world because of these platforms
‎有意义的、系统的变革

40
00:02:39,242 --> 00:02:40,285
that were positive!
‎这是这些平台积极的一面

41
00:02:40,827 --> 00:02:44,539
I think we were naive about the flip side of that coin.
‎我认为我们对它的消极一面 ‎过度看轻了

42
00:02:45,540 --> 00:02:48,585
Yeah, these things, you release them, and they take on a life of their own.
‎对 这些东西 只要你发布 ‎它们自己就会存活

43
00:02:48,668 --> 00:02:52,005
And how they're used is pretty different than how you expected.
‎它们被使用的方式 ‎与你当初的预期大相径庭

44
00:02:52,088 --> 00:02:56,509
Nobody, I deeply believe, ever intended any of these consequences.
‎我深信 这些负面的后果 ‎不是任何人刻意为之

45
00:02:56,593 --> 00:02:59,554
There's no one bad guy. No. Absolutely not.
‎没有任何一个坏人 没有 绝对没有

46
00:03:01,598 --> 00:03:03,975
So, then, what's the... what's the problem?
‎那么问题在哪里？

47
00:03:09,147 --> 00:03:11,482
Is there a problem, and what is the problem?
‎是否有问题？问题在哪里？

48
00:03:17,614 --> 00:03:19,991
Yeah, it is hard to give a single, succinct...
‎是 很难给出一个单一的、简明的…

49
00:03:20,074 --> 00:03:22,118
I'm trying to touch on many different problems.
‎我在试着谈论很多不同的问题

50
00:03:22,535 --> 00:03:23,953
What is the problem?
‎问题在哪里？

51
00:03:28,208 --> 00:03:32,253
‎NETFLIX 原创纪录片

52
00:03:33,463 --> 00:03:35,340
Despite facing mounting criticism,
‎虽然面对越来越多的质疑

53
00:03:35,423 --> 00:03:37,675
the so  called Big Tech names are getting bigger.
‎这些所谓的大型技术公司却越发壮大

54
00:03:37,759 --> 00:03:40,929
The entire tech industry is under a new level of scrutiny.
‎整个技术产业 ‎正在面临全新维度的审查

55
00:03:41,012 --> 00:03:43,806
And a new study sheds light on the link
‎一项新的研究初步揭示了心理健康

56
00:03:43,890 --> 00:03:46,142
between mental health and social media use.
‎与社交媒体使用之间的联系

57
00:03:46,226 --> 00:03:48,686
Here to talk about the latest research...
‎来谈谈最新的研究…

58
00:03:48,770 --> 00:03:51,397
...is going on that gets no coverage at all.
‎…的情况 根本没有保险

59
00:03:51,481 --> 00:03:54,108
Tens of millions of Americans are hopelessly addicted
‎数千万美国人无望地

60
00:03:54,192 --> 00:03:56,319
to their electronic devices.
‎玩电子设备成瘾

61
00:03:56,402 --> 00:03:57,987
It's exacerbated by the fact
‎因为技术 现在你真的能把自己

62
00:03:58,071 --> 00:04:00,698
that you can literally isolate yourself now
‎隔离在一个泡泡内 周围都是

63
00:03:59,447 --> 00:04:00,698
‎（你的朋友在照片中圈出了你）

64
00:04:00,782 --> 00:04:02,742
in a bubble, thanks to our technology.
‎与你观点相似的人 ‎瘾性就更加恶化了

65
00:04:02,825 --> 00:04:04,577
Fake news is becoming more advanced
‎虚假新闻变得更发达了

66
00:04:04,661 --> 00:04:06,788
and threatening societies around the world.
‎威胁着全世界的社会

67
00:04:06,871 --> 00:04:10,250
We weren't expecting any of this when we created Twitter over 12 years ago.
‎我们12年多以前创造推特的时候 ‎根本没有想到这些

68
00:04:10,333 --> 00:04:12,502
White House officials say they have no reason to believe
‎官方说 他们没有理由相信

69
00:04:12,585 --> 00:04:14,754
the Russian cyberattacks will stop.
‎俄罗斯网络攻击会停止

70
00:04:14,837 --> 00:04:18,132
YouTube is being forced to concentrate on cleansing the site.
‎YouTube被迫专注于清理网站

71
00:04:18,216 --> 00:04:21,552
TikTok, if you talk to any tween out there...
‎如果你和十几岁的孩子去聊

72
00:04:21,636 --> 00:04:24,013
...there's no chance they'll delete this thing...
‎他们是绝对不可能删除抖音的…

73
00:04:24,097 --> 00:04:26,224
Hey, Isla, can you get the table ready, please?
‎喂 艾拉 可以帮忙摆桌子吗？

74
00:04:26,307 --> 00:04:28,601
There's a question about whether social media
‎有一个严肃的问题

75
00:04:28,685 --> 00:04:29,978
is making your child depressed.
‎社交媒体是否让您的孩子抑郁

76
00:04:30,061 --> 00:04:32,105
Isla, can you set the table, please?
‎艾拉 可以帮忙摆桌子吗？

77
00:04:32,188 --> 00:04:35,316
These cosmetic procedures are becoming so popular with teens,
‎整容在青少年中已经十分受欢迎

78
00:04:35,400 --> 00:04:37,902
plastic surgeons have coined a new syndrome for it,
‎整容医生甚至造出了一种新病征

79
00:04:37,986 --> 00:04:40,822
"Snapchat dysmorphia," with young patients wanting surgery
‎“图片分享畸形征” ‎指的是年轻的患者想做整容手术

80
00:04:40,905 --> 00:04:43,741
so they can look more like they do in filtered selfies.
‎让自己看上去 ‎更接近加了滤镜的自拍中的样子

81
00:04:43,825 --> 00:04:45,910
Still don't see why you let her have that thing.
‎我还是不明白 ‎你为什么让她用那东西

82
00:04:45,994 --> 00:04:47,412
What was I supposed to do?
‎我能怎么办？

83
00:04:47,495 --> 00:04:49,580
I mean, every other kid in her class had one.
‎我是说 她们班上的其他孩子全都有

84
00:04:50,164 --> 00:04:51,165
She's only 11.
‎她才11岁

85
00:04:51,249 --> 00:04:52,959
Cass, no one's forcing you to get one.
‎卡桑 没人强迫你用

86
00:04:53,042 --> 00:04:55,086
You can stay disconnected as long as you want.
‎你想和别人切断联系多久 都随你

87
00:04:55,169 --> 00:04:59,340
Hey, I'm connected without a cell phone, okay?  I'm on the Internet right now.
‎喂 我没有手机也不会和别人 ‎切断联系 好吗？我现在就在网上

88
00:04:59,424 --> 00:05:03,094
Also, that isn't even actual connection. It's just a load of sh
‎而且 这根本不是实际的联系 ‎全都是没用的…

89
00:05:03,177 --> 00:05:05,013
Surveillance capitalism has come to shape
‎监视资本主义 ‎已经形成了我们的政治和文化

90
00:05:05,096 --> 00:05:07,765
our politics and culture in ways many people don't perceive.
‎而很多人根本没有察觉

91
00:05:07,849 --> 00:05:10,101
ISIS inspired followers online,
‎伊斯兰国煽动网上的关注者

92
00:05:10,184 --> 00:05:12,812
and now white supremacists are doing the same.
‎现在 白人至上主义也在这样做

93
00:05:12,895 --> 00:05:14,147
Recently in India,
‎最近在印度

94
00:05:14,230 --> 00:05:17,442
Internet lynch mobs have killed a dozen people, including these five...
‎网络暴民害死了十几个人 ‎包括这五个…

95
00:05:17,525 --> 00:05:20,361
It's not just fake news; it's fake news with consequences.
‎虚假新闻不只是虚假新闻 ‎是有后果的

96
00:05:20,445 --> 00:05:24,073
How do you handle an epidemic in the age of fake news?
‎在虚假新闻的时代 ‎你要怎么解决传染病？

97
00:05:24,157 --> 00:05:26,993
Can you get the coronavirus by eating Chinese food?
‎吃中餐会感染新冠病毒吗？

98
00:05:27,535 --> 00:05:32,540
We have gone from the information age into the disinformation age.
‎我们已经从信息时代 ‎过渡到了虚假信息时代

99
00:05:32,623 --> 00:05:34,667
Our democracy is under assault.
‎我们的民主受到了攻击

100
00:05:34,751 --> 00:05:36,919
What I said was, "I think the tools
‎我说的是：“我认为如今 ‎

101
00:05:37,003 --> 00:05:39,005
that have been created today are starting
创造出的工具正在开始

102
00:05:39,088 --> 00:05:41,799
to erode the social fabric of how society works."
‎侵蚀社会正常运转的社交纽带”

103
00:06:00,151 --> 00:06:03,446
Aza does welcoming remarks. We play the video.
‎阿莎愿意接受评论 我们播放视频

104
00:06:04,197 --> 00:06:07,325
And then, "Ladies and gentlemen, Tristan Harris."
‎然后说 “女士们先生们 ‎我是特里斯坦·哈里斯”

105
00:06:07,408 --> 00:06:08,868
Right.      Great.
‎  好 ‎  很好

106
00:06:08,951 --> 00:06:12,038
So, I come up, and...
‎就是说 我上来 然后…

107
00:06:12,121 --> 00:06:13,748
‎（人道 技术的新议程）

108
00:06:13,831 --> 00:06:17,126
basically say, "Thank you all for coming." Um...
‎你就说“欢迎各位的到来”

109
00:06:17,919 --> 00:06:22,048
So, today, I wanna talk about a new agenda for technology.
‎今天 我想聊聊技术的一个新议程

110
00:06:22,131 --> 00:06:25,468
And why we wanna do that is because if you ask people,
‎以及我们为什么要这样做 ‎因为如果你问人们

111
00:06:25,551 --> 00:06:27,804
"What's wrong in the tech industry right now? "
‎“如今的技术产业怎么了？”

112
00:06:28,262 --> 00:06:31,641
there's a cacophony of grievances and scandals,
‎有一种不满和丑闻的杂音

113
00:06:31,724 --> 00:06:33,893
and "They stole our data." And there's tech addiction.
‎“他们盗用了我们的数据” ‎还有技术成瘾问题

114
00:06:33,976 --> 00:06:35,978
And there's fake news. And there's polarization
‎有虚假新闻问题 有两极分化问题

115
00:06:36,062 --> 00:06:37,855
and some elections that are getting hacked.
‎有些竞选过程被黑客操控的问题

116
00:06:38,189 --> 00:06:41,609
But is there something that is beneath all these problems
‎但是这些问题的背后 ‎是否有一个原因

117
00:06:41,692 --> 00:06:44,612
that's causing all these things to happen at once?
‎导致这些问题在同时发生？

118
00:06:46,447 --> 00:06:48,408
Does this feel good?    Very good. Yeah.
‎  感觉还行吗？ ‎  非常好 好

119
00:06:50,743 --> 00:06:52,954
I'm just trying to... Like, I want people to see...
‎我只是想… 我想让人们看到…

120
00:06:53,037 --> 00:06:55,123
Like, there's a problem happening in the tech industry,
‎在技术产业 正面临着一个问题

121
00:06:55,206 --> 00:06:56,707
and it doesn't have a name,
‎这个问题连名字都没有 ‎

122
00:06:56,791 --> 00:07:00,211
and it has to do with one source, like, one...
这个问题有一个源头…

123
00:07:05,091 --> 00:07:09,387
When you look around you, it feels like the world is going crazy.
‎环顾你身边 ‎感觉这个世界在逐渐疯狂

124
00:07:12,765 --> 00:07:15,309
You have to ask yourself, like, "Is this normal?
‎你要问自己 这是正常的吗？

125
00:07:16,102 --> 00:07:18,771
Or have we all fallen under some kind of spell? "
‎还是我们都中了什么魔咒？

126
00:07:22,316 --> 00:07:25,153
‎（特里斯坦·哈里斯 ‎谷歌前设计道德伦理学家）

127
00:07:25,236 --> 00:07:27,905
‎（人道技术中心 合作创始人）

128
00:07:27,989 --> 00:07:30,491
I wish more people could understand how this works
‎我希望更多的人能够理解它的原理

129
00:07:30,575 --> 00:07:34,036
because it shouldn't be something that only the tech industry knows.
‎因为它不应该 ‎只被技术产业的业内知道

130
00:07:34,120 --> 00:07:36,247
It should be something that everybody knows.
‎应该让所有人都知道

131
00:07:41,419 --> 00:07:42,378
Bye.
拜

132
00:07:47,383 --> 00:07:48,676
Hello!      Hi.
‎  你好！ ‎  嗨！

133
00:07:48,759 --> 00:07:50,678
Tristan. Nice to meet you.   It's Tris  tan, right?
‎  特里斯坦 幸会 ‎  特里斯坦？

134
00:07:50,761 --> 00:07:51,721
Yes.   Awesome. Cool.
‎  对 ‎  太好了 好

135
00:07:53,181 --> 00:07:55,933
Tristan Harris is a former design ethicist for Google
‎特里斯坦·哈里斯 ‎是谷歌前设计道德伦理学家

136
00:07:56,017 --> 00:07:59,395
and has been called the closest thing Silicon Valley has to a conscience.
‎被称为硅谷最接近良知的人物

137
00:07:59,479 --> 00:08:00,730
He's asking tech
‎他呼吁技术产业

138
00:08:00,813 --> 00:08:04,192
to bring what he calls "ethical design" to its products.
‎在产品中引进 ‎被他称为“道德伦理设计”的要素

139
00:08:04,275 --> 00:08:06,903
It's rare for a tech insider to be so blunt,
‎搞技术的业内人士 ‎极少如此直言不讳

140
00:08:06,986 --> 00:08:10,114
but Tristan Harris believes someone needs to be.
‎特里斯坦·哈里斯相信 ‎总有人要这样

141
00:08:11,324 --> 00:08:12,700
When I was at Google,
‎我在谷歌工作的时候

142
00:08:12,783 --> 00:08:16,037
I was on the Gmail team, and I just started getting burnt out
‎我在谷歌邮箱团队 ‎我就开始觉得很疲惫

143
00:08:16,120 --> 00:08:18,372
'cause we'd had so many conversations about...
‎因为我们讨论了很多…

144
00:08:19,457 --> 00:08:23,169
you know, what the inbox should look like and what color it should be, and...
‎收件箱应该长什么样 ‎应该是什么颜色

145
00:08:23,252 --> 00:08:25,880
And I, you know, felt personally addicted to e  mail,
‎我自己感觉对邮件成瘾

146
00:08:26,297 --> 00:08:27,632
and I found it fascinating
‎我觉得有趣的是

147
00:08:27,715 --> 00:08:31,511
there was no one at Gmail working on making it less addictive.
‎在谷歌邮箱工作的人 ‎没有一个想把它做得不那么致瘾

148
00:08:31,969 --> 00:08:34,514
And I was like, "Is anybody else thinking about this?
‎我想：“别人想过这个问题吗？

149
00:08:34,597 --> 00:08:36,390
I haven't heard anybody talk about this."
‎我 没听谁谈论过这个问题”

150
00:08:36,849 --> 00:08:39,685
And I was feeling this frustration...
‎我对技术产业

151
00:08:39,769 --> 00:08:41,229
...with the tech industry, overall,
整体感到沮丧

152
00:08:41,312 --> 00:08:43,147
that we'd kind of, like, lost our way.
‎感觉我们有点迷路了

153
00:08:46,817 --> 00:08:49,820
You know, I really struggled to try and figure out
‎我真的很努力地去尝试 想办法

154
00:08:49,904 --> 00:08:52,573
how, from the inside, we could change it.
‎怎样能从行业内部改变这个问题

155
00:08:55,201 --> 00:08:58,120
And that was when I decided to make a presentation,
‎就在这个时候 我决定做一次展示

156
00:08:58,204 --> 00:08:59,497
kind of a call to arms.
‎算是号召大家吧

157
00:09:00,998 --> 00:09:04,961
Every day, I went home and I worked on it for a couple hours every single night.
‎每天我回到家 每一个晚上 ‎都要花几个小时去做这件事

158
00:09:06,170 --> 00:09:08,548
It basically just said, you know,
‎我的呼吁是

159
00:09:08,631 --> 00:09:11,884
never before in history have 50 designers
历史上从来没有过50个

160
00:09:12,426 --> 00:09:15,263
20   to 35  year  old white guys in California
‎20到35岁之间的加州白人设计师

161
00:09:15,888 --> 00:09:19,725
made decisions that would have an impact on two billion people.
‎做出一个能影响20亿人的决定

162
00:09:21,018 --> 00:09:24,438
Two billion people will have thoughts that they didn't intend to have
‎20亿人将会拥有 ‎他们从来不曾预料的想法

163
00:09:24,522 --> 00:09:28,401
because a designer at Google said, "This is how notifications work
‎只因为一个谷歌的设计师说 ‎“你每天早上醒来

164
00:09:28,484 --> 00:09:30,778
on that screen that you wake up to in the morning."
‎屏幕上的通知就是这样工作的”

165
00:09:31,195 --> 00:09:35,283
And we have a moral responsibility, as Google, for solving this problem.
‎我们作为谷歌 ‎有解决这个问题的道德责任

166
00:09:36,075 --> 00:09:37,743
And I sent this presentation
‎我把这个展示

167
00:09:37,827 --> 00:09:41,789
to about 15, 20 of my closest colleagues at Google,
‎发给了在谷歌 ‎与我关系最近的15到20个同事

168
00:09:41,872 --> 00:09:44,959
and I was very nervous about it. I wasn't sure how it was gonna land.
‎我很紧张 我不知道他们会怎样想

169
00:09:46,460 --> 00:09:48,045
When I went to work the next day,
‎我第二天去上班的时候

170
00:09:48,129 --> 00:09:50,464
most of the laptops had the presentation open.
‎多数的电脑上都开着这个展示

171
00:09:52,133 --> 00:09:54,552
Later that day, there was, like, 400 simultaneous viewers,
‎那天下午 有400个人同时观看

172
00:09:54,635 --> 00:09:56,053
so it just kept growing and growing.
‎看到的人越来越多

173
00:09:56,137 --> 00:10:00,266
I got e  mails from all around the company. I mean, people in every department saying,
‎我收到整个公司同事发来的各种邮件 ‎每一个部门的人都说

174
00:10:00,349 --> 00:10:02,852
"I totally agree." "I see this affecting my kids."
‎“我太同意了 ‎我看到这个问题正在影响我的孩子

175
00:10:02,935 --> 00:10:04,979
"I see this affecting the people around me."
‎我看到这个问题正在影响我身边的人

176
00:10:05,062 --> 00:10:06,939
"We have to do something about this."
‎我们应该做点什么 来解决这个问题”

177
00:10:07,481 --> 00:10:10,818
It felt like I was sort of launching a revolution or something like that.
‎我感觉自己 ‎好像开启了一场革命之类的

178
00:10:11,861 --> 00:10:15,197
Later, I found out Larry Page had been notified about this presentation
‎后来 我才知道莱利·佩吉 ‎那一天在三个不同会议

179
00:10:15,281 --> 00:10:17,908
in three separate meetings that day.
‎都被人告知这个展示的存在

180
00:10:17,992 --> 00:10:20,286
And so, it created this kind of cultural moment
‎于是这个展示 ‎创造了这个文化性的时刻

181
00:10:20,870 --> 00:10:24,415
that Google needed to take seriously.
‎谷歌需要认真对待

182
00:10:26,000 --> 00:10:28,878
And then... nothing.
‎然后…杳无音讯了

183
00:10:34,300 --> 00:10:36,135
Everyone in 2006...
‎2006年 所有人…

184
00:10:37,219 --> 00:10:39,221
including all of us at Facebook,
‎包括我们在脸书的所有人

185
00:10:39,305 --> 00:10:43,392
just had total admiration for Google and what Google had built,
‎超级羡慕谷歌 ‎羡慕谷歌所创建的一切

186
00:10:43,476 --> 00:10:47,396
which was this incredibly useful service
‎超级实用的服务

187
00:10:47,480 --> 00:10:51,442
that did, far as we could tell, lots of goodness for the world,
‎据我们当时所知 ‎为世界带来了很多好处

188
00:10:51,525 --> 00:10:54,695
and they built this parallel money machine.
‎他们建立了一个平行的造钱机器

189
00:10:55,404 --> 00:11:00,034
We had such envy for that, and it seemed so elegant to us...
‎我们超级羡慕嫉妒谷歌 ‎在我们看来太优雅了…

190
00:11:00,826 --> 00:11:02,161
and so perfect.
‎太完美了

191
00:11:02,953 --> 00:11:05,289
Facebook had been around for about two years,
‎脸书当时才成立大概两年

192
00:11:05,373 --> 00:11:08,376
um, and I was hired to come in and figure out
‎我被雇到脸书

193
00:11:08,459 --> 00:11:10,586
what the business model was gonna be for the company.
‎ 去找出公司未来要走怎样的商业模式

194
00:11:10,670 --> 00:11:13,422
I was the director of monetization. The point was, like,
‎我曾是盈利总监 大概意思就是

195
00:11:13,506 --> 00:11:17,051
"You're the person who's gonna figure out how this thing monetizes."
‎“你是要去想出 ‎这个东西怎样盈利的人”

196
00:11:17,134 --> 00:11:19,804
And there were a lot of people who did a lot of the work,
‎当时很多人做了很多工作

197
00:11:19,887 --> 00:11:25,476
but I was clearly one of the people who was pointing towards...
‎但我明显是其中一个指向…

198
00:11:26,769 --> 00:11:28,562
"Well, we have to make money, A...
‎首先 我们必须要赚钱

199
00:11:29,313 --> 00:11:33,651
and I think this advertising model is probably the most elegant way.
‎我认为这个广告模式 ‎可能是最优雅的方式

200
00:11:42,243 --> 00:11:44,370
Uh  oh. What's this video Mom just sent us?
‎妈妈刚给我们发的什么视频？

201
00:11:44,453 --> 00:11:46,747
Oh, that's from a talk show, but that's pretty good.
‎一个脱口秀 不过挺不错的 ‎

202
00:11:46,831 --> 00:11:47,873
Guy's kind of a genius.
那个人还算是个天才

203
00:11:47,957 --> 00:11:50,584
He's talking all about deleting social media, which you gotta do.
‎他在谈论删除社交媒体 ‎你们真应该这样做

204
00:11:50,668 --> 00:11:52,878
I might have to start blocking her e  mails.
‎我可能要开始屏蔽她的邮件了

205
00:11:52,962 --> 00:11:54,880
I don't even know what she's talking about, man.
‎讲真 我都不知道她在说什么 ‎天啊

206
00:11:54,964 --> 00:11:56,090
She's worse than I am.
她还不如我

207
00:11:56,173 --> 00:11:58,509
No, she only uses it for recipes.   Right, and work.
‎  不 她只用来找菜谱 ‎  对 还有工作

208
00:11:58,592 --> 00:12:00,553
And workout videos.      And to check up on us.
‎  还有健身视频 ‎  还看我们在做什么

209
00:12:00,636 --> 00:12:03,055
And everyone else she's ever met in her entire life.
‎还看她这辈子遇到过的每一个人 ‎在做什么

210
00:12:04,932 --> 00:12:07,893
If you are scrolling through your social media feed
‎如果你一边往下划着 ‎你的社交媒体推送

211
00:12:07,977 --> 00:12:11,731
while you're watchin' us, you need to put the damn phone down and listen up
‎一边看着我们 ‎你需要把你该死的手机放下 听好

212
00:12:11,814 --> 00:12:14,817
'cause our next guest has written an incredible book
‎因为我们的下一位嘉宾 ‎写了一本优秀的书

213
00:12:14,900 --> 00:12:18,112
about how much it's wrecking our lives.
‎书的内容是社交媒体 ‎多大程度上破坏了我们的生活

214
00:12:18,195 --> 00:12:19,447
Please welcome author
‎掌声有请

215
00:12:19,530 --> 00:12:23,951
of Ten Arguments for Deleting Your Social Media Accounts Right Now...
《立刻删除 ‎你社交媒体的十个论点》作者

216
00:12:24,034 --> 00:12:26,287
Uh  huh.   ...Jaron Lanier.
‎贾伦·拉尼尔

217
00:12:27,997 --> 00:12:31,834
Companies like Google and Facebook are some of the wealthiest
‎谷歌、脸书这样的公司是有史以来

218
00:12:31,917 --> 00:12:33,544
and most successful of all time.
‎最富有、最成功的几个公司

219
00:12:33,711 --> 00:12:36,839
Uh, they have relatively few employees.
‎他们的员工数量相对较少

220
00:12:36,922 --> 00:12:41,427
They just have this giant computer that rakes in money, right?  Uh...
‎他们只有一个大电脑 在那里摇钱

221
00:12:41,510 --> 00:12:42,970
Now, what are they being paid for?
‎问题是 别人为什么给他们钱呢？ ‎

222
00:12:43,053 --> 00:12:45,222
That's a really important question.
这是一个非常重要的问题

223
00:12:47,308 --> 00:12:50,311
So, I've been an investor in technology for 35 years.
‎我做了35年的技术产业投资者

224
00:12:51,020 --> 00:12:54,356
The first 50 years of Silicon Valley, the industry made products
‎硅谷的前50年 行业制造产品…

225
00:12:54,440 --> 00:12:55,566
hardware, software
‎硬件、软件

226
00:12:55,649 --> 00:12:58,402
sold 'em to customers. Nice, simple business.
卖给顾客 ‎简单良好的商业模式

227
00:12:58,486 --> 00:13:01,447
For the last ten years, the biggest companies in Silicon Valley
‎过去十年 硅谷最大的公司

228
00:13:01,530 --> 00:13:03,866
have been in the business of selling their users.
‎一直涉足贩卖他们用户的勾当

229
00:13:03,949 --> 00:13:05,910
It's a little even trite to say now,
‎现在这样说 有点陈词滥调

230
00:13:05,993 --> 00:13:09,205
but... because we don't pay for the products that we use,
‎但因为我们不为使用这些产品付钱

231
00:13:09,288 --> 00:13:12,166
advertisers pay for the products that we use.
‎广告商为我们使用的产品付钱

232
00:13:12,249 --> 00:13:14,210
Advertisers are the customers.
‎广告商是顾客

233
00:13:14,710 --> 00:13:16,086
We're the thing being sold.
‎我们是被销售的商品

234
00:13:16,170 --> 00:13:17,630
The classic saying is:
‎经典的说法是

235
00:13:17,713 --> 00:13:21,592
"If you're not paying for the product, then you are the product."
‎“如果你没有花钱买产品 ‎那你就是被卖的产品”

236
00:13:23,385 --> 00:13:27,223
A lot of people think, you know, "Oh, well, Google's just a search box,
‎很多人想：“谷歌只是一个搜索框

237
00:13:27,306 --> 00:13:29,850
and Facebook's just a place to see what my friends are doing
‎脸书只是一个看我朋友们在做什么

238
00:13:29,934 --> 00:13:31,101
and see their photos."
‎看他们照片的地方”

239
00:13:31,185 --> 00:13:35,481
But what they don't realize is they're competing for your attention.
‎但他们没有意识到的是 ‎他们在竞争你的关注

240
00:13:36,524 --> 00:13:41,111
So, you know, Facebook, Snapchat, Twitter, Instagram, YouTube,
‎脸书、阅后即焚图片分享、推特 ‎Instagram、YouTube

241
00:13:41,195 --> 00:13:45,699
companies like this, their business model is to keep people engaged on the screen.
‎这种公司 他们的商业模式 ‎是让人们的注意力持续吸引在屏幕上

242
00:13:46,283 --> 00:13:49,578
Let's figure out how to get as much of this person's attention
‎我们来想办法 怎样尽最大可能

243
00:13:49,662 --> 00:13:50,955
as we possibly can.
‎获得这个人的注意力

244
00:13:51,455 --> 00:13:53,374
How much time can we get you to spend?
‎我们能让你在上面花多少时间？

245
00:13:53,874 --> 00:13:56,669
How much of your life can we get you to give to us?
‎我们能让你给我们分出 ‎你人生的多少时间？

246
00:13:58,629 --> 00:14:01,090
When you think about how some of these companies work,
‎当你去想 这些公司是怎样运作的

247
00:14:01,173 --> 00:14:02,424
it starts to make sense.
‎就能开始想通了

248
00:14:03,050 --> 00:14:06,095
There are all these services on the Internet that we think of as free,
‎网络上有过各种服务 ‎我们都认为是免费的

249
00:14:06,178 --> 00:14:09,473
but they're not free. They're paid for by advertisers.
‎但它们并不是免费的 ‎是广告商在付钱

250
00:14:09,557 --> 00:14:11,559
Why do advertisers pay those companies?
‎广告商为什么给这些公司付钱？

251
00:14:11,642 --> 00:14:14,687
They pay in exchange for showing their ads to us.
‎它们付钱 交换给你展示广告

252
00:14:14,770 --> 00:14:18,357
We're the product. Our attention is the product being sold to advertisers.
‎我们是产品 我们的关注 ‎就是卖给广告商的产品

253
00:14:18,816 --> 00:14:20,442
That's a little too simplistic.
‎这样说 过于简单化了

254
00:14:20,860 --> 00:14:23,654
It's the gradual, slight, imperceptible change
‎产品其实是我们行为和认知的

255
00:14:23,737 --> 00:14:26,574
in your own behavior and perception that is the product.
‎逐渐的、一点一点的 ‎我们未察觉到的变化

256
00:14:26,365 --> 00:14:27,575
‎（行为和认知的 变化）

257
00:14:27,658 --> 00:14:30,244
And that is the product. It's the only possible product.
‎这才是产品 是唯一可能的产品

258
00:14:30,327 --> 00:14:34,081
There's nothing else on the table that could possibly be called the product.
‎这其中 没有任何东西 ‎能再被称为产品了

259
00:14:34,164 --> 00:14:37,001
That's the only thing there is for them to make money from.
‎这是他们能拿来赚钱的唯一东西

260
00:14:37,668 --> 00:14:39,253
Changing what you do,
‎改变你做的事

261
00:14:39,336 --> 00:14:41,714
how you think, who you are.
‎你的思维模式 改变你这个人

262
00:14:42,631 --> 00:14:45,301
It's a gradual change. It's slight.
‎这是一种逐渐的变化 非常轻微

263
00:14:45,384 --> 00:14:48,971
If you can go to somebody and you say, "Give me $10 million,
‎如果你去找一个人 ‎你说：“给我一千万美元

264
00:14:49,054 --> 00:14:54,310
and I will change the world one percent in the direction you want it to change..."
‎我会让世界往你希望的方向改变1%…”

265
00:14:54,852 --> 00:14:58,188
It's the world! That can be incredible, and that's worth a lot of money.
‎是整个世界！这就很神奇 值很多钱

266
00:14:59,315 --> 00:15:00,149
Okay.
‎好

267
00:15:00,691 --> 00:15:04,570
This is what every business has always  dreamt of:
‎这是每种商业都一直梦想的

268
00:15:04,653 --> 00:15:10,910
to have a guarantee that if it places an ad, it will be successful.
‎就是投放一个广告 ‎有一定能够成功的保证

269
00:15:11,327 --> 00:15:12,786
That's their business.
‎这就是他们的生意 ‎

270
00:15:12,870 --> 00:15:14,413
They sell certainty.
他们卖的是确定性

271
00:15:14,079 --> 00:15:14,914
‎（确定性）

272
00:15:14,997 --> 00:15:17,625
In order to be successful in that business,
‎为了在这个生意中成功

273
00:15:17,708 --> 00:15:19,793
you have to have great predictions.
‎你必须要有优秀的预判能力

274
00:15:20,085 --> 00:15:24,173
Great predictions begin with one imperative:
‎（优秀的预判能力）

275
00:15:20,628 --> 00:15:24,173
‎优秀的预判能力始于一个必要条件

276
00:15:25,215 --> 00:15:26,926
you need a lot of data.
‎你需要很多数据

277
00:15:27,009 --> 00:15:29,053
‎（数据）

278
00:15:29,136 --> 00:15:31,305
Many people call this surveillance capitalism,
‎很多人把它称作监视资本主义

279
00:15:31,639 --> 00:15:34,350
capitalism profiting off of the infinite tracking
‎资本主义利用大型技术公司 ‎对每个人去的每一个地方

280
00:15:34,433 --> 00:15:38,062
of everywhere everyone goes by large technology companies
‎进行无限追踪获利

281
00:15:38,145 --> 00:15:40,356
whose business model is to make sure
‎大型技术公司的商业模式

282
00:15:40,439 --> 00:15:42,858
that advertisers are as successful as possible.
‎是保证广告商能尽最大可能成功

283
00:15:42,942 --> 00:15:45,569
This is a new kind of marketplace now.
‎这是现在的一种新市场

284
00:15:45,653 --> 00:15:48,072
It's a marketplace that never existed before.
‎这种市场 以前从未出现过

285
00:15:48,822 --> 00:15:55,371
And it's a marketplace that trades exclusively in human futures.
‎这个市场交易的 只有人类期货

286
00:15:56,080 --> 00:16:01,585
Just like there are markets that trade in pork belly futures or oil futures.
‎就像交易五花肉期货 ‎和石油期货的市场

287
00:16:02,127 --> 00:16:07,591
We now have markets that trade in human futures at scale,
‎我们现在有了 ‎交易大范围人类期货的市场

288
00:16:08,175 --> 00:16:13,472
and those markets have produced the trillions of dollars
‎这些市场创造了万亿美元

289
00:16:14,014 --> 00:16:19,269
that have made the Internet companies the richest companies
‎让网络公司成为了人类历史上

290
00:16:19,353 --> 00:16:22,356
in the history of humanity.
‎最富有的公司

291
00:16:27,361 --> 00:16:30,990
What I want people to know is that everything they're doing online
‎我想让人们知道的是 ‎他们在网上做的一切

292
00:16:31,073 --> 00:16:34,326
is being watched, is being tracked, is being measured.
‎都被监控着 被追踪着 被评估着

293
00:16:35,035 --> 00:16:39,623
Every single action you take is carefully monitored and recorded.
‎你所做出的每一个行为 ‎都被小心翼翼地监控着、记录着

294
00:16:39,707 --> 00:16:43,836
Exactly what image you stop and look at, for how long you look at it.
‎具体到你停在哪一张图片上看了 ‎你看了多久

295
00:16:43,919 --> 00:16:45,796
Oh, yeah, seriously, for how long you look at it.
‎是的 真的 看了多久都记录了

296
00:16:45,879 --> 00:16:47,548
‎（纳维亚 参与时间）

297
00:16:47,631 --> 00:16:49,341
‎（瑞恩 参与时间）

298
00:16:49,425 --> 00:16:50,426
‎（雷恩 参与时间）

299
00:16:50,509 --> 00:16:52,219
They know when people are lonely.
‎人们孤独的时候 他们知道 ‎

300
00:16:52,302 --> 00:16:53,804
They know when people are depressed.
人们抑郁的时候 他们知道

301
00:16:53,887 --> 00:16:57,099
They know when people are looking at photos of your ex  romantic partners.
‎人们看前任爱侣的时候 他们知道

302
00:16:57,182 --> 00:17:00,853
They know what you're doing late at night. They know the entire thing.
‎你深夜在做什么 他们知道 ‎他们全都知道

303
00:17:01,270 --> 00:17:03,230
Whether you're an introvert or an extrovert,
‎你是内向还是外向

304
00:17:03,313 --> 00:17:06,817
or what kind of neuroses you have, what your personality type is like.
‎你的神经哪种类型 ‎你的性格是哪种类型

305
00:17:08,193 --> 00:17:11,613
They have more information about us
‎他们所掌握的我们的信息

306
00:17:11,697 --> 00:17:14,324
than has ever been imagined in human history.
‎超越人类历史上所有的想象

307
00:17:14,950 --> 00:17:16,368
It is unprecedented.
‎这是史无前例的

308
00:17:18,579 --> 00:17:22,791
And so, all of this data that we're... that we're just pouring out all the time
‎所有这些我们不经意间 ‎不断流露出的数据

309
00:17:22,875 --> 00:17:26,754
is being fed into these systems that have almost no human supervision
‎都被输入到这些系统中 ‎几乎不用人类看管

310
00:17:27,463 --> 00:17:30,883
and that are making better and better and better and better predictions
‎会做出越来越好的预判

311
00:17:30,966 --> 00:17:33,552
about what we're gonna do and... and who we are.
‎预判出我们要做什么 ‎我们是怎样的人

312
00:17:34,887 --> 00:17:36,346
‎（为您推荐）

313
00:17:36,305 --> 00:17:39,349
People have the misconception it's our data being sold.
‎很多人有一种误解 认为被卖掉的 ‎是我们的数据

314
00:17:40,350 --> 00:17:43,187
It's not in Facebook's business interest to give up the data.
‎脸书的商业兴趣 ‎肯定不是放掉这些数据

315
00:17:45,522 --> 00:17:47,107
What do they do with that data?
‎他们用这些数据做什么呢？

316
00:17:51,070 --> 00:17:54,490
They build models that predict our actions,
‎他们做出预判我们行为的模型

317
00:17:54,573 --> 00:17:57,618
and whoever has the best model wins.
‎拥有最优秀模型的公司就赢了

318
00:18:02,706 --> 00:18:04,041
His scrolling speed is slowing.
‎他向下滑网页的速度慢

319
00:18:04,124 --> 00:18:06,085
Nearing the end of his average session length.
‎接近他平均阅读一屏时间长度的末尾

320
00:18:06,168 --> 00:18:07,002
Decreasing ad load.
‎减少广告加载

321
00:18:07,086 --> 00:18:08,337
Pull back on friends and family.
‎把朋友和家人弄回来

322
00:18:09,588 --> 00:18:11,340
On the other side of the screen,
‎在屏幕的另一端

323
00:18:11,423 --> 00:18:15,469
it's almost as if they had this avatar voodoo doll  like model of us.
‎他们就好像拥有一个 ‎我们的巫毒娃娃化身一样

324
00:18:16,845 --> 00:18:18,180
All of the things we've ever done,
‎我们做过的所有事情

325
00:18:18,263 --> 00:18:19,473
all the clicks we've ever made,
‎点击过的每一个地方

326
00:18:19,556 --> 00:18:21,642
all the videos we've watched, all the likes,
‎我们看过的所有视频 ‎点赞过的所有内容

327
00:18:21,725 --> 00:18:25,354
that all gets brought back into building a more and more accurate model.
‎这些数据都会被返回去 ‎用来建造一个越来越精准的模型

328
00:18:25,896 --> 00:18:27,481
The model, once you have it,
‎一旦有了这个模型

329
00:18:27,564 --> 00:18:29,858
you can predict the kinds of things that person does.
‎就能预判这个人做怎样的事

330
00:18:29,942 --> 00:18:31,777
Right, let me just test.
‎好 让我测试一下

331
00:18:32,569 --> 00:18:34,988
Where you'll go. I can predict  what kind of videos
‎你将要去哪里  ‎我能预判出 ‎

332
00:18:35,072 --> 00:18:36,115
will keep you watching.
你会继续看什么样的视频

333
00:18:36,198 --> 00:18:39,159
I can predict what kinds of emotions tend to trigger you.
‎我能预判 ‎什么样的情感更能让你产生共鸣

334
00:18:39,243 --> 00:18:40,410
Yes, perfect.
‎好 完美

335
00:18:41,578 --> 00:18:43,372
The most epic fails of the year.
‎年度最悲壮失败

336
00:18:43,747 --> 00:18:44,665
‎（悲壮失败）

337
00:18:48,627 --> 00:18:51,088
Perfect. That worked.   Following with another video.
‎  完美 有效果 ‎  接着另一个视频

338
00:18:51,171 --> 00:18:54,049
Beautiful. Let's squeeze in a sneaker ad before it starts.
‎漂亮 在它开始之前 ‎我们插进去一个运动鞋广告

339
00:18:56,426 --> 00:18:58,178
At a lot of technology companies,
‎很多这种技术公司

340
00:18:58,262 --> 00:18:59,721
there's three main goals.
有三个主要目标

341
00:18:59,805 --> 00:19:01,348
There's the engagement goal:
‎有一个参与度目标 ‎

342
00:19:01,431 --> 00:19:03,684
to drive up your usage, to keep you scrolling.
增加你的使用 让你一直滑动屏幕

343
00:19:04,601 --> 00:19:06,145
There's the growth goal:
‎有一个增长目标

344
00:19:06,228 --> 00:19:08,689
to keep you coming back and inviting as many friends
‎让你不断回来 尽可能多地邀请朋友

345
00:19:08,772 --> 00:19:10,816
and getting them to invite more friends.
‎让他们再邀请更多的朋友

346
00:19:11,650 --> 00:19:13,152
And then there's the advertising goal:
‎还有一个广告目标 ‎

347
00:19:13,235 --> 00:19:14,987
to make sure that, as all that's happening,
确保一切按照预期发展

348
00:19:15,070 --> 00:19:17,406
we're making as much money as possible from advertising.
‎我们尽量多地从广告上挣钱

349
00:19:19,241 --> 00:19:21,994
Each of these goals are powered by algorithms
‎每一个目标都由一个算法驱动

350
00:19:22,077 --> 00:19:24,454
whose job is to figure out what to show you
‎算法的作用是找出 给你展示什么

351
00:19:24,538 --> 00:19:26,165
to keep those numbers going up.
‎让数据上涨

352
00:19:26,623 --> 00:19:29,918
We often talked about, at Facebook, this idea
‎我们在脸书 经常聊到这个想法

353
00:19:30,002 --> 00:19:34,006
of being able to just dial that as needed.
‎能够按照我们的需要调控

354
00:19:34,673 --> 00:19:38,594
And, you know, we talked about having Mark have those dials.
‎我们聊过 让马克来调控

355
00:19:41,305 --> 00:19:44,474
"Hey, I want more users in Korea today."
‎“喂 我今天想让韩国的用户增加”

356
00:19:45,684 --> 00:19:46,602
"Turn the dial."
‎开始调控

357
00:19:47,436 --> 00:19:49,188
"Let's dial up the ads a little bit."
‎“我们调控提高一点广告”

358
00:19:49,980 --> 00:19:51,899
"Dial up monetization, just slightly."
‎“调控提高一点盈利”

359
00:19:52,858 --> 00:19:55,444
And so, that happ
‎所以说…

360
00:19:55,527 --> 00:19:59,239
I mean, at all of these companies, there is that level of precision.
‎所有这些公司 ‎都能做到这种程度的精准

361
00:19:59,990 --> 00:20:02,409
Dude, how       I don't know how I didn't get carded.
‎  兄弟 怎么… ‎  我不知道我怎么没有得到黄牌罚下

362
00:20:02,492 --> 00:20:05,704
That ref just, like, sucked or something.   You got literally all the way...
‎  那个裁判真是逊 ‎  真的是一直…

363
00:20:05,787 --> 00:20:07,956
That's Rebecca. Go talk to her.   I know who it is.
‎  那是瑞贝卡 去和她说话 ‎  我知道那是谁

364
00:20:08,040 --> 00:20:10,834
Dude, yo, go talk to her.      I'm workin' on it.
‎  兄弟 去啊 去跟她说话 ‎  我正在努力

365
00:20:10,918 --> 00:20:14,171
His calendar says he's on a break right now. We should be live.
‎他的日历上说 他正在休假 ‎我们应该实时操作

366
00:20:14,755 --> 00:20:16,465
Want me to nudge him?
‎要我给他发一个窗口抖动吗？

367
00:20:17,132 --> 00:20:18,050
Yeah, nudge away.
‎好 抖吧

368
00:20:21,637 --> 00:20:24,181
"Your friend Tyler just joined. Say hi with a wave."
‎“您的好友泰勒刚刚加入了 ‎去挥手打个招呼吧”

369
00:20:26,016 --> 00:20:27,184
Come on, Ben.
‎快啊 兄弟

370
00:20:27,267 --> 00:20:29,311
Send a wave.
‎发一个挥手

371
00:20:29,394 --> 00:20:32,606
You're not... Go talk to her, dude.
‎你都没有… 去和她说话 兄弟

372
00:20:31,730 --> 00:20:34,900
‎您的好友泰勒刚刚加入了！ ‎挥手打个招呼吧

373
00:20:36,902 --> 00:20:37,986
‎（联系人网络）

374
00:20:38,070 --> 00:20:40,447
New link! All right, we're on.
‎新联系人！好 连上了

375
00:20:40,948 --> 00:20:46,078
Follow that up with a post from User 079044238820, Rebecca.
‎接下来推送 ‎用户079044238820瑞贝卡的发帖

376
00:20:46,161 --> 00:20:49,790
Good idea. GPS coordinates indicate that they're in close proximity.
‎好主意 卫星定位坐标显示 ‎他们距离很近

377
00:20:52,167 --> 00:20:55,837
‎（瑞贝卡 找到了我的灵魂伴侣 ‎#闺蜜#呲溜呲溜#好朋友）

378
00:20:55,921 --> 00:20:57,172
He's primed for an ad.
‎他容易受到广告影响

379
00:20:57,631 --> 00:20:58,632
Auction time.
‎拍卖时间

380
00:20:58,715 --> 00:21:00,050
‎（广告预览 深度衰落发蜡）

381
00:21:00,133 --> 00:21:02,803
Sold! To Deep Fade hair wax.
‎卖了！给深度衰落发蜡

382
00:21:03,387 --> 00:21:07,933
We had 468 interested bidders. We sold Ben at 3.262 cents for an impression.
‎我们有468个感兴趣的竞标人 ‎我们以3.262分卖给本 换来他的印象

383
00:21:17,109 --> 00:21:18,735
We've created a world
‎我们创造了一个世界 ‎

384
00:21:18,819 --> 00:21:21,530
in which online connection has become primary,
这个世界中 在线联系变成了主体

385
00:21:22,072 --> 00:21:23,907
especially for younger generations.
‎尤其是对年轻一代

386
00:21:23,991 --> 00:21:28,328
And yet, in that world, any time two people connect,
‎然而在这个世界 每次两个人联系

387
00:21:29,162 --> 00:21:33,250
the only way it's financed is through a sneaky third person
‎唯一能提供经济支持的 ‎是通过一个鬼祟的第三方

388
00:21:33,333 --> 00:21:35,627
who's paying to manipulate those two people.
‎有人给第三方钱 去操纵这两个人

389
00:21:36,128 --> 00:21:39,381
So, we've created an entire global generation of people
‎所以 我们创造了全球的一整代人

390
00:21:39,464 --> 00:21:44,011
who are raised within a context where the very meaning of communication,
‎他们成长的背景中 交流的意义

391
00:21:44,094 --> 00:21:47,431
the very meaning of culture, is manipulation.
‎文化的意义 就是操纵

392
00:21:47,514 --> 00:21:49,641
We've put deceit and sneakiness
‎我们所做的每一件事的中心

393
00:21:49,725 --> 00:21:52,311
at the absolute center of everything we do.
‎都加入了欺骗和鬼祟

394
00:21:56,356 --> 00:22:01,445
‎（“任何足够先进的技术 ‎都极其类似于魔术”）

395
00:22:01,528 --> 00:22:04,057
‎（——亚瑟·C·克拉克）

396
00:22:05,615 --> 00:22:07,242
Grab the...      Okay.
‎  拿起另一个… ‎  好

397
00:22:07,326 --> 00:22:09,286
Where's it help to hold it?       Great.
‎  放在哪里才有用？ ‎  这样很好

398
00:22:09,369 --> 00:22:10,787
Here?       Yeah.
‎  这里？ ‎  好

399
00:22:10,871 --> 00:22:13,832
How does this come across on camera if I were to do, like, this move
‎这会怎样出现在摄影机中 ‎如果我要做…

400
00:22:13,915 --> 00:22:15,542
We can          Like that?
‎  其实 我们可以… ‎  就这样？

401
00:22:15,625 --> 00:22:16,918
What?    Yeah.
‎  什么？ ‎  对

402
00:22:17,002 --> 00:22:19,004
Do that again.   Exactly. Yeah.
‎  再来一次？ ‎  没错 好

403
00:22:19,087 --> 00:22:20,589
Yeah. No, it's probably not...
‎对 不是 可能不是…

404
00:22:20,672 --> 00:22:21,965
Like... yeah.
‎这样… 对

405
00:22:22,466 --> 00:22:23,884
I mean, this one is less...
‎这个就稍微没那么…

406
00:22:29,681 --> 00:22:33,268
Larissa's, like, actually freaking out over here.
‎克里斯在那边已经烦死了

407
00:22:34,728 --> 00:22:35,562
Is that good?
‎可以了吗？

408
00:22:35,645 --> 00:22:37,773
‎（魔术！）

409
00:22:37,856 --> 00:22:41,068
I was, like, five years old when I learned how to do magic.
‎我从五岁开始学习变魔术

410
00:22:41,151 --> 00:22:45,781
And I could fool adults, fully  grown adults with, like,  PhDs.
‎我可以骗过成年人 ‎有博士学位的完全成熟的成年人

411
00:22:55,040 --> 00:22:57,709
Magicians were almost like the first neuroscientists
‎魔术师几乎像是最早期的神经学家

412
00:22:57,793 --> 00:22:58,960
and psychologists.
‎和心理学家

413
00:22:59,044 --> 00:23:02,005
Like, they were the ones who first understood
‎他们是最先明白

414
00:23:02,089 --> 00:23:03,382
how people's minds work.
‎人们思想工作原理的人

415
00:23:04,216 --> 00:23:07,677
They just, in real time, are testing lots and lots of stuff on people.
‎他们在人们身上 ‎实时测试着很多很多的东西

416
00:23:09,137 --> 00:23:11,139
A magician understands something,
‎魔术师懂一些事

417
00:23:11,223 --> 00:23:14,017
some part of your mind that we're not aware of.
‎你思想中 ‎你自己都没意识到的某一部分

418
00:23:14,101 --> 00:23:15,936
That's what makes the illusion work.
‎这是让幻觉起作用的关键

419
00:23:16,019 --> 00:23:20,607
Doctors, lawyers, people who know how to build 747s or nuclear missiles,
‎医生、律师 ‎知道怎样构造747飞机或者核弹的人

420
00:23:20,690 --> 00:23:24,361
they don't know more about how their own mind is vulnerable.
‎他们并不会比别人更了解 ‎自己的思想有多么脆弱

421
00:23:24,444 --> 00:23:26,113
That's a separate discipline.
‎因为这是一个完全不用的学科领域

422
00:23:26,571 --> 00:23:28,990
And it's a discipline that applies to all human beings.
‎这个学科领域 对所有人类都适用

423
00:23:29,074 --> 00:23:30,909
‎（斯坦福大学）

424
00:23:30,909 --> 00:23:34,079
From that perspective, you can have a very different understanding
‎从这个角度来说 ‎你就能对技术做了什么

425
00:23:34,162 --> 00:23:35,580
of what technology is doing.
‎有一个不同的理解

426
00:23:36,873 --> 00:23:39,584
When I was at the Stanford Persuasive Technology Lab,
‎我在斯坦福劝服技术实验室的时候 ‎

427
00:23:39,668 --> 00:23:41,044
this is what we learned.
这就是我们所学到的

428
00:23:41,628 --> 00:23:43,463
How could you use everything we know
‎怎样利用我们知道的一切心理学知识

429
00:23:43,547 --> 00:23:45,882
about the psychology of what persuades people
‎什么东西能劝服人们

430
00:23:45,966 --> 00:23:48,385
and build that into technology?
‎把这个运用到技术中？

431
00:23:48,468 --> 00:23:50,887
Now, many of you in the audience are geniuses already.
‎在座观众中的很多人 已经是天才了

432
00:23:50,971 --> 00:23:55,851
I think that's true, but my goal is to turn you into a behavior  change genius.
‎我认为这是事实 但我的目标是 ‎让你们变成行为改变的天才

433
00:23:56,852 --> 00:24:01,148
There are many prominent Silicon Valley figures who went through that class
‎很多著名的硅谷人物 ‎都上过这个课

434
00:24:01,231 --> 00:24:05,485
key growth figures at Facebook and Uber and... and other companies
‎脸书、优步和其他公司的 ‎业绩增长关键人物

435
00:24:05,569 --> 00:24:09,197
and learned how to make technology more persuasive,
‎学习怎样让技术更能劝服人们

436
00:24:09,614 --> 00:24:10,782
Tristan being one.
‎特里斯坦就是其中一个

437
00:24:12,284 --> 00:24:14,619
Persuasive technology is just sort of design
‎劝服性技术 可以说是极端应用的

438
00:24:14,703 --> 00:24:16,580
intentionally applied to the extreme,
‎刻意设计

439
00:24:16,663 --> 00:24:18,874
where we really want to modify someone's behavior.
‎我们真的想去修改一个人的行为

440
00:24:18,957 --> 00:24:20,542
We want them to take this action.
‎我们想让他们这样做

441
00:24:20,625 --> 00:24:23,336
We want them to keep doing this with their finger.
‎我们想让他们继续用手指这样做

442
00:24:23,420 --> 00:24:26,256
You pull down and you refresh, it's gonna be a new thing at the top.
‎你往下拉 刷新 最上面就是新的内容

443
00:24:26,339 --> 00:24:28,508
Pull down and refresh again, it's new. Every single time.
‎再下拉 再刷新 又是新的 ‎每一次都是

444
00:24:28,592 --> 00:24:33,722
Which, in psychology, we call a positive intermittent reinforcement.
‎在心理学上 我们称为“正积极强化”

445
00:24:33,805 --> 00:24:37,142
You don't know when you're gonna get it or if you're gonna get something,
‎你不知道什么时候能刷到 ‎或者你是否能刷到什么

446
00:24:37,225 --> 00:24:40,061
which operates just like the slot machines in Vegas.
‎它的原理就像是赌城的老虎机

447
00:24:40,145 --> 00:24:42,230
It's not enough that you use the product consciously,
‎你有意识地使用产品 还远远不够

448
00:24:42,314 --> 00:24:44,024
I wanna dig down deeper into the brain stem
‎我想深度侵入你的大脑根部

449
00:24:44,107 --> 00:24:47,652
and implant, inside of you, an unconscious habit
‎在你脑中植入一个无意识的习惯

450
00:24:47,736 --> 00:24:50,864
so that you are being programmed at a deeper level.
‎让你在更深的层次被编程

451
00:24:50,947 --> 00:24:52,115
You don't even realize it.
‎你自己都没有意识到

452
00:24:52,532 --> 00:24:54,034
A man, James Marshall...
‎一个叫做詹姆斯·马歇尔的人

453
00:24:54,117 --> 00:24:56,286
Every time you see it there on the counter,
‎每一次你在看到老虎机在柜台上

454
00:24:56,369 --> 00:24:59,789
and you just look at it, and you know if you reach over,
‎你看一眼 你知道如果你过去

455
00:24:59,873 --> 00:25:01,333
it just might have something for you,
‎它可能有东西给你

456
00:25:01,416 --> 00:25:03,877
so you play that slot machine to see what you got, right?
‎于是你就玩了一下老虎机 ‎看你能得到什么 对吧？

457
00:25:03,960 --> 00:25:06,046
That's not by accident. That's a design technique.
‎这不是偶然 这是设计好的手段

458
00:25:06,129 --> 00:25:11,301
He brings a golden nugget to an officer in the army in San Francisco.
‎他把一个金块 ‎给了旧金山军队的一个军官

459
00:25:12,219 --> 00:25:15,388
Mind you, the... the population of San Francisco was only...
‎别忘了 旧金山的人口数量 ‎当时只有…

460
00:25:15,472 --> 00:25:17,432
Another example is photo tagging.
‎另一个例子是照片圈人

461
00:25:19,726 --> 00:25:21,186
So, if you get an e  mail
‎如果你收到一封邮件 ‎

462
00:25:21,269 --> 00:25:24,064
that says your friend just tagged you in a photo,
说你朋友刚刚在一张照片中圈出了你

463
00:25:24,147 --> 00:25:28,568
of course you're going to click on that e  mail and look at the photo.
‎你当然会点击那封邮件 看一下照片

464
00:25:29,152 --> 00:25:31,821
It's not something you can just decide to ignore.
‎这不是你能选择忽略的事情

465
00:25:32,364 --> 00:25:34,157
This is deep  seated, like,
‎他们所利用的 ‎

466
00:25:34,241 --> 00:25:36,326
human personality that they're tapping into.
是根植于人类本性中的东西

467
00:25:36,409 --> 00:25:38,078
What you should be asking yourself is:
‎你应该问自己的是

468
00:25:38,161 --> 00:25:40,288
"Why doesn't that e  mail contain the photo in it?
‎“这封邮件中 ‎为什么没把照片放进来？

469
00:25:40,372 --> 00:25:42,457
It would be a lot easier to see the photo."
‎这样 看照片就会容易很多 ”

470
00:25:42,541 --> 00:25:45,919
When Facebook found that feature, they just dialed the hell out of that
‎当脸书发现这个功能之后 ‎他们可真是尽情调控

471
00:25:46,002 --> 00:25:48,505
because they said, "This is gonna be a great way to grow activity.
‎因为他们说 ‎“这将是增长积极性的绝好方式

472
00:25:48,588 --> 00:25:51,091
Let's just get people tagging each other in photos all day long."
‎我们让大家整天在照片中互相圈吧”

473
00:25:56,263 --> 00:25:58,890
‎（本：至少我们中 ‎有一个人拍得很好）

474
00:25:59,349 --> 00:26:00,475
He commented.
‎他评论了

475
00:26:00,559 --> 00:26:01,434
Nice.
‎很好

476
00:26:01,935 --> 00:26:04,688
Okay, Rebecca received it, and she is responding.
‎瑞贝卡收到了 她正在回复

477
00:26:04,771 --> 00:26:07,566
All right, let Ben know that she's typing so we don't lose him.
‎好 让本知道她在输入 别让他下线了

478
00:26:07,649 --> 00:26:08,733
Activating ellipsis.
‎激活省略号

479
00:26:09,234 --> 00:26:13,613
‎（至少我们中有一个人拍得很好 ‎…）

480
00:26:19,953 --> 00:26:21,329
Great, she posted.
‎太好了 她发布了

481
00:26:21,454 --> 00:26:24,249
He's commenting on her comment about his comment on her post.
‎他在评论 他给她发帖的评论的评论

482
00:26:25,041 --> 00:26:26,418
Hold on, he stopped typing.
‎等一下 他停止输入了

483
00:26:26,751 --> 00:26:27,752
Let's autofill.
‎我们来自动填入

484
00:26:28,420 --> 00:26:30,005
Emojis. He loves emojis.
‎表情 他喜欢使用表情

485
00:26:31,381 --> 00:26:33,758
‎（自动完成 参与度）

486
00:26:33,842 --> 00:26:34,676
He went with fire.
‎他选择了“火辣”表情

487
00:26:34,759 --> 00:26:36,803
I was rootin' for eggplant.
‎我以为他会选择茄子呢

488
00:26:38,597 --> 00:26:42,726
There's an entire discipline and field called "growth hacking."
‎有一整个学科 ‎这个领域叫做“增长量黑客学”

489
00:26:42,809 --> 00:26:47,147
Teams of engineers whose job is to hack people's psychology
‎无数工程师团队 ‎他们的工作就是黑入人们的心理

490
00:26:47,230 --> 00:26:48,565
so they can get more growth.
‎让他们拥有更多的增长量

491
00:26:48,648 --> 00:26:50,984
They can get more user sign  ups, more engagement.
‎他们能得到更多的用户注册 ‎更高的参与度

492
00:26:51,067 --> 00:26:52,861
They can get you to invite more people.
‎能让你邀请更多人

493
00:26:52,944 --> 00:26:55,989
After all the testing, all the iterating, all of this stuff,
‎各种测试、迭代 种种操作之后

494
00:26:56,072 --> 00:26:57,907
you know the single biggest thing we realized?
‎知道我们发现最重要现象是什么吗？

495
00:26:57,991 --> 00:27:00,702
Get any individual to seven friends in ten days.
‎让任何个体在十天内邀请七个朋友

496
00:26:59,909 --> 00:27:01,870
‎（查马斯·帕里哈皮提亚 ‎脸书前增长副总裁）

497
00:27:01,953 --> 00:27:02,787
That was it.
‎就是这个

498
00:27:02,871 --> 00:27:05,498
Chamath was the head of growth at Facebook early on,
‎查马斯是脸书早期的增长负责人

499
00:27:05,582 --> 00:27:08,251
and he's very well known in the tech industry
‎他在技术领域非常知名

500
00:27:08,335 --> 00:27:11,004
for pioneering a lot of the growth tactics
‎因为他首创了很多增长手段

501
00:27:11,087 --> 00:27:14,758
that were used to grow Facebook at incredible speed.
‎这些手段的使用 ‎让脸书用户飞速增长

502
00:27:14,841 --> 00:27:18,553
And those growth tactics have then become the standard playbook for Silicon Valley.
‎后来那些增长手段 ‎变成了硅谷的标准战术

503
00:27:18,637 --> 00:27:21,222
They were used at Uber and at a bunch of other companies.
‎在优步使用了 ‎在很多其他公司也使用了

504
00:27:21,306 --> 00:27:27,062
One of the things that he pioneered was the use of scientific A/B testing
‎他首创的一个东西 ‎就是对功能上的小变化

505
00:27:27,145 --> 00:27:28,480
of small feature changes.
‎使用科学A/B测试

506
00:27:29,022 --> 00:27:30,940
Companies like Google and Facebook
‎谷歌和脸书这种公司

507
00:27:31,024 --> 00:27:34,569
would roll out lots of little, tiny experiments
‎会推出很多小实验

508
00:27:34,653 --> 00:27:36,821
that they were constantly doing on users.
‎他们不断在用户身上测试

509
00:27:36,905 --> 00:27:39,866
And over time, by running these constant experiments,
‎随着时间发展 ‎不停运行这些实验之后

510
00:27:39,949 --> 00:27:43,036
you... you develop the most optimal way
‎就能发展出能让用户 ‎做你想让他们做的事

511
00:27:43,119 --> 00:27:45,288
to get users to do what you want them to do.
‎的最佳方式

512
00:27:45,372 --> 00:27:46,790
It's... It's manipulation.
‎这就是操纵

513
00:27:47,332 --> 00:27:49,459
Uh, you're making me feel like a lab rat.
‎你让我感觉自己像是实验室的小白鼠

514
00:27:49,834 --> 00:27:51,920
You are a lab rat. We're all lab rats.
‎你就是实验室的小白鼠 ‎我们所有人都是

515
00:27:52,545 --> 00:27:55,548
And it's not like we're lab rats for developing a cure for cancer.
‎我们和开发治愈癌症药物的 ‎实验室小白鼠又不一样

516
00:27:55,632 --> 00:27:58,134
It's not like they're trying to benefit us.
‎这些实验的最终获益人 不是我们

517
00:27:58,218 --> 00:28:01,680
Right?  We're just zombies, and they want us to look at more ads
‎对吧？我们就像僵尸一样 ‎他们想让我们看更多的广告

518
00:28:01,763 --> 00:28:03,181
so they can make more  money.
‎让他们挣更多的钱

519
00:28:03,556 --> 00:28:05,266
Facebook conducted
‎脸书做了一个实验 ‎

520
00:28:05,350 --> 00:28:08,228
what they called "massive  scale contagion experiments."
他们称为“海量规模蔓延实验”

521
00:28:08,311 --> 00:28:09,145
Okay.
‎好吧

522
00:28:09,229 --> 00:28:13,066
How do we use subliminal cues on the Facebook pages
‎我们怎样用脸书页面上的潜意识信号

523
00:28:13,400 --> 00:28:17,654
to get more people to go vote in the midterm elections?
‎来让更多人在中期选举中投票？

524
00:28:17,987 --> 00:28:20,824
And they discovered that they were able to do that.
‎他们发现 他们能做到

525
00:28:20,907 --> 00:28:24,160
One thing they concluded is that we now know
‎他们得出的一个结论是 ‎现在我们知道

526
00:28:24,744 --> 00:28:28,915
we can affect real  world behavior and emotions
‎我们能影响现实世界中的行为和情感

527
00:28:28,998 --> 00:28:32,877
without ever triggering the user's awareness.
‎而根本不用触发用户的意识

528
00:28:33,378 --> 00:28:37,382
They are completely clueless.
‎他们自己完全不知道

529
00:28:38,049 --> 00:28:41,970
We're pointing these engines of AI back at ourselves
‎我们将这些人工智能引擎 ‎指回到我们身上

530
00:28:42,053 --> 00:28:46,224
to reverse  engineer what elicits responses from us.
‎来反向编程 什么能引诱我们的回应

531
00:28:47,100 --> 00:28:49,561
Almost like you're stimulating nerve cells on a spider
‎就很像你在蜘蛛身上模拟神经细胞

532
00:28:49,644 --> 00:28:51,479
to see what causes its legs to respond.
‎来看是什么引起它的腿反应

533
00:28:51,938 --> 00:28:53,940
So, it really is this kind of prison experiment
‎就是这种监狱实验

534
00:28:54,023 --> 00:28:56,735
where we're just, you know, roping people into the matrix,
‎我们在实验中 捆绑人们进入矩阵

535
00:28:56,818 --> 00:29:00,572
and we're just harvesting all this money and... and data from all their activity
‎我们从他们的行为中获取金钱和数据 ‎

536
00:29:00,655 --> 00:29:01,489
to profit from.
用他们的行为牟利

537
00:29:01,573 --> 00:29:03,450
And we're not even aware that it's happening.
‎我们甚至都不知道 发生了这些

538
00:29:04,117 --> 00:29:07,912
So, we want to psychologically figure out how to manipulate you as fast as possible
‎我们想在心理学上弄清楚 ‎怎样以最快的速度操纵你

539
00:29:07,996 --> 00:29:10,081
and then give you back that dopamine hit.
‎然后返回给你让你产生兴奋的事物

540
00:29:10,165 --> 00:29:12,375
We did that brilliantly at Facebook.
‎我们在脸书做得非常出色

541
00:29:12,625 --> 00:29:14,919
Instagram has done it. WhatsApp has done it.
‎Instagram也这样做了 ‎WhatsApp也这样做了

542
00:29:15,003 --> 00:29:17,380
You know, Snapchat has done it. Twitter has done it.
‎阅后即焚图片分享也这样做了 ‎推特也这样做了

543
00:29:17,464 --> 00:29:19,424
I mean, it's exactly the kind of thing
‎这种东西正是

544
00:29:19,507 --> 00:29:22,427
that a... that a hacker like myself would come up with
‎我这种黑客能想出来的

545
00:29:22,510 --> 00:29:27,015
because you're exploiting a vulnerability in... in human psychology.
‎因为你在利用人类心理中的脆弱挣钱

546
00:29:27,098 --> 00:29:28,725
‎（希恩·帕克 脸书前总经理）

547
00:29:27,807 --> 00:29:29,726
And I just... I think that we...
‎我只是想

548
00:29:29,809 --> 00:29:33,438
you know, the inventors, creators...
我们这些发明者、创造者…

549
00:29:33,980 --> 00:29:37,317
uh, you know, and it's me, it's Mark, it's the...
‎有我 有马克 有…

550
00:29:37,400 --> 00:29:40,403
you know, Kevin Systrom at Instagram... It's all of these people...
‎Instagram的凯文·斯特罗姆 ‎所有这些人…

551
00:29:40,487 --> 00:29:46,451
um, understood this consciously, and we did it anyway.
‎意识里非常清楚 但我们依然利用了

552
00:29:50,580 --> 00:29:53,750
No one got upset when bicycles showed up.
‎自行车问世的时候 没有人不满

553
00:29:55,043 --> 00:29:58,004
Right?  Like, if everyone's starting to go around on bicycles,
‎对吧？所有人都开始用自行车出行

554
00:29:58,087 --> 00:30:00,924
no one said, "Oh, my God, we've just ruined society.
‎没有人说 ‎“天啊 我们刚刚毁掉了社会

555
00:30:01,007 --> 00:30:03,051
Like, bicycles are affecting people.
‎因为自行车能影响人

556
00:30:03,134 --> 00:30:05,303
They're pulling people away from their kids.
‎拉远了他们和孩子间的距离

557
00:30:05,386 --> 00:30:08,723
They're ruining the fabric of democracy. People can't tell what's true."
‎他们在毁掉民主的结构 ‎人们无法判断真假了”

558
00:30:08,807 --> 00:30:11,476
Like, we never said any of that stuff about a bicycle.
‎对于自行车 ‎我们从来没有说过这种话

559
00:30:12,769 --> 00:30:16,147
If something is a tool, it genuinely is just sitting there,
‎如果一个东西是工具 ‎它就会忠诚地坐在那里

560
00:30:16,731 --> 00:30:18,733
waiting patiently.
‎耐心等待

561
00:30:19,317 --> 00:30:22,821
If something is not a tool, it's demanding things from you.
‎如果一个东西不是工具 ‎它会在你身上有所求

562
00:30:22,904 --> 00:30:26,533
It's seducing you. It's manipulating you. It wants things from you.
‎引诱你、操纵你 想从你身上获利

563
00:30:26,950 --> 00:30:30,495
And we've moved away from having a tools  based technology environment
‎我们已经走过了 ‎以工具为基础的技术环境

564
00:30:31,037 --> 00:30:34,499
to an addiction   and manipulation  based technology environment.
‎来到了以致瘾和操纵 ‎为基础的技术环境

565
00:30:34,582 --> 00:30:35,708
That's what's changed.
‎这是技术环境的改变

566
00:30:35,792 --> 00:30:39,420
Social media isn't a tool that's just waiting to be used.
‎社交媒体 ‎不是原地等在那里被使用的工具

567
00:30:39,504 --> 00:30:43,466
It has its own goals, and it has its own means of pursuing them
‎它有自己的目标 ‎有自己的办法去实现这些目标

568
00:30:43,550 --> 00:30:45,677
by using your psychology against you.
‎利用你的心理 来对付你

569
00:30:49,055 --> 00:30:52,517
‎（“只有两个行业 ‎把他们的客户叫做‘使用者’

570
00:30:52,600 --> 00:30:55,562
‎非法毒品和软件”）

571
00:30:55,645 --> 00:30:57,480
‎（——爱德华·塔夫特）

572
00:30:57,564 --> 00:31:00,567
Rewind a few years ago, I was the...
‎回想几年之前

573
00:31:00,650 --> 00:31:02,318
I was the president of Pinterest.
‎我是Pinterest的总经理

574
00:31:03,152 --> 00:31:05,113
I was coming home,
‎我回到家

575
00:31:05,196 --> 00:31:08,366
and I couldn't get off my phone once I got home,
到家之后就无法放下手机

576
00:31:08,449 --> 00:31:12,161
despite having two young kids who needed my love and attention.
‎虽然我有两个小孩子 需要我的关爱

577
00:31:12,245 --> 00:31:15,748
I was in the pantry, you know, typing away on an e  mail
‎我在食物储藏室里打字回邮件

578
00:31:15,832 --> 00:31:17,542
or sometimes looking at Pinterest.
‎有时候会看Pinterest

579
00:31:18,001 --> 00:31:19,627
I thought, "God, this is classic irony.
‎我想：“天啊 这真是典型的讽刺

580
00:31:19,711 --> 00:31:22,046
I am going to work during the day
‎我白天去工作

581
00:31:22,130 --> 00:31:26,426
and building something that then I am falling prey to."
‎构造一个把我当猎物的东西”

582
00:31:26,509 --> 00:31:30,096
And I couldn't... I mean, some of those moments, I couldn't help myself.
‎我无法… 有时候 ‎我真的情不自禁使用

583
00:31:32,307 --> 00:31:36,102
The one that I'm... I'm most prone to is Twitter.
‎我最无法摆脱的是推特

584
00:31:36,185 --> 00:31:38,021
Uh, used to be Reddit.
‎以前无法摆脱的是Reddit

585
00:31:38,104 --> 00:31:42,859
I actually had to write myself software to break my addiction to reading Reddit.
‎我后来不得不给自己写程序 ‎来切断我阅读Reddit的瘾

586
00:31:45,403 --> 00:31:47,780
I'm probably most addicted to my e  mail.
‎我最成瘾的 可能是邮件

587
00:31:47,864 --> 00:31:49,866
I mean, really. I mean, I... I feel it.
‎真的 我是认真的 我自己能感觉到

588
00:31:52,577 --> 00:31:54,954
Well, I mean, it's sort     it's interesting
‎这很有趣

589
00:31:55,038 --> 00:31:58,166
that knowing what was going on behind the curtain,
‎我很清楚 这一切的幕后发生着什么

590
00:31:58,249 --> 00:32:01,628
I still wasn't able to control my usage.
‎我还是无法控制自己去使用

591
00:32:01,711 --> 00:32:03,046
So, that's a little scary.
‎这就有点可怕了

592
00:32:03,630 --> 00:32:07,050
Even knowing how these tricks work, I'm still susceptible to them.
‎即便知道这些手段的原理 ‎我还是容易受到它们影响

593
00:32:07,133 --> 00:32:09,886
I'll still pick up the phone, and 20 minutes will disappear.
‎我拿起手机 ‎20分钟就不知不觉过去了

594
00:32:12,805 --> 00:32:15,725
Do you check your smartphone before you pee in the morning
‎你晨起排尿之前 ‎会看一眼智能手机吗？

595
00:32:15,808 --> 00:32:17,477
or while you're peeing in the morning?
‎或者晨起排尿过程中 会看吗？

596
00:32:17,560 --> 00:32:19,479
'Cause those are the only two choices.
‎因为只有这两个选择

597
00:32:19,562 --> 00:32:23,274
I tried through willpower, just pure willpower...
‎我试过用意志力克制 纯意志力…

598
00:32:23,358 --> 00:32:26,903
"I'll put down my phone, I'll leave my phone in the car when I get home."
‎“我要放下手机 ‎我到家之后 要把手机丢在车里”

599
00:32:26,986 --> 00:32:30,573
I think I told myself a thousand times, a thousand different days,
‎我应该在千万个不同的日子 ‎告诉过自己千万次

600
00:32:30,657 --> 00:32:32,617
"I am not gonna bring my phone to the bedroom,"
‎“我不要把手机带到卧室”

601
00:32:32,700 --> 00:32:34,535
and then 9:00 p.m. rolls around.
‎然后晚上九点到了

602
00:32:34,619 --> 00:32:37,121
"Well, I wanna bring my phone in the bedroom."
‎“哎 我想把手机带进卧室”

603
00:32:37,205 --> 00:32:39,290
And so, that was sort of...
‎这就有点…

604
00:32:39,374 --> 00:32:41,125
Willpower was kind of attempt one,
‎意志力是一种努力

605
00:32:41,209 --> 00:32:44,295
and then attempt two was, you know, brute force.
‎兽性在做着另一种努力

606
00:32:44,379 --> 00:32:48,091
Introducing the Kitchen Safe. The Kitchen Safe is a revolutionary,
‎隆重介绍“厨房保险箱”  ‎“厨房保险箱”

607
00:32:48,174 --> 00:32:51,678
new, time  locking container that helps you fight temptation.
是革命性的全新发明 ‎帮你战胜诱惑的 ‎时间锁保鲜盒

608
00:32:51,761 --> 00:32:56,724
All David has to do is place those temptations in the Kitchen Safe.
‎大卫只需要把各种诱惑 ‎放进这个“厨房保险箱”

609
00:32:57,392 --> 00:33:00,395
Next, he rotates the dial to set the timer.
‎下一步 进行调控 设置时间

610
00:33:01,479 --> 00:33:04,232
And, finally, he presses the dial to activate the lock.
‎最后 按下调控 激活锁

611
00:33:04,315 --> 00:33:05,525
The Kitchen Safe is great...
‎“厨房保险箱”超级好

612
00:33:05,608 --> 00:33:06,776
We have that, don't we?
‎我们家有 是吧？

613
00:33:06,859 --> 00:33:08,653
...video games, credit cards, and cell phones.
‎…电子游戏、信用卡、手机

614
00:33:08,736 --> 00:33:09,654
Yeah, we do.
‎对 有

615
00:33:09,737 --> 00:33:12,407
Once the Kitchen Safe is locked, it cannot be opened
‎“厨房保险箱”一旦上锁 ‎直到计时器归零之前

616
00:33:12,490 --> 00:33:13,866
until the timer reaches zero.
没办法打开

617
00:33:13,950 --> 00:33:15,618
So, here's the thing.
‎问题是

618
00:33:15,702 --> 00:33:17,537
Social media is a drug.
社交媒体就是一种毒品

619
00:33:17,620 --> 00:33:20,873
I mean, we have a basic biological imperative
‎我们有着基本的生物学欲望

620
00:33:20,957 --> 00:33:23,084
to connect with other people.
‎去和别人联系

621
00:33:23,167 --> 00:33:28,214
That directly affects the release of dopamine in the reward pathway.
‎这直接影响着 ‎奖赏通路中的多巴胺释放

622
00:33:28,297 --> 00:33:32,552
Millions of years of evolution, um, are behind that system
‎这个机制背后 是几百万年的进化

623
00:33:32,635 --> 00:33:35,596
to get us to come together and live in communities,
‎让我们聚在一起 群居生活

624
00:33:35,680 --> 00:33:38,016
to find mates, to propagate our species.
‎找到伴侣 繁殖我们的物种

625
00:33:38,099 --> 00:33:41,853
So, there's no doubt that a vehicle like social media,
‎所以 毫无疑问 社交媒体这种载体

626
00:33:41,936 --> 00:33:45,690
which optimizes this connection between people,
‎它会优化人们之间的联系

627
00:33:45,773 --> 00:33:48,568
is going to have the potential for addiction.
‎自然会有致瘾的可能性

628
00:33:52,071 --> 00:33:54,115
Mmm!      Dad, stop!
‎爸 停！

629
00:33:55,450 --> 00:33:58,453
I have, like, 1,000 more snips to send before dinner.
‎我在晚饭前 还有上千个消息要回

630
00:33:58,536 --> 00:34:00,788
Snips?    I don't know what a snip is.
‎消息？

631
00:33:59,620 --> 00:34:01,080
‎我不知道消息是什么

632
00:34:00,872 --> 00:34:03,207
Mm, that smells good, baby.   All right. Thank you.
‎  闻起来好香 宝贝 ‎  谢谢

633
00:34:03,291 --> 00:34:05,877
I was, um, thinking we could use all five senses
‎我想 我们可以用所有五官

634
00:34:05,960 --> 00:34:07,712
to enjoy our dinner tonight.
‎来享受今晚的晚餐

635
00:34:07,795 --> 00:34:11,382
So, I decided that we're not gonna have any cell phones at the table tonight.
‎所以我决定 ‎今晚的餐桌上不能使用手机

636
00:34:11,466 --> 00:34:13,301
So, turn 'em in.
‎好了 交上来

637
00:34:13,801 --> 00:34:14,802
Really?       Yep.
‎  真的吗？ ‎  是

638
00:34:15,928 --> 00:34:18,056
All right.   Thank you. Ben?
‎  好吧 ‎  谢谢 本？

639
00:34:18,139 --> 00:34:20,433
Okay.   Mom, the phone pirate.
‎  好 ‎  妈妈是手机海盗

640
00:34:21,100 --> 00:34:21,934
Got it.   Mom!
‎  拿走了 ‎  妈！

641
00:34:22,518 --> 00:34:26,147
So, they will be safe in here until after dinner...
‎晚餐结束前 ‎这些手机会安全地放在这里

642
00:34:27,273 --> 00:34:30,651
and everyone can just chill out.
‎所有人可以安静待着了

643
00:34:30,735 --> 00:34:31,569
Okay?
好吗？

644
00:34:47,418 --> 00:34:49,253
Can I just see who it is?    No.
‎  我能看一眼是谁吗？ ‎  不行

645
00:34:54,759 --> 00:34:56,969
Just gonna go get another fork.
‎我去再拿一个叉子

646
00:34:58,304 --> 00:34:59,263
Thank you.
‎谢谢

647
00:35:04,727 --> 00:35:06,771
Honey, you can't open that.
‎宝贝 不能打开

648
00:35:06,854 --> 00:35:09,315
I locked it for an hour, so just leave it alone.
‎我锁上了一个小时 别动了

649
00:35:11,192 --> 00:35:13,361
So, what should we talk about?
‎我们要聊点什么？

650
00:35:13,444 --> 00:35:14,695
Well, we could talk
‎我们可以聊聊

651
00:35:14,779 --> 00:35:17,615
about the, uh, Extreme Center wackos I drove by today.
我今天开车 ‎身边经过的极端中心政党疯子

652
00:35:17,698 --> 00:35:18,825
Please, Frank.   What?
‎  算了 弗兰科 ‎  怎么了？

653
00:35:18,908 --> 00:35:20,785
I don't wanna talk about politics.
‎我不想聊政治

654
00:35:20,868 --> 00:35:23,538
What's wrong with the Extreme Center?    See?  He doesn't even get it.
‎  极端中心怎么了？ ‎  看吧？他都没明白

655
00:35:23,621 --> 00:35:24,622
It depends on who you ask.
‎取决于你问谁

656
00:35:24,705 --> 00:35:26,624
It's like asking, "What's wrong with propaganda? "
‎这就像是你问：“政治鼓吹怎么了？”

657
00:35:28,709 --> 00:35:29,710
Isla!
‎艾拉！

658
00:35:32,797 --> 00:35:33,756
Oh, my God.
‎天啊

659
00:35:36,425 --> 00:35:38,553
Do you want me to...      Yeah.
‎  需要我去… ‎  嗯

660
00:35:41,973 --> 00:35:43,933
I... I'm worried about my kids.
‎我很担心我的孩子们

661
00:35:44,016 --> 00:35:46,686
And if you have kids, I'm worried about your kids.
‎等你们有了孩子 ‎我还会担心你们的孩子

662
00:35:46,769 --> 00:35:50,189
Armed with all the knowledge that I have and all of the experience,
‎虽然我有着各种知识储备 各种经验

663
00:35:50,273 --> 00:35:52,108
I am fighting my kids about the time
‎我还是和孩子们争论

664
00:35:52,191 --> 00:35:54,443
that they spend on phones and on the computer.
‎他们使用手机和电脑的时间

665
00:35:54,527 --> 00:35:58,197
I will say to my son, "How many hours do you think you're spending on your phone? "
‎我会对我儿子说 ‎“你觉得自己会在手机上花多久？”

666
00:35:58,281 --> 00:36:01,075
He'll be like, "It's, like, half an hour. It's half an hour, tops."
‎他会说 ‎“也就半小时吧 最多半小时了”

667
00:36:01,159 --> 00:36:04,829
I'd say upwards hour, hour and a half.
‎我觉得比一小时多一点 一个半小时

668
00:36:04,912 --> 00:36:06,789
I looked at his screen report a couple weeks ago.
‎几周之前看了他的屏幕使用报告

669
00:36:06,873 --> 00:36:08,708
Three hours and 45 minutes.      That...
‎  是3小时45分 ‎  那不是…

670
00:36:11,377 --> 00:36:13,588
I don't think that's... No. Per day, on average?
‎我觉得没有… 平均每天？

671
00:36:13,671 --> 00:36:15,506
Yeah.   Should I go get it right now?
‎  对 ‎  我现在去拿来吗？

672
00:36:15,590 --> 00:36:19,177
There's not a day that goes by that I don't remind my kids
‎每一天 我都要提醒我的孩子们

673
00:36:19,260 --> 00:36:21,762
about the pleasure  pain balance,
‎愉悦和痛苦的平衡

674
00:36:21,846 --> 00:36:24,390
about dopamine deficit states,
‎多巴胺短缺的状态

675
00:36:24,473 --> 00:36:26,267
about the risk of addiction.
上瘾的风险

676
00:36:26,350 --> 00:36:27,310
Moment of truth.
‎  对 ‎  来揭晓真相

677
00:36:27,935 --> 00:36:29,687
Two hours, 50 minutes per day.
‎每天2小时50分

678
00:36:29,770 --> 00:36:31,772
Let's see.   Actually, I've been using a lot today.
‎  我们看看 ‎  其实 我今天用了很多

679
00:36:31,856 --> 00:36:33,357
Last seven days.   That's probably why.
‎  过去七天 ‎  可能这就是原因

680
00:36:33,441 --> 00:36:37,361
Instagram, six hours, 13 minutes. Okay, so my Instagram's worse.
‎Instagram 6小时13分 ‎好吧 我使用Instagram是最严重的

681
00:36:39,572 --> 00:36:41,991
My screen's completely shattered.
‎我的屏幕彻底碎了

682
00:36:42,200 --> 00:36:43,201
Thanks, Cass.
谢谢你 卡桑

683
00:36:44,410 --> 00:36:45,995
What do you mean, "Thanks, Cass"?
‎“谢谢你 卡桑”是什么意思？

684
00:36:46,078 --> 00:36:49,040
You keep freaking Mom out about our phones when it's not really a problem.
‎你一直让妈妈担心我们的手机问题 ‎但其实这根本不是问题

685
00:36:49,373 --> 00:36:51,167
We don't need our phones to eat dinner!
‎我们吃晚餐不需要手机

686
00:36:51,250 --> 00:36:53,878
I get what you're saying. It's just not that big a deal. It's not.
‎我明白你说的 ‎但这又不是什么大事 没什么啊

687
00:36:56,047 --> 00:36:58,382
If it's not that big a deal, don't use it for a week.
‎不是什么大事 那就一周别用手机

688
00:37:01,135 --> 00:37:06,349
Yeah. Yeah, actually, if you can put that thing away for, like, a whole week...
‎对 ‎对 其实 如果你能把那东西 ‎收起来一整周…

689
00:37:07,725 --> 00:37:09,518
I will buy you a new screen.
‎我就给你买一个新的屏幕

690
00:37:10,978 --> 00:37:12,897
Like, starting now?       Starting now.
‎  从现在开始吗？ ‎  现在开始

691
00:37:15,149 --> 00:37:16,859
Okay. You got a deal.      Okay.
‎好 成交

692
00:37:16,943 --> 00:37:19,111
Okay, you gotta leave it here, though, buddy.
‎好 不过你要放在这里 小朋友

693
00:37:19,862 --> 00:37:21,364
All right, I'm plugging it in.
‎好 我把它放进去

694
00:37:22,531 --> 00:37:25,076
Let the record show... I'm backing away.
‎计时开始 我退后了

695
00:37:25,159 --> 00:37:25,993
Okay.
‎好

696
00:37:27,787 --> 00:37:29,413
You're on the clock.      One week.
‎  计时开始了 ‎  一周

697
00:37:29,497 --> 00:37:30,331
Oh, my...
‎天啊…

698
00:37:31,457 --> 00:37:32,416
Think he can do it?
‎你觉得他能做到吗？

699
00:37:33,000 --> 00:37:34,252
I don't know. We'll see.
‎不知道 走着瞧

700
00:37:35,002 --> 00:37:36,128
Just eat, okay?
‎你吃饭吧 好吗？

701
00:37:44,220 --> 00:37:45,263
Good family dinner!
‎美好的家庭晚餐！

702
00:37:47,682 --> 00:37:49,809
These technology products were not designed
‎这些技术产品不是由

703
00:37:49,892 --> 00:37:53,896
by child psychologists who are trying to protect and nurture children.
‎努力保护和培育孩子的 ‎儿童心理学家设计的

704
00:37:53,980 --> 00:37:56,148
They were just designing to make these algorithms
‎它们的设计 是让这些算法

705
00:37:56,232 --> 00:37:58,734
that were really good at recommending the next video to you
‎非常擅于给你推荐下一个视频

706
00:37:58,818 --> 00:38:02,321
or really good at getting you to take a photo with a filter on it.
‎非常擅于让你拍照加滤镜

707
00:38:03,072 --> 00:38:05,324
‎（两个赞）

708
00:38:13,291 --> 00:38:15,126
‎（确定删除吗？否）

709
00:38:15,209 --> 00:38:16,210
‎（是）

710
00:38:16,752 --> 00:38:18,879
It's not just that it's controlling
‎这些东西不仅在控制 ‎

711
00:38:18,963 --> 00:38:20,548
where they spend their attention.
他们把注意力花在哪里

712
00:38:21,173 --> 00:38:26,304
Especially social media starts to dig deeper and deeper down into the brain stem
‎尤其是社交媒体越来越深入大脑根部

713
00:38:26,387 --> 00:38:29,765
and take over kids' sense of self  worth and identity.
‎夺走孩子们的判断力 ‎自我价值和身份

714
00:38:29,849 --> 00:38:31,851
‎（美化我）

715
00:38:42,069 --> 00:38:43,112
‎（莉莉：可爱！）

716
00:38:43,195 --> 00:38:44,822
‎（索薇娅：天啊 好美）

717
00:38:44,905 --> 00:38:46,490
‎（奥利维亚：你太美了）

718
00:38:46,574 --> 00:38:48,200
‎（阿瓦：你把耳朵P大了吗？）

719
00:38:48,284 --> 00:38:49,118
‎（哈哈）

720
00:38:52,371 --> 00:38:56,208
We evolved to care about whether other people in our tribe...
‎我们进化出 ‎在意我们社群中的其他人…

721
00:38:55,583 --> 00:38:56,667
‎（布里亚纳：漂亮！）

722
00:38:56,751 --> 00:38:59,128
think well of us or not 'cause it matters.
‎…是否对我们有好印象的机制 ‎因为这很重要

723
00:38:59,837 --> 00:39:04,550
But were we evolved to be aware of what 10,000 people think of us?
‎但我们的进化 需要我们在意 ‎一万个人怎么看我们吗？

724
00:39:04,633 --> 00:39:08,763
We were not evolved to have social approval being dosed to us
‎我们的进化 ‎不需要每隔五分钟

725
00:39:08,846 --> 00:39:10,348
every five minutes.
‎就获得一次社交认可

726
00:39:10,431 --> 00:39:13,142
That was not at all what we were built to experience.
‎这根本不是我们需要去体验的

727
00:39:15,394 --> 00:39:19,982
We curate our lives around this perceived sense of perfection
‎我们管理自己的生活 ‎建立在获得的完美感上

728
00:39:20,733 --> 00:39:23,527
because we get rewarded in these short  term signals
‎因为爱心、点赞、竖起大拇指 ‎这些短期的信号

729
00:39:23,611 --> 00:39:25,154
hearts, likes, thumbs  up
‎给我们奖赏

730
00:39:25,237 --> 00:39:28,407
and we conflate that with value, and we conflate it with truth.
‎我们把它融合到价值中 ‎融合到真相中

731
00:39:29,825 --> 00:39:33,120
And instead, what it really is is fake, brittle popularity...
‎不论是否虚假 易破碎的人气

732
00:39:33,913 --> 00:39:37,458
that's short  term and that leaves you even more, and admit it,
‎这是短期的 你需要承认 这让你更加

733
00:39:37,541 --> 00:39:39,919
vacant and empty before you did it.
‎空虚 于是会再次这样做

734
00:39:41,295 --> 00:39:43,381
Because then it forces you into this vicious cycle
‎因为这样 ‎它会将你逼入这样一个恶性循环

735
00:39:43,464 --> 00:39:47,176
where you're like, "What's the next thing I need to do now?   'Cause I need it back."
‎你会想：“我接下来要做什么？ ‎因为我还想要这种感觉”

736
00:39:48,260 --> 00:39:50,846
Think about that compounded by two billion people,
‎想一下 这种现象被20亿人复杂化

737
00:39:50,930 --> 00:39:54,767
and then think about how people react then to the perceptions of others.
‎然后想一下 之后人们 ‎会怎样回应别人对自己的看法

738
00:39:54,850 --> 00:39:56,435
It's just a... It's really bad.
‎真的… 真的很恶劣

739
00:39:56,977 --> 00:39:58,229
It's really, really bad.
‎真的太恶劣了

740
00:40:00,856 --> 00:40:03,484
There has been a gigantic increase
‎美国青少年群体中

741
00:40:03,567 --> 00:40:06,529
in depression and anxiety for American teenagers
‎出现了大幅增长的抑郁和焦虑

742
00:40:06,612 --> 00:40:10,950
which began right around... between 2011 and 2013.
‎大概就在2011年到2013年开始的

743
00:40:11,033 --> 00:40:15,371
The number of teenage girls out of 100,000 in this country
‎这个国家中 每十万名少女中

744
00:40:15,454 --> 00:40:17,123
who were admitted to a hospital every year
‎每年因为割腕或者自残

745
00:40:17,206 --> 00:40:19,917
because they cut themselves or otherwise harmed themselves,
‎进医院接受治疗的人数

746
00:40:20,000 --> 00:40:23,921
that number was pretty stable until around 2010, 2011,
‎在2010年到2011年是非常平稳的

747
00:40:24,004 --> 00:40:25,756
and then it begins going way up.
‎在那之后 直线上升

748
00:40:28,759 --> 00:40:32,513
It's up 62 percent for older teen girls.
‎大一点的少女中 增加了62%

749
00:40:32,054 --> 00:40:33,931
‎（美国非致命性自残住院人数）

750
00:40:33,848 --> 00:40:38,310
It's up 189 percent for the preteen girls. That's nearly triple.
‎进入青春期前的少女 增加了189% ‎将近三倍了

751
00:40:40,312 --> 00:40:43,524
Even more horrifying, we see the same pattern with suicide.
‎更可怕的是 ‎自杀也呈现出相同的趋势

752
00:40:43,190 --> 00:40:44,900
‎（美国自杀率 ‎每百万女孩死亡人数）

753
00:40:44,775 --> 00:40:47,570
The older teen girls, 15 to 19 years old,
‎大一点的少女 15到19岁

754
00:40:47,653 --> 00:40:51,699
they're up 70 percent,compared to the first decade of this century.
‎与本世纪初相比 增长了70%

755
00:40:52,158 --> 00:40:55,077
The preteen girls, who have very low rates to begin with,
‎青春期前的少女 ‎最开始的比率非常低

756
00:40:55,161 --> 00:40:57,663
they are up 151 percent.
‎现在增长了151%

757
00:40:58,831 --> 00:41:01,709
And that pattern points to social media.
‎这个增长模式 指向了社交媒体

758
00:41:01,792 --> 00:41:03,961
‎（2009年手机上的社交媒体数量）

759
00:41:04,044 --> 00:41:07,214
Gen Z, the kids born after 1996 or so,
‎Z代人 ‎1996年之后那会儿出生的孩子们

760
00:41:07,298 --> 00:41:10,342
those kids are the first generation in history
‎那些孩子们是历史上第一代

761
00:41:10,426 --> 00:41:12,636
that got on social media in middle school.
‎在初中开始使用社交媒体的

762
00:41:15,890 --> 00:41:17,600
How do they spend their time?
‎她们的时间花在了哪里呢？

763
00:41:19,727 --> 00:41:22,730
They come home from school, and they're on their devices.
‎她们放学回家 就拿起手机

764
00:41:24,315 --> 00:41:29,195
A whole generation is more anxious, more fragile, more depressed.
‎整个一代人都更加焦虑 ‎更加脆弱、更加抑郁

765
00:41:30,613 --> 00:41:33,282
They're much less comfortable taking risks.
她们更不愿意冒险

766
00:41:34,325 --> 00:41:37,536
The rates at which they get driver's licenses have been dropping.
‎她们拿到驾照的比率下降了

767
00:41:38,954 --> 00:41:41,081
The number who have ever gone out on a date
‎出去约会过的人数

768
00:41:41,165 --> 00:41:44,251
or had any kind of romantic interaction is dropping rapidly.
‎有过任何形式浪漫互动的人数骤减

769
00:41:47,505 --> 00:41:49,715
This is a real change in a generation.
‎整个一代人 有了真正的改变

770
00:41:53,177 --> 00:41:57,306
And remember, for every one of these, for every hospital admission,
‎别忘了 这些人中的每一个 ‎每一个住院的人

771
00:41:57,389 --> 00:42:00,267
there's a family that is traumatized and horrified.
‎背后都有一个受伤的、惊恐的家庭

772
00:42:00,351 --> 00:42:02,353
"My God, what is happening to our kids? "
‎“天啊 我的孩子们怎么了？”

773
00:42:19,411 --> 00:42:21,413
It's plain as day to me.
‎在我看来 问题很显而易见

774
00:42:22,873 --> 00:42:28,128
These services are killing people... and causing people to kill themselves.
‎这些服务正在杀人 ‎也在导致人们自杀

775
00:42:29,088 --> 00:42:33,300
I don't know any parent who says, "Yeah, I really want my kids to be growing up
‎我不认识哪个家长会说 ‎“是 我希望我的孩子们 成长过程中

776
00:42:33,384 --> 00:42:36,887
feeling manipulated by tech designers, uh,
‎感觉被技术设计师操控

777
00:42:36,971 --> 00:42:39,723
manipulating their attention, making it impossible to do their homework,
‎操控他们的注意力 ‎让他们无法完成作业

778
00:42:39,807 --> 00:42:42,560
making them compare themselves to unrealistic standards of beauty."
‎让他们将自己 ‎和不切实际的审美标准相对比”

779
00:42:42,643 --> 00:42:44,687
Like, no one wants that.
‎没有人希望那样

780
00:42:45,104 --> 00:42:46,355
No one does.
‎没有一个人

781
00:42:46,438 --> 00:42:48,482
We... We used to have these protections.
‎我们以前有一些保护措施

782
00:42:48,566 --> 00:42:50,943
When children watched Saturday morning cartoons,
‎小孩子们观看周六早间动画片的时候

783
00:42:51,026 --> 00:42:52,778
we cared about protecting children.
‎我们关心保护儿童

784
00:42:52,861 --> 00:42:56,574
We would say, "You can't advertise to these age children in these ways."
‎我们会说：“你不能这样 ‎给这个年龄段的孩子看广告”

785
00:42:57,366 --> 00:42:58,784
But then you take YouTube for Kids,
‎然后有了YouTube儿童频道

786
00:42:58,867 --> 00:43:02,454
and it gobbles up that entire portion of the attention economy,
‎蚕食了注意力经济的全部

787
00:43:02,538 --> 00:43:04,915
and now all kids are exposed to YouTube for Kids.
‎现在所有的孩子 ‎都能看YouTube儿童频道

788
00:43:04,999 --> 00:43:07,710
And all those protections and all those regulations are gone.
‎所有的保护措施 ‎所有的管理规定都不见了

789
00:43:18,304 --> 00:43:22,141
We're training and conditioning a whole new generation of people...
‎我们在训练、调节整个一代人…

790
00:43:23,434 --> 00:43:29,148
that when we are uncomfortable or lonely or uncertain or afraid,
‎我们不自在、孤独、不确定或害怕时

791
00:43:29,231 --> 00:43:31,775
we have a digital pacifier for ourselves
‎有一个自己的数码安慰

792
00:43:32,234 --> 00:43:36,488
that is kind of atrophying our own ability to deal with that.
‎这有点让我们 ‎自己处理这些情绪的能力退化了

793
00:43:53,881 --> 00:43:55,674
Photoshop didn't have 1,000 engineers
‎Photoshop屏幕的另一端 ‎

794
00:43:55,758 --> 00:43:58,969
on the other side of the screen, using notifications, using your friends,
‎没有几千个工程师 用通知 用你的朋友

795
00:43:59,053 --> 00:44:02,431
using AI to predict what's gonna perfectly addict you, or hook you,
‎用人工智能去预判 ‎什么能完美地让你上瘾 引诱你

796
00:44:02,514 --> 00:44:04,516
or manipulate you, or allow advertisers
‎或者操纵你 或者允许广告商

797
00:44:04,600 --> 00:44:08,437
to test 60,000 variations of text or colors to figure out
‎去测试六万种不同的文本或颜色

798
00:44:06,977 --> 00:44:08,395
‎（芝加哥反垄断技术会议）

799
00:44:08,520 --> 00:44:11,065
what's the perfect manipulation of your mind.
‎…以此来找到 ‎怎样能完美地操纵你的思想

800
00:44:11,148 --> 00:44:14,985
This is a totally new species of power and influence.
‎这是一种全新的力量和影响

801
00:44:16,070 --> 00:44:19,156
I... I would say, again, the methods used
‎我想再一次强调 他们使用的

802
00:44:19,239 --> 00:44:22,785
to play on people's ability to be addicted or to be influenced
‎玩弄人们的能力 ‎让人们成瘾或者受到影响

803
00:44:22,868 --> 00:44:25,204
may be different this time, and they probably are different.
‎或许这一次是不同的 ‎或许他们是不同的

804
00:44:25,287 --> 00:44:28,749
They were different when newspapers came in and the printing press came in,
‎报纸问世时 印刷媒体问世时 ‎与现在很不同

805
00:44:28,832 --> 00:44:31,835
and they were different when television came in,
‎电视问世的时候 与现在很不同

806
00:44:31,919 --> 00:44:34,004
and you had three major networks and...
‎当时有三个主要的…‎  网络

807
00:44:34,463 --> 00:44:36,423
At the time.   At the time. That's what I'm saying.
‎  当时 我也想说

808
00:44:36,507 --> 00:44:38,384
But I'm saying the idea that there's a new level
‎但我在说 这是一个全新的层次

809
00:44:38,467 --> 00:44:42,054
and that new level has happened so many times before.
‎这个新的层次 以前也发生过很多次

810
00:44:42,137 --> 00:44:45,099
I mean, this is just the latest new level that we've seen.
‎这只是我们看见的 最新的一个层次

811
00:44:45,182 --> 00:44:48,727
There's this narrative that, you know, "We'll just adapt to it.
‎有这样一种说法：“我们去适应它

812
00:44:48,811 --> 00:44:51,188
We'll learn how to live with these devices,
‎我们要学着与这些设备共存

813
00:44:51,271 --> 00:44:53,732
just like we've learned how to live with everything else."
‎就像我们学着 ‎和其他所有事物共存一样”

814
00:44:53,816 --> 00:44:56,694
And what this misses is there's something distinctly new here.
‎但这个说法没有注意到的是 ‎有些东西明显是全新的

815
00:44:57,486 --> 00:45:00,155
Perhaps the most dangerous piece of all this is the fact
‎或许这其中最危险的是

816
00:45:00,239 --> 00:45:04,410
that it's driven by technology that's advancing exponentially.
‎这是由技术驱动的 ‎在成指数地向前发展

817
00:45:04,910 --> 00:45:05,801
‎（计算机处理能力）

818
00:45:05,869 --> 00:45:09,081
Roughly, if you say from, like, the 1960s to today,
‎大体上 ‎如果你看从20世纪60年代至今

819
00:45:09,873 --> 00:45:12,960
processing power has gone up about a trillion times.
‎计算机处理能力增长了万亿倍

820
00:45:13,794 --> 00:45:18,340
Nothing else that we have has improved at anything near that rate.
‎我们身边没有任何其他东西 ‎以这个速率增长

821
00:45:18,424 --> 00:45:22,177
Like, cars are, you know, roughly twice as fast.
‎比如 汽车速度基本上才实现翻倍

822
00:45:22,261 --> 00:45:25,013
And almost everything else is negligible.
‎几乎所有其他的东西都显得微不足道

823
00:45:25,347 --> 00:45:27,182
And perhaps most importantly,
‎或许最重要的是

824
00:45:27,266 --> 00:45:31,353
our human     our physiology, our brains have evolved not at all.
‎我们人类… 我们的生理 ‎我们的大脑 根本没有丝毫进化

825
00:45:31,854 --> 00:45:35,232
‎（没有手机的时间）

826
00:45:37,401 --> 00:45:41,488
Human beings, at a mind and body and sort of physical level,
‎人类的思想、身体和体质

827
00:45:41,947 --> 00:45:43,866
are not gonna fundamentally change.
‎基本上不会改变了

828
00:45:47,035 --> 00:45:48,954
I know, but they...

829
00:45:56,837 --> 00:46:00,924
We can do genetic engineering and develop new kinds of human beings,
‎我们可以搞基因工程 ‎在未来开发新的人类物种

830
00:46:01,008 --> 00:46:05,220
but realistically speaking, you're living inside of hardware, a brain,
‎但是现实来讲 ‎你生活在大脑这个硬件下

831
00:46:05,304 --> 00:46:07,222
that was, like, millions of years old,
‎已经存在几百万年了

832
00:46:07,306 --> 00:46:10,559
and then there's this screen, and then on the opposite side of the screen,
‎然后出现了这样一个屏幕 ‎在屏幕的另一端

833
00:46:10,642 --> 00:46:13,562
there's these thousands of engineers and supercomputers
‎有上千名工程师和超级计算机

834
00:46:13,645 --> 00:46:16,106
that have goals that are different than your goals,
‎有着与你不同的目标

835
00:46:16,190 --> 00:46:19,693
and so, who's gonna win in that game?  Who's gonna win?
‎那么 这个游戏谁能赢呢？谁会赢？

836
00:46:25,699 --> 00:46:26,617
How are we losing?
‎我们怎么会输？

837
00:46:27,159 --> 00:46:29,828
I don't know.   Where is he?  This is not normal.
‎  我不知道 ‎  他在哪里？这太不正常了

838
00:46:29,912 --> 00:46:32,080
Did I overwhelm him with friends and family content?
‎我给他推送朋友和家人的内容太多 ‎他烦了吗？

839
00:46:32,164 --> 00:46:34,082
Probably.   Well, maybe it was all the ads.
‎  或许吧 ‎  或许是广告太多了

840
00:46:34,166 --> 00:46:37,795
No. Something's very wrong. Let's switch to resurrection mode.
‎不 一定出了严重的问题 ‎我们切换到复苏模式吧

841
00:46:39,713 --> 00:46:44,051
When you think of AI, you know, an AI's gonna ruin the world,
‎当你想到人工智能 ‎人工智能会毁掉世界

842
00:46:44,134 --> 00:46:47,221
and you see, like, a Terminator, and you see Arnold Schwarzenegger.
‎你会看到《终结者》 看到施瓦辛格…

843
00:46:47,638 --> 00:46:48,680
I'll be back.
‎我会回来的

844
00:46:48,764 --> 00:46:50,933
You see drones, and you think, like,
‎…你会看到无人机 你觉得 ‎

845
00:46:51,016 --> 00:46:52,684
"Oh, we're gonna kill people with AI."
“人工智能会杀人的”

846
00:46:53,644 --> 00:46:59,817
And what people miss is that AI already runs today's world right now.
‎人们忽略的是 人工智能 ‎现在已经在运营着当今世界了

847
00:46:59,900 --> 00:47:03,237
Even talking about "an AI" is just a metaphor.
‎甚至谈论“人工智能”都只是暗喻

848
00:47:03,320 --> 00:47:09,451
At these companies like... like Google, there's just massive, massive rooms,
‎在谷歌这种公司 有超级大的房间

849
00:47:10,327 --> 00:47:13,121
some of them underground, some of them underwater,
‎有些在地下 有些在水下

850
00:47:13,205 --> 00:47:14,498
of just computers.
‎房间里全是电脑

851
00:47:14,581 --> 00:47:17,835
Tons and tons of computers, as far as the eye can see.
‎无数的电脑 连绵不绝

852
00:47:18,460 --> 00:47:20,504
They're deeply interconnected with each other
‎它们互相之间在内部深度连接

853
00:47:20,587 --> 00:47:22,923
and running extremely complicated programs,
‎在运行着极其复杂的程序

854
00:47:23,006 --> 00:47:26,009
sending information back and forth between each other all the time.
‎始终不停地在彼此之间交换信息

855
00:47:26,802 --> 00:47:28,595
And they'll be running many different programs,
它们会运行很多不同的程序

856
00:47:28,679 --> 00:47:31,014
many different products on those same machines.
‎在同样的机器上 有不同的产品

857
00:47:31,348 --> 00:47:33,684
Some of those things could be described as simple algorithms,
‎有些东西可以被描述为简单算法

858
00:47:33,767 --> 00:47:35,227
some could be described as algorithms
‎有些算法太过复杂

859
00:47:35,310 --> 00:47:37,521
that are so complicated, you would call them intelligence.
‎就可以被称为“智能”

860
00:47:40,065 --> 00:47:43,777
I like to say that algorithms are opinions embedded in code...
‎我想说 算法是内嵌在代码中的观点…

861
00:47:45,070 --> 00:47:47,656
and that algorithms are not objective.
‎算法并不是客观的

862
00:47:48,365 --> 00:47:51,577
Algorithms are optimized to some definition of success.
‎算法被某种成功的定义优化

863
00:47:52,244 --> 00:47:53,370
So, if you can imagine,
‎所以 如果你能想象

864
00:47:53,453 --> 00:47:57,124
if a... if a commercial enterprise builds an algorithm
‎一个商业公司成功的定义

865
00:47:57,207 --> 00:47:59,293
to their definition of success,
‎需要依靠算法

866
00:47:59,835 --> 00:48:01,211
it's a commercial interest.
‎那就是商业利益

867
00:48:01,587 --> 00:48:02,671
It's usually profit.
‎通常都有利润

868
00:48:03,130 --> 00:48:07,384
You are giving the computer the goal state, "I want this outcome,"
‎你给电脑一个目标 ‎说“我想要这个结果”

869
00:48:07,467 --> 00:48:10,262
and then the computer itself is learning how to do it.
‎电脑自己去学习 怎样实现

870
00:48:10,345 --> 00:48:12,598
That's where the term "machine learning" comes from.
‎这是“机器学习”概念的由来

871
00:48:12,681 --> 00:48:14,850
And so, every day, it gets slightly better
‎所以 每一天 都会更好一点

872
00:48:14,933 --> 00:48:16,977
at picking the right posts in the right order
‎在正确命令获取正确的推送数据

873
00:48:17,060 --> 00:48:19,438
so that you spend longer and longer in that product.
‎让你在这个产品上花的时间越来越多

874
00:48:19,521 --> 00:48:22,232
And no one really understands what they're doing
‎没人能真正明白 为了实现这个目标

875
00:48:22,316 --> 00:48:23,901
in order to achieve that goal.
‎他们在做什么

876
00:48:23,984 --> 00:48:28,238
The algorithm has a mind of its own, so even though a person writes it,
‎算法有着自己的思想 虽然是人写的

877
00:48:28,906 --> 00:48:30,657
it's written in a way
‎它写出来的目的

878
00:48:30,741 --> 00:48:35,037
that you kind of build the machine, and then the machine changes itself.
‎是你建立一个机器 ‎这个机器会自己改变

879
00:48:35,120 --> 00:48:37,873
There's only a handful of people at these companies,
‎这种公司 员工非常少

880
00:48:37,956 --> 00:48:40,000
at Facebook and Twitter and other companies...
‎在脸书、推特和其他公司

881
00:48:40,083 --> 00:48:43,795
There's only a few people who understand how those systems work,
‎只有几个人能明白 ‎这些系统的工作原理

882
00:48:43,879 --> 00:48:46,715
and even they don't necessarily fully understand
‎虽然他们不需要完全理解

883
00:48:46,798 --> 00:48:49,551
what's gonna happen with a particular piece of content.
‎某一条特定的内容 会发生什么

884
00:48:49,968 --> 00:48:55,474
So, as humans, we've almost lost control over these systems.
‎作为人类 我们几乎 ‎已经失去了对这些系统的控制

885
00:48:55,891 --> 00:48:59,603
Because they're controlling, you know, the information that we see,
‎因为是它们在控制我们看到的信息

886
00:48:59,686 --> 00:49:02,189
they're controlling us more than we're controlling them.
‎更多的是它们在控制我们 ‎而不是我们控制它们

887
00:49:02,522 --> 00:49:04,733
Cross  referencing him
‎在他的地理区域

888
00:49:04,816 --> 00:49:07,319
against comparables in his geographic zone.
对他和可以对比的人 ‎进行交叉参照

889
00:49:07,402 --> 00:49:09,571
His psychometric  doppelgangers.
‎他的心理测定相似者

890
00:49:09,655 --> 00:49:13,700
There are 13,694 people behaving just like him in his region.
‎在他的地区 ‎有13694个和他行为相似的人

891
00:49:13,784 --> 00:49:16,370
What's trending with them?    We need something actually good
‎  他们中盛行什么？ ‎  我们需要真实的好东西

892
00:49:16,453 --> 00:49:17,704
for a proper resurrection,
‎才能进行有效的复苏

893
00:49:17,788 --> 00:49:19,957
given that the typical stuff isn't working.
‎因为平常的那些东西已经不起作用了

894
00:49:20,040 --> 00:49:21,875
Not even that cute girl from school.
‎学校那个可爱的姑娘都没用了

895
00:49:22,334 --> 00:49:25,253
My analysis shows that going political with Extreme Center content
‎我的分析显示 ‎用极端中心内容搞政治

896
00:49:25,337 --> 00:49:28,256
has a 62.3 percent chance of long  term engagement.
‎有62.3%的几率能够获得长期参与

897
00:49:28,340 --> 00:49:29,299
That's not bad.
‎还不错

898
00:49:29,383 --> 00:49:32,010
It's not good enough to lead with.
‎想用它引导 还不太够

899
00:49:32,302 --> 00:49:35,305
Okay, okay, so we've tried notifying him about tagged photos,
‎好 所以我们已经试过 ‎通知给他圈人照片

900
00:49:35,389 --> 00:49:39,017
invitations, current events, even a direct message from  Rebecca.
‎邀请、实事、甚至是瑞贝卡的私信

901
00:49:39,101 --> 00:49:42,813
But what about User 01265923010?
‎但是用户01265923010呢？

902
00:49:42,896 --> 00:49:44,648
Yeah, Ben loved all of her posts.
‎是 本点赞了她所有的发帖

903
00:49:44,731 --> 00:49:47,776
For months and, like, literally all of them, and then nothing.
‎几个月的所有发帖 真的点了所有 ‎然后就没有然后了

904
00:49:47,859 --> 00:49:50,445
I calculate a 92.3 percent chance of resurrection
‎我算出了通知安娜的内容

905
00:49:50,529 --> 00:49:52,030
with a notification about Ana.
‎会有92.3%的复苏概率

906
00:49:53,907 --> 00:49:55,993
‎（新感情）

907
00:49:56,535 --> 00:49:57,494
And her new friend.
‎还有她的新朋友

908
00:49:58,495 --> 00:50:04,001
‎（没有手机的时间）

909
00:50:24,354 --> 00:50:25,663
‎（你的前女友有了新感情！）

910
00:50:25,689 --> 00:50:27,441
Oh, you gotta be kiddin' me.
‎不是吧

911
00:50:35,657 --> 00:50:36,616
Okay.
‎好吧

912
00:50:37,576 --> 00:50:38,785
‎（安娜与路易斯正在热恋）

913
00:50:38,869 --> 00:50:40,996
What?
‎什么？

914
00:50:41,413 --> 00:50:42,789
Bam! We're back!
‎当！我们回来了！

915
00:50:42,873 --> 00:50:44,374
Let's get back to making money, boys.
‎我们继续挣钱 兄弟们

916
00:50:44,458 --> 00:50:46,334
Yes, and connecting Ben with the entire world.
‎好 让他们和整个世界联系起来

917
00:50:46,418 --> 00:50:49,087
I'm giving him access to all the information he might like.
‎我给他看所有他可能喜欢的信息

918
00:50:49,755 --> 00:50:53,717
Hey, do you guys ever wonder if, you know, like, the feed is good for Ben?
‎你们是否想过 ‎这些推送对本是好的吗？

919
00:50:57,095 --> 00:50:58,430
No.   No.
‎  没想过 ‎  没有

920
00:51:17,491 --> 00:51:19,076
I put a spell on you
‎我在你身上下了咒语

921
00:51:25,040 --> 00:51:26,374
'Cause you're mine
‎因为你是我的

922
00:51:34,508 --> 00:51:36,593
You better stop the things you do
‎你最好停止你做的事情

923
00:51:41,181 --> 00:51:42,265
I ain't lyin'
‎我不骗你

924
00:51:42,349 --> 00:51:44,893
‎（A/B测试 极端中心）

925
00:51:44,976 --> 00:51:46,686
No, I ain't lyin'
‎不 我不骗你

926
00:51:49,981 --> 00:51:51,817
You know I can't stand it
‎你知道我无法忍受

927
00:51:53,026 --> 00:51:54,611
You're runnin' around
‎你在四处奔跑

928
00:51:55,612 --> 00:51:57,239
You know better, Daddy
‎爸爸 你更清楚

929
00:51:58,782 --> 00:52:02,077
I can't stand it 'Cause you put me down
‎我无法忍受 因为你将我放下

930
00:52:03,286 --> 00:52:04,121
Yeah, yeah
‎耶

931
00:52:06,456 --> 00:52:08,375
I put a spell on you
‎我在你身上下了咒语

932
00:52:12,379 --> 00:52:14,840
Because you're mine
‎因为你是我的

933
00:52:18,718 --> 00:52:19,845
You're mine
‎你是我的

934
00:52:20,929 --> 00:52:24,349
So, imagine you're on Facebook...
‎想象一下 你在用脸书…

935
00:52:24,766 --> 00:52:29,312
and you're effectively playing against this artificial intelligence
‎你的对手是人工智能

936
00:52:29,396 --> 00:52:31,314
that knows everything about you,
‎它知道你的一切

937
00:52:31,398 --> 00:52:34,568
can anticipate your next move, and you know literally nothing about it,
‎能够预测你未来的举动 ‎你对它却一无所知

938
00:52:34,651 --> 00:52:37,404
except that there are cat videos and birthdays on it.
‎除了上面有猫的视频和出生日期

939
00:52:37,821 --> 00:52:39,656
That's not a fair fight.
‎这根本不是公平的竞争

940
00:52:41,575 --> 00:52:43,869
Ben and Jerry, it's time to go, bud!
‎本与杰瑞 该走了 孩子？

941
00:52:51,126 --> 00:52:51,960
Ben?
‎本？

942
00:53:02,679 --> 00:53:04,723
Ben.      Mm.
‎本

943
00:53:05,182 --> 00:53:06,057
Come on.
‎快点

944
00:53:07,225 --> 00:53:08,351
School time.
‎该上学了

945
00:53:08,435 --> 00:53:09,269
Let's go.
我们走

946
00:53:13,231 --> 00:53:16,735
‎（人道技术中心）

947
00:53:31,374 --> 00:53:33,627
How you doing today?    Oh, I'm... I'm nervous.
‎  你今天怎么样？ ‎  我很紧张

948
00:53:33,710 --> 00:53:35,003
Are ya?    Yeah.
‎  你紧张吗？ ‎  是啊

949
00:53:37,380 --> 00:53:39,132
We were all looking for the moment
‎我们都在当心这个时刻

950
00:53:39,216 --> 00:53:42,969
when technology would overwhelm human strengths and intelligence.
‎当技术会超越人类力量和智慧

951
00:53:43,053 --> 00:53:47,015
When is it gonna cross the singularity, replace our jobs, be smarter than humans?
‎技术什么时候会超越人类 ‎取代我们的工作 比人类更聪明？

952
00:53:48,141 --> 00:53:50,101
But there's this much earlier moment...
‎但有更早的时刻…

953
00:53:50,977 --> 00:53:55,315
when technology exceeds and overwhelms human weaknesses.
‎技术超越人类的弱点时

954
00:53:57,484 --> 00:54:01,780
This point being crossed is at the root of addiction,
‎这个超越的点就是上瘾

955
00:54:02,113 --> 00:54:04,741
polarization, radicalization, outrage  ification,
‎两极分化、激进化、激化愤怒

956
00:54:04,824 --> 00:54:06,368
vanity  ification, the entire thing.
‎激化虚荣 一切的根源

957
00:54:07,702 --> 00:54:09,913
This is overpowering human nature,
‎它在压制人类天性

958
00:54:10,538 --> 00:54:13,500
and this is checkmate on humanity.
‎在挫伤人性

959
00:54:30,558 --> 00:54:31,851
I'm sorry.
‎很抱歉

960
00:54:41,736 --> 00:54:44,656
One of the ways I try to get people to understand
‎我努力让人们明白

961
00:54:45,198 --> 00:54:49,828
just how wrong feeds from places like Facebook are
‎脸书这种地方的推送 ‎有多错误的一种方式

962
00:54:49,911 --> 00:54:51,454
is to think about the Wikipedia.
‎是让他们去想想维基百科

963
00:54:51,538 --> 00:54:52,872
‎（新标签页）

964
00:54:52,956 --> 00:54:56,209
When you go to a page, you're seeing the same thing as other people.
‎当你打开一个维基百科网页 ‎你和别人看到的东西是一样的

965
00:54:55,792 --> 00:54:56,960
‎（维基百科 自由的百科全书）

966
00:54:56,584 --> 00:55:00,297
So, it's one of the few things online that we at least hold in common.
‎所以 这是网络上少有的 ‎我们统一共享的东西

967
00:55:00,380 --> 00:55:03,425
Now, just imagine for a second that Wikipedia said,
‎现在 现象一下 维基百科说

968
00:55:03,508 --> 00:55:07,178
"We're gonna give each person a different customized definition,
‎我们要给每一个人不同的个性化定义

969
00:55:07,262 --> 00:55:09,472
and we're gonna be paid by people for that."
‎有人给我们钱 让我们这样做”

970
00:55:09,556 --> 00:55:13,435
So, Wikipedia would be spying on you. Wikipedia would calculate,
‎维基百科就会监视你 会计算

971
00:55:13,518 --> 00:55:17,188
"What's the thing I can do to get this person to change a little bit
‎“我要做什么 才能代表一些商业利益

972
00:55:17,272 --> 00:55:19,899
on behalf of some commercial interest? " Right?
‎让这个人产生一点改变？” 对吧？

973
00:55:19,983 --> 00:55:21,818
And then it would change the entry.
‎然后就会改变整个词条

974
00:55:22,444 --> 00:55:24,738
Can you imagine that?  Well, you should be able to,
‎你能想象吗？  ‎你应该能够想象得到

975
00:55:24,821 --> 00:55:26,823
'cause that's exactly what's happening on Facebook.
‎因为在脸书页面上 就是这样的

976
00:55:26,906 --> 00:55:28,992
It's exactly what's happening in your YouTube feed.
‎你的YouTube推送 就是这样的

977
00:55:29,075 --> 00:55:31,786
When you go to Google and type in "Climate change is,"
‎当你登录谷歌 输入“气候变化是…”

978
00:55:31,870 --> 00:55:34,998
you're going to see different results depending on where you live.
‎你会看到 根据你所居住的地区不同 ‎会出现不同的结果

979
00:55:35,081 --> 00:55:36,082
‎（气候变化是）

980
00:55:36,166 --> 00:55:38,460
In certain cities, you're gonna see it  autocomplete
‎在某些城市 你会看到自动完成…

981
00:55:38,543 --> 00:55:40,462
with "climate change is a hoax."
‎“气候变化是一场骗局”

982
00:55:40,545 --> 00:55:42,088
In other cases, you're gonna see
‎在其他地方 你将会看到

983
00:55:42,172 --> 00:55:44,841
"climate change is causing the destruction of nature."
‎“气候变化是对自然的破坏”

984
00:55:44,924 --> 00:55:48,428
And that's a function not of what the truth is about climate change,
‎这个功能 ‎提供的不都是气候变化的真相

985
00:55:48,511 --> 00:55:51,097
but about where you happen to be Googling from
‎而是你在哪里进行谷歌搜索

986
00:55:51,181 --> 00:55:54,100
and the particular things Google knows about your interests.
‎以及谷歌对你个人兴趣的了解

987
00:55:54,851 --> 00:55:58,021
Even two friends who are so close to each other,
‎即便是两个非常亲近的朋友

988
00:55:58,104 --> 00:56:00,190
who have almost the exact same set of friends,
‎他们两个有着完全相同的朋友圈子

989
00:56:00,273 --> 00:56:02,817
they think, you know, "I'm going to news feeds on Facebook.
‎他们会认为 ‎“我们会看到脸书上的新推送

990
00:56:02,901 --> 00:56:05,403
I'll see the exact same set of updates."
‎会看到完全相同的更新” ‎

991
00:56:05,487 --> 00:56:06,738
But it's not like that at all.
但事实远非如此

992
00:56:06,821 --> 00:56:08,448
They see completely different worlds
‎他们会看到完全不同的世界

993
00:56:08,531 --> 00:56:10,575
because they're based on these computers calculating
‎因为这些是基于计算机的计算

994
00:56:10,658 --> 00:56:12,035
what's perfect for each of them.
‎对每一个人来说 怎样最完美

995
00:56:12,118 --> 00:56:14,245
‎（直播中）

996
00:56:14,329 --> 00:56:18,416
The way to think about it is it's 2.7 billion Truman Shows.
‎想象这件事的一个方式是 ‎这是27亿人的《楚门的世界》

997
00:56:18,500 --> 00:56:21,294
Each person has their own reality, with their own...
‎每一个人都有自己的现实 自己的…

998
00:56:22,670 --> 00:56:23,671
facts.
‎事实

999
00:56:23,755 --> 00:56:27,008
Why do you think that, uh, Truman has never come close
‎你觉得楚门为什么到现在都 ‎从来没有接近

1000
00:56:27,092 --> 00:56:30,095
to discovering the true nature of his world until now?
‎发现他所在世界的真实本质？

1001
00:56:31,054 --> 00:56:34,140
We accept the reality of the world with which we're presented.
‎我们接受了 ‎呈现在我们面前的世界就是现实

1002
00:56:34,224 --> 00:56:35,141
It's as simple as that.
‎就是这么简单

1003
00:56:35,225 --> 00:56:36,393
‎（直播中）

1004
00:56:36,476 --> 00:56:41,064
Over time, you have the false sense that everyone agrees with you,
‎随着时间推移 你会有一种错觉 ‎觉得每一个人都认同你

1005
00:56:41,147 --> 00:56:44,067
because everyone in your news feed sounds just like you.
‎因为给你推送的新闻中 ‎每个人都和你极其相似

1006
00:56:44,567 --> 00:56:49,072
And that once you're in that state, it turns out you're easily manipulated,
‎一旦你达到了这种状态 ‎你就很容易被操纵了

1007
00:56:49,155 --> 00:56:51,741
the same way you would be manipulated by a magician.
‎和你被魔术师操纵 是同样的方式

1008
00:56:51,825 --> 00:56:55,370
A magician shows you a card trick and says, "Pick a card, any card."
‎魔术师给你看纸牌魔术 跟你说 ‎“选一张牌 哪张都行”

1009
00:56:55,453 --> 00:56:58,164
What you don't realize was that they've done a set  up,
‎你没有意识到的是 ‎他们早就给你设好陷阱了

1010
00:56:58,456 --> 00:57:00,583
so you pick the card they want you to pick.
‎于是你选的那张牌 ‎是他们想让你选的

1011
00:57:00,667 --> 00:57:03,169
And that's how Facebook works. Facebook sits there and says,
‎这就是脸书的工作原理 ‎脸书坐在那里说

1012
00:57:03,253 --> 00:57:06,172
"Hey, you pick your friends. You pick the links that you follow."
‎“喂 选择你的朋友 ‎选择你关注的联系人”

1013
00:57:06,256 --> 00:57:08,716
But that's all nonsense. It's just like the magician.
‎根本是胡扯 它就跟魔术师一样

1014
00:57:08,800 --> 00:57:11,302
Facebook is in charge of your news feed.
‎脸书负责给你进行新闻推送

1015
00:57:11,386 --> 00:57:14,514
We all simply are operating on a different set of facts.
‎我们都不过是 ‎在基于不同的一系列事实行事

1016
00:57:14,597 --> 00:57:16,474
When that happens at scale,
‎当大范围发生时

1017
00:57:16,558 --> 00:57:20,645
you're no longer able to reckon with or even consume information
‎你就再也无法考虑甚至消化

1018
00:57:20,728 --> 00:57:23,690
that contradicts with that world view that you've created.
‎与你所创造的世界观相悖的信息了

1019
00:57:23,773 --> 00:57:26,443
That means we aren't actually being objective,
‎那就意味着 ‎我们其实不是客观、

1020
00:57:26,526 --> 00:57:28,319
constructive individuals.
有建设性的个体

1021
00:57:28,403 --> 00:57:32,449
Open up your eyes, don't believe the lies! Open up...
‎睁大你的眼睛 别相信谎言！睁大…

1022
00:57:32,532 --> 00:57:34,701
And then you look over at the other side,
‎然后你扫了一眼另一边

1023
00:57:35,243 --> 00:57:38,746
and you start to think, "How can those people be so stupid?
‎你开始想 ‎“这些人怎么会如此愚蠢？”

1024
00:57:38,830 --> 00:57:42,125
Look at all of this information that I'm constantly seeing.
‎他们也看到了我不停看到的这些信息

1025
00:57:42,208 --> 00:57:44,627
How are they not seeing that same information? "
‎他们怎么会看不到相同的信息？

1026
00:57:44,711 --> 00:57:47,297
And the answer is, "They're not seeing that same information."
‎问题的答案是 ‎“他们没有看到相同的信息”

1027
00:57:47,380 --> 00:57:50,800
Open up your eyes, don't believe the lies!
‎睁大你的眼睛 别相信谎言！

1028
00:57:52,093 --> 00:57:55,472
What are Republicans like?    People that don't have a clue.
‎共和党人什么样？  ‎愚昧无知

1029
00:57:55,555 --> 00:57:58,933
The Democrat Party is a crime syndicate, not a real political party.
‎民主党就是一个犯罪团伙 ‎不是真正的政治党派

1030
00:57:59,017 --> 00:58:03,188
A huge new Pew Research Center study of 10,000 American adults
‎皮尤研究中心一项全新的大型研究 ‎对一万名美国成年人进行调查

1031
00:58:03,271 --> 00:58:05,315
finds us more divided than ever,
‎发现我们比任何时候都要分裂

1032
00:58:05,398 --> 00:58:09,152
with  personal and political polarization at a 20  year high.
‎个人和政治两极分化达到20年来最高

1033
00:58:11,738 --> 00:58:14,199
You have more than a third of Republicans saying
‎有超过三分之一的共和党人说

1034
00:58:14,282 --> 00:58:16,826
the Democratic Party is a threat to the nation,
‎民主党是对这个国家的威胁

1035
00:58:16,910 --> 00:58:20,580
more than a quarter of Democrats saying the same thing about the Republicans.
‎民主党超过四分之一的人 ‎也这样说共和党

1036
00:58:20,663 --> 00:58:22,499
So many of the problems that we're discussing,
‎我们讨论的很多问题

1037
00:58:22,582 --> 00:58:24,417
like, around political polarization
‎比如政治两极分化

1038
00:58:24,501 --> 00:58:28,046
exist in spades on cable television.
‎在有线电视上大量存在

1039
00:58:28,129 --> 00:58:31,007
The media has this exact same problem,
‎媒体也有着同样的问题

1040
00:58:31,090 --> 00:58:33,343
where their business model, by and large,
‎整体上来说 他们的商业模式

1041
00:58:33,426 --> 00:58:35,762
is that they're selling our attention to advertisers.
‎是把我们的关注出售给广告商

1042
00:58:35,845 --> 00:58:38,890
And the Internet is just a new, even more efficient way to do that.
‎网络只是一个新的、更有效率的 ‎实现方式罢了

1043
00:58:40,141 --> 00:58:44,145
At YouTube, I was working on YouTube recommendations.
‎我曾在YouTube的工作 ‎是研究YouTube推荐

1044
00:58:44,229 --> 00:58:47,148
It worries me that an algorithm that I worked on
‎让我担心的是 我研究的一个算法

1045
00:58:47,232 --> 00:58:50,401
is actually increasing polarization in society.
‎增加了社会中的两极分化

1046
00:58:50,485 --> 00:58:53,112
But from the point of view of watch time,
‎但从观看时间来看

1047
00:58:53,196 --> 00:58:57,617
this polarization is extremely efficient at keeping people online.
‎这种两极分化 ‎在让人们持续在线观看上 极其有效

1048
00:58:58,785 --> 00:59:00,870
The only reason these teachers are teaching this stuff
‎这些老师教这些东西的唯一原因

1049
00:59:00,954 --> 00:59:02,288
is 'cause they're getting paid to.
‎是有人给他们钱 让他们教

1050
00:59:02,372 --> 00:59:04,374
It's absolutely absurd.      Hey, Benji.
‎  太扯了 ‎  喂 本杰

1051
00:59:04,916 --> 00:59:06,292
No soccer practice today?
‎今天没有足球训练吗？

1052
00:59:06,376 --> 00:59:08,795
Oh, there is. I'm just catching up on some news stuff.
‎有 我只想看看今天的新闻

1053
00:59:08,878 --> 00:59:11,506
Do research. Anything that sways from the Extreme Center
‎你去研究一下 极端中心说的任何话…

1054
00:59:11,589 --> 00:59:14,008
Wouldn't exactly call the stuff that you're watching news.
‎都不会把你看的这个东西称作新闻

1055
00:59:15,552 --> 00:59:18,846
You're always talking about how messed up everything is. So are they.
‎你总是说一切都那么混乱 确实是

1056
00:59:19,305 --> 00:59:21,140
But that stuff is just propaganda.
‎但那东西只是政治鼓吹

1057
00:59:21,224 --> 00:59:24,060
Neither is true. It's all about what makes sense.
‎没有一个是真的 ‎全都是让你觉得合理

1058
00:59:24,769 --> 00:59:26,938
Ben, I'm serious. That stuff is bad for you.
‎本 我很严肃 这些东西对你有害

1059
00:59:27,021 --> 00:59:29,232
You should go to soccer practice.      Mm.
‎你应该去足球训练

1060
00:59:35,154 --> 00:59:37,490
I share this stuff because I care.
‎我分享这个东西 是因为我在意

1061
00:59:37,574 --> 00:59:41,077
I care that you are being misled, and it's not okay. All right?
‎我在意你被误导 这是不对的 好吗？

1062
00:59:41,160 --> 00:59:43,121
People think the algorithm is designed
‎人们认为算法的设计

1063
00:59:43,204 --> 00:59:46,833
to give them what they really want, only it's not.
‎是给他们真正想要的 但其实不然

1064
00:59:46,916 --> 00:59:52,589
The algorithm is actually trying to find a few rabbit holes that are very powerful,
‎算法其实是在试图 ‎找到几个非常强大的兔子洞

1065
00:59:52,672 --> 00:59:56,217
trying to find which rabbit hole is the closest to your interest.
‎试图找到哪一个兔子洞 ‎最贴近你的兴趣

1066
00:59:56,301 --> 00:59:59,262
And then if you start watching one of those videos,
‎然后 如果你开始观看其中一个视频

1067
00:59:59,846 --> 01:00:02,223
then it will recommend it over and over again.
‎它就会不停继续推荐

1068
01:00:02,682 --> 01:00:04,934
It's not like anybody wants this to happen.
‎这种情况 并不是有人刻意为之

1069
01:00:05,018 --> 01:00:07,812
It's just that this is what the recommendation system is doing.
‎只是推荐系统一直在做而已

1070
01:00:07,895 --> 01:00:10,815
So much so that Kyrie Irving, the famous basketball player,
‎以至于著名篮球运动员凯里·欧文

1071
01:00:11,065 --> 01:00:14,235
uh, said he believed the Earth was flat, and he apologized later
‎说他相信地球是平的 后来又道歉

1072
01:00:14,319 --> 01:00:16,154
because he blamed it on a YouTube rabbit hole.
‎因为他把责任推给 ‎YouTube的一个兔子洞

1073
01:00:16,487 --> 01:00:18,656
You know, like, you click the YouTube click
‎你点击 YouTube视频

1074
01:00:18,740 --> 01:00:21,534
and it goes, like, how deep the rabbit hole goes.
‎它会继续 这个兔子洞能有多深

1075
01:00:21,618 --> 01:00:23,369
When he later came on to NPR to say,
‎他后来在国家公共广播电台上说

1076
01:00:23,453 --> 01:00:25,955
"I'm sorry for believing this. I didn't want to mislead people,"
‎“很抱歉我相信了这个 ‎我无意误导人们”

1077
01:00:26,039 --> 01:00:28,291
a bunch of students in a classroom were interviewed saying,
‎有人采访了教室中的一群学生 ‎他们说

1078
01:00:28,374 --> 01:00:29,667
"The round  Earthers got to him."
‎“相信地球是圆的人肯定找他谈了”

1079
01:00:31,044 --> 01:00:33,963
The flat  Earth conspiracy theory was recommended
‎地球平面阴谋论被算法

1080
01:00:34,047 --> 01:00:37,634
hundreds of millions of times by the algorithm.
‎推荐了几亿次

1081
01:00:37,717 --> 01:00:43,890
It's easy to think that it's just a few stupid people who get convinced,
‎很容易去想 ‎只有几个愚蠢的人被说服罢了

1082
01:00:43,973 --> 01:00:46,893
but the algorithm is getting smarter and smarter every day.
‎但是算法每一天都在变得更聪明

1083
01:00:46,976 --> 01:00:50,188
So, today, they are convincing the people that the Earth is flat,
‎今天 它们说服人们相信 地球是平的

1084
01:00:50,271 --> 01:00:53,983
but tomorrow, they will be convincing you of something that's false.
‎但是明天 它们就会说服你相信 ‎一个完全虚假的事情

1085
01:00:54,317 --> 01:00:57,820
On November 7th, the hashtag "Pizzagate" was born.
‎11月7日 话题标签‘披萨门’诞生了

1086
01:00:57,904 --> 01:00:59,197
Pizzagate...
‎披萨门…

1087
01:01:00,114 --> 01:01:01,449
Oh, boy.
‎天啊

1088
01:01:03,159 --> 01:01:06,913
I still am not 100 percent sure how this originally came about,
‎我还是不能百分之百确定 ‎这个最初是从哪里来的

1089
01:01:06,996 --> 01:01:12,377
but the idea that ordering a pizza meant ordering a trafficked person.
‎但是订披萨等于订一个贩卖的人口 ‎这个想法

1090
01:01:12,460 --> 01:01:15,046
As the groups got bigger on Facebook,
‎由于脸书上的多个小组越来越大

1091
01:01:15,129 --> 01:01:19,967
Facebook's recommendation engine started suggesting to regular users
‎脸书推荐引擎开始建议普通用户

1092
01:01:20,051 --> 01:01:21,761
that they join Pizzagate groups.
‎让他们加入披萨门小组

1093
01:01:21,844 --> 01:01:27,392
So, if a user was, for example, anti  vaccine or believed in chemtrails
‎所以 如果一个用户反对疫苗 ‎或者相信飞机喷洒重金属阴谋论

1094
01:01:27,475 --> 01:01:30,645
or had indicated to Facebook's algorithms in some way
‎或者对脸书的算法表示过

1095
01:01:30,728 --> 01:01:33,398
that they were prone to belief in conspiracy theories,
‎他们易于相信阴谋论

1096
01:01:33,481 --> 01:01:36,859
Facebook's recommendation engine would serve them  Pizzagate groups.
‎脸熟的推荐引擎 ‎就会推荐给他们披萨门小组

1097
01:01:36,943 --> 01:01:41,072
Eventually, this culminated in a man showing up with a gun,
‎最终 这件事达到高潮 ‎一名男子携枪出现

1098
01:01:41,155 --> 01:01:44,617
deciding that he was gonna go liberate the children from the basement
‎决定他要给披萨店地下室的 ‎那些孩子们自由

1099
01:01:44,701 --> 01:01:46,911
of the pizza place that did not have a basement.
‎而这个披萨店根本没有地下室

1100
01:01:46,994 --> 01:01:48,538
What were you doing?
‎  你当时在这个地方做什么？ ‎

1101
01:01:48,871 --> 01:01:50,498
Making sure there was nothing there.
确保这里什么都没有

1102
01:01:50,581 --> 01:01:52,458
Regarding?       Pedophile ring.
关于什么？ ‎  恋童癖怪圈

1103
01:01:52,542 --> 01:01:54,293
What?       Pedophile ring.
‎  什么？‎  恋童癖怪圈

1104
01:01:54,377 --> 01:01:55,962
He's talking about Pizzagate.
‎披萨门 他在说披萨门

1105
01:01:56,045 --> 01:02:00,216
This is an example of a conspiracy theory
‎这是阴谋论在所有社交媒体上

1106
01:02:00,299 --> 01:02:03,678
that was propagated across all social networks.
‎到处传播的一个例子

1107
01:02:03,761 --> 01:02:06,097
The social network's own recommendation engine
‎社交网络自己的推荐引擎

1108
01:02:06,180 --> 01:02:07,974
is voluntarily serving this up to people
‎自愿把这个东西推送给

1109
01:02:08,057 --> 01:02:10,643
who had never searched for the term "Pizzagate" in their life.
‎这辈子从来没有搜索过 ‎“披萨门”的人们

1110
01:02:10,727 --> 01:02:12,687
‎（披萨门 ‎民主党和恋童癖的深盘披萨）

1111
01:02:12,437 --> 01:02:14,439
There's a study, an MIT study,
‎有一个研究 麻省理工的研究

1112
01:02:14,522 --> 01:02:19,819
that fake news on Twitter spreads six times faster than true news.
‎说推特上传播的虚假新闻 ‎比真实新闻传播速度快六倍

1113
01:02:19,902 --> 01:02:21,863
What is that world gonna look like
‎当一个人有着高于另一个人

1114
01:02:21,946 --> 01:02:24,741
when one has a six  times advantage to the other one?
‎六倍的优势 这种世界会是什么样？

1115
01:02:25,283 --> 01:02:27,660
You can imagine these things are sort of like...
‎你可以想象 这些事情有点…

1116
01:02:27,744 --> 01:02:31,706
they... they tilt the floor of... of human behavior.
‎将人类行为的基础平面倾斜了

1117
01:02:31,789 --> 01:02:34,709
They make some behavior harder and some easier.
‎让一些行为更难 让一些行为更容易

1118
01:02:34,792 --> 01:02:37,420
And you're always free to walk up the hill,
‎你总是可以自由地走上山坡

1119
01:02:37,503 --> 01:02:38,796
but fewer people do,
‎但是这样做的人越来越少

1120
01:02:38,880 --> 01:02:43,092
and so, at scale, at society's scale, you really are just tilting the floor
‎所以大范围内 在整个社会范围内 ‎你就是将基础平面倾斜了

1121
01:02:43,176 --> 01:02:45,970
and changing what billions of people think and do.
‎改变了数十亿人的想法和行为

1122
01:02:46,053 --> 01:02:52,018
We've created a system that biases towards false information.
‎我们创造了一个 ‎偏心于虚假消息的体系

1123
01:02:52,643 --> 01:02:54,437
Not because we want to,
‎并不是因为我们想这样做

1124
01:02:54,520 --> 01:02:58,816
but because false information makes the companies more money
‎而是因为虚假信息比真实信息 ‎能让各个公司

1125
01:02:59,400 --> 01:03:01,319
than the truth. The truth is boring.
‎赚到更多钱 真实信息比较无聊

1126
01:03:01,986 --> 01:03:04,489
It's a disinformation  for  profit business model.
‎这是一个 ‎利用虚假信息牟利的商业模式

1127
01:03:04,906 --> 01:03:08,159
You make money the more you allow unregulated messages
‎允许未受监管的信息传送给更多的人

1128
01:03:08,701 --> 01:03:11,287
to reach anyone for the best price.
‎卖出最好的价钱 以此来赚钱

1129
01:03:11,662 --> 01:03:13,956
Because climate change?  Yeah.
‎因为气候变化？对

1130
01:03:14,040 --> 01:03:16,751
It's a hoax. Yeah, it's real. That's the point.
‎这是骗局 对 是真的 这才是重点

1131
01:03:16,834 --> 01:03:20,046
The more they talk about it and the more they divide us,
‎他们谈论这件事情越多 ‎就越会将我们分化

1132
01:03:20,129 --> 01:03:22,423
the more they have the power, the more...
‎他们越有力量 就越有控制权

1133
01:03:22,507 --> 01:03:25,468
Facebook has trillions of these news feed posts.
‎脸书有万亿个新闻推送贴

1134
01:03:26,552 --> 01:03:29,180
They can't know what's real or what's true...
‎他们无法知道哪些是真的 ‎哪些是事实…

1135
01:03:29,972 --> 01:03:33,726
which is why this conversation is so critical right now.
‎所以当下这个议题 才如此重要

1136
01:03:33,810 --> 01:03:37,021
It's not just COVID  19 that's spreading fast.
‎传播迅速的 不只是新冠病毒

1137
01:03:37,104 --> 01:03:40,191
There's a flow of misinformation online about the virus.
‎网上有关于这个病毒的大量虚假信息

1138
01:03:40,274 --> 01:03:41,818
The notion drinking water
‎多喝水能将新冠病毒

1139
01:03:41,901 --> 01:03:43,694
will flush coronavirus from your system
‎从你身体中冲走的提议

1140
01:03:43,778 --> 01:03:47,490
is one of several myths about the virus circulating on social media.
‎是在社交媒体上广泛流传的 ‎该病毒的谜题之一

1141
01:03:47,573 --> 01:03:50,451
The government planned this event, created the virus,
‎这是政府计划的事件 ‎创造了这个病毒

1142
01:03:50,535 --> 01:03:53,621
and had a simulation of how the countries would react.
‎来模拟世界各国将会如何应对

1143
01:03:53,955 --> 01:03:55,581
Coronavirus is a... a hoax.
‎新冠病毒是一场骗局

1144
01:03:56,165 --> 01:03:57,959
SARS, coronavirus.
‎非典 新冠病毒

1145
01:03:58,376 --> 01:04:01,045
And look at when it was made. 2018.
‎看看这是什么时候制造的 2018年

1146
01:04:01,128 --> 01:04:03,798
I think the US government started this shit.
‎我觉得是美国政府开始的这场闹剧

1147
01:04:04,215 --> 01:04:09,095
Nobody is sick. Nobody is sick. Nobody knows anybody who's sick.
‎根本没有人生病 没人生病

1148
01:04:07,426 --> 01:04:09,095
‎没有人认识哪个人真正生病了

1149
01:04:09,512 --> 01:04:13,015
Maybe the government is using the coronavirus as an excuse
‎或许是政府在利用新冠病毒当借口

1150
01:04:13,099 --> 01:04:15,643
to get everyone to stay inside because something else is happening.
‎让所有人留在家 ‎因为有其他的事情要发生

1151
01:04:15,726 --> 01:04:18,020
Coronavirus is not killing people,
‎新冠病毒不会杀人

1152
01:04:18,104 --> 01:04:20,940
it's the 5G radiation that they're pumping out.
‎他们是在掩盖5G辐射害死的人

1153
01:04:21,023 --> 01:04:22,525
‎（5G信号塔被割倒焚烧）

1154
01:04:22,608 --> 01:04:24,944
We're being bombarded with rumors.
‎我们被谣言轰炸

1155
01:04:25,403 --> 01:04:28,823
People are blowing up actual physical cell phone towers.
‎人们去毁掉了真实的手机信号塔

1156
01:04:28,906 --> 01:04:32,201
We see Russia and China spreading rumors and conspiracy theories.
‎我们看到俄罗斯和中国 ‎传播谣言和阴谋论

1157
01:04:32,285 --> 01:04:35,246
This morning, panic and protest in Ukraine as...
‎今天早上 乌克兰的恐慌与抗议…

1158
01:04:35,329 --> 01:04:38,916
People have no idea what's true, and now it's a matter of life and death.
‎人们不知道什么是真相 ‎现在已经闹出人命了

1159
01:04:39,876 --> 01:04:42,628
Those sources that are spreading coronavirus misinformation
‎那些传播新冠病毒虚假信息的来源

1160
01:04:42,712 --> 01:04:45,798
have amassed something like 52 million engagements.
‎积累了5200万人参与

1161
01:04:45,882 --> 01:04:50,094
You're saying that silver solution would be effective.
‎你是说 胶银溶液会有效

1162
01:04:50,177 --> 01:04:54,140
Well, let's say it hasn't been tested on this strain of the coronavirus, but...
‎我们这样说吧 ‎虽然没有用新冠病毒的毒株测试过

1163
01:04:54,223 --> 01:04:57,226
What we're seeing with COVID is just an extreme version
‎我们看到的新冠病毒 ‎只是发生在我们信息生态系统中的

1164
01:04:57,310 --> 01:05:00,521
of what's happening across our information ecosystem.
‎一个极端案例

1165
01:05:00,187 --> 01:05:01,022
‎（惠特默 希特勒）

1166
01:05:00,938 --> 01:05:05,026
Social media amplifies exponential gossip and exponential hearsay
‎社交媒体放大了增长迅速的谣言 ‎和增长迅速的道听途说

1167
01:05:05,109 --> 01:05:07,111
to the point that we don't know what's true,
‎以至于我们都不知道 什么是真了

1168
01:05:07,194 --> 01:05:08,946
no matter what issue we care about.
‎不管我们关注的是什么问题

1169
01:05:15,161 --> 01:05:16,579
He discovers this.
他发现了这个

1170
01:05:19,874 --> 01:05:21,292
Ben.
本

1171
01:05:26,130 --> 01:05:28,257
Are you still on the team?       Mm  hmm.
‎你还在队里吗？

1172
01:05:30,384 --> 01:05:32,678
Okay, well, I'm gonna get a snack before practice
‎好 如果你想来 我在训练之前 ‎先去吃点零食 你要来吗？

1173
01:05:32,762 --> 01:05:34,430
if you... wanna come.
先去吃点零食 你要来吗？

1174
01:05:35,640 --> 01:05:36,515
Hm?
嗯？

1175
01:05:36,974 --> 01:05:38,601
You know, never mind.
‎当我没说

1176
01:05:45,066 --> 01:05:47,526
Nine out of ten people are dissatisfied right now.
‎当前 十个人当中 有九个人不满意

1177
01:05:47,610 --> 01:05:50,613
The EC is like any political movement in history, when you think about it.
‎你仔细想想 极端中心和历史上 ‎任何政治运动无异

1178
01:05:50,696 --> 01:05:54,492
We are standing up, and we are... we are standing up to this noise.
‎我们要站起来反抗 ‎我们要站起来反抗这种杂音

1179
01:05:54,575 --> 01:05:57,036
You are my people. I trust you guys.
‎你是我的人民 我相信你们

1180
01:05:59,246 --> 01:06:02,583
The Extreme Center content is brilliant.   He absolutely loves it.
‎  极端中心的内容好棒 ‎  他非常喜欢

1181
01:06:02,667 --> 01:06:03,626
Running an auction.
‎运行一个拍卖

1182
01:06:04,627 --> 01:06:08,547
840 bidders. He sold for 4.35 cents to a weapons manufacturer.
‎843个竞标人 他以4.35美分 ‎卖给了一个武器生产商

1183
01:06:08,631 --> 01:06:10,800
Let's promote some of these events.
‎我们来宣传一下这些活动

1184
01:06:10,883 --> 01:06:13,511
Upcoming rallies in his geographic zone later this week.
‎这星期晚些时候 ‎将在这片地理区域发生的群众集会

1185
01:06:13,594 --> 01:06:15,179
I've got a new vlogger lined up, too.
‎新的视频博主也安排好了

1186
01:06:17,765 --> 01:06:22,979
And... and, honestly, I'm telling you, I'm willing to do whatever it takes.
‎说实话 我告诉你 ‎我愿意付出任何代价

1187
01:06:23,062 --> 01:06:24,939
And I mean whatever.
‎我说 任何代价

1188
01:06:32,154 --> 01:06:33,197
Subscribe...      Ben?
‎  订阅… ‎  本？

1189
01:06:33,280 --> 01:06:35,908
...and also come back because I'm telling you, yo...
‎…记得回来 因为我告诉你们…

1190
01:06:35,992 --> 01:06:38,869
...I got some real big things comin'.
‎我后面会有大事件

1191
01:06:38,953 --> 01:06:40,162
Some real big things.
‎非常大的事件

1192
01:06:40,788 --> 01:06:45,292
One of the problems with Facebook is that, as a tool of persuasion,
‎脸书的一个问题是 ‎作为一个有劝说性质的工具

1193
01:06:45,793 --> 01:06:47,920
it may be the greatest thing ever created.
‎它或许是史上最伟大的发明

1194
01:06:48,004 --> 01:06:52,508
Now, imagine what that means in the hands of a dictator or an authoritarian.
‎现在 你来想象一下 落在独裁者 ‎或者集权主义者手中 会怎样

1195
01:06:53,718 --> 01:06:57,638
If you want to control the population of your country,
‎如果你想控制你们国家的人民

1196
01:06:57,722 --> 01:07:01,308
there has never been a tool as effective as Facebook.
‎从未有过像脸书这样有效的工具

1197
01:07:04,937 --> 01:07:07,398
Some of the most troubling implications
‎一个问题最大的影响是

1198
01:07:07,481 --> 01:07:10,985
of governments and other bad actors weaponizing social media,
‎一些政府和其他不良人士 ‎把社交媒体当作武器

1199
01:07:11,235 --> 01:07:13,612
um, is that it has led to real, offline harm.
‎导致了真实的线下伤害

1200
01:07:13,696 --> 01:07:15,072
I think the most prominent example
‎我认为最明显的案例

1201
01:07:15,156 --> 01:07:17,658
that's gotten a lot of press is what's happened in Myanmar.
‎广泛被媒体关注的 ‎是缅甸发生的事情

1202
01:07:17,742 --> 01:07:19,160
‎（缅甸总统办公室）

1203
01:07:19,243 --> 01:07:21,203
In Myanmar, when people think of the Internet,
‎在缅甸 人们想到网络时

1204
01:07:21,287 --> 01:07:22,913
what they are thinking about is Facebook.
‎他们想到的 是脸书

1205
01:07:22,997 --> 01:07:25,916
And what often happens is when people buy their cell phone,
‎经常发生的事情是 人们买了手机

1206
01:07:26,000 --> 01:07:29,920
the cell phone shop owner will actually preload Facebook on there for them
‎手机店主会提前帮他们下载好脸书

1207
01:07:30,004 --> 01:07:31,505
and open an account for them.
‎帮他们开好账户

1208
01:07:31,589 --> 01:07:34,884
And so when people get their phone, the first thing they open
‎于是人们拿到手机之后 ‎第一个打开的应用

1209
01:07:34,967 --> 01:07:37,595
and the only thing they know how to open is Facebook.
‎他们唯一知道怎样打开的 就是脸书

1210
01:07:38,179 --> 01:07:41,891
Well, a new bombshell investigation exposes Facebook's growing struggle
‎一个新的震惊调查显示 ‎脸书日益增长的

1211
01:07:41,974 --> 01:07:43,809
to tackle hate speech in Myanmar.
‎对抗缅甸仇恨言论的难题

1212
01:07:43,893 --> 01:07:46,020
‎（停止杀害穆斯林）

1213
01:07:46,103 --> 01:07:49,190
Facebook really gave the military and other bad actors
‎脸书真的给军人和其他不良人士

1214
01:07:49,273 --> 01:07:51,776
a new way to manipulate public opinion
‎一种控制公众言论的新手段

1215
01:07:51,859 --> 01:07:55,529
and to help incite violence against the Rohingya Muslims
‎并协助煽动 ‎针对罗兴亚族穆斯林的暴力

1216
01:07:55,613 --> 01:07:57,406
that included mass killings,
‎包括大屠杀

1217
01:07:58,115 --> 01:07:59,867
burning of entire villages,
‎焚烧整个村庄

1218
01:07:59,950 --> 01:08:03,704
mass rape, and other serious crimes against humanity
‎违反人道主义的 ‎大规模强奸和其他严重犯罪行为

1219
01:08:03,788 --> 01:08:04,955
that have now led
‎已经导致

1220
01:08:05,039 --> 01:08:08,209
to 700,000  Rohingya Muslims having to flee the country.
七十万罗兴亚族穆斯林 ‎逃出这个国家

1221
01:08:11,170 --> 01:08:14,799
It's not that highly motivated propagandists
‎这种情绪高涨的鼓吹 ‎

1222
01:08:14,882 --> 01:08:16,550
haven't existed before.
以前并不是没有出现过

1223
01:08:16,634 --> 01:08:19,762
It's that the platforms make it possible
‎只是这个平台实现了

1224
01:08:19,845 --> 01:08:23,724
to spread manipulative narratives with phenomenal ease,
‎让操纵性言论传播变得异常容易

1225
01:08:23,808 --> 01:08:25,434
and without very much money.
‎也不用花多少钱

1226
01:08:25,518 --> 01:08:27,812
If I want to manipulate an election,
‎如果我想操纵竞选

1227
01:08:27,895 --> 01:08:30,564
I can now go into a conspiracy theory group on Facebook,
‎我现在可以去脸书上的 ‎一个阴谋论小组

1228
01:08:30,648 --> 01:08:32,233
and I can find 100 people
‎可以找到一百个人

1229
01:08:32,316 --> 01:08:34,443
who believe that the Earth is completely flat
‎他们深信地球是平的

1230
01:08:34,860 --> 01:08:37,780
and think it's all this conspiracy theory that we landed on the moon,
‎认为我们登月 完全是阴谋论

1231
01:08:37,863 --> 01:08:41,450
and I can tell Facebook, "Give me 1,000 users who look like that."
‎我可以告诉脸书 ‎“给我推荐一千个这种用户”

1232
01:08:42,118 --> 01:08:46,080
Facebook will happily send me thousands of users that look like them
‎脸书会非常开心地发给我 ‎几千个这种用户

1233
01:08:46,163 --> 01:08:49,250
that I can now hit with more conspiracy theories.
‎我现在可以给他们讲更多的阴谋论

1234
01:08:50,376 --> 01:08:53,087
Sold for 3.4 cents an impression.
‎以3.4美分卖了一个印象

1235
01:08:53,379 --> 01:08:56,382
New EC video to promote.   Another ad teed up.
‎推广新的极端中心  ‎再安排一个广告

1236
01:08:58,509 --> 01:09:00,928
Algorithms and manipulative politicians
‎算法和操纵人的政治家

1237
01:09:01,011 --> 01:09:02,138
are becoming so expert
‎在学习如何激发我们的方面

1238
01:09:02,221 --> 01:09:04,056
at learning how to trigger us,
‎变得非常专业

1239
01:09:04,140 --> 01:09:08,352
getting so good at creating fake news that we absorb as if it were reality,
‎非常擅长制造我们容易接受的 ‎虚假新闻 假装这就是事实

1240
01:09:08,435 --> 01:09:10,813
and confusing us into believing those lies.
‎给我们造成混乱 ‎让我们相信这些谎言

1241
01:09:10,896 --> 01:09:12,606
It's as though we have less and less control
‎我们似乎对自己是怎样的人 ‎

1242
01:09:12,690 --> 01:09:14,150
over who we are and what we believe.
自己的信仰 有越来越少的控制权

1243
01:09:31,375 --> 01:09:32,835
...so they can pick sides.
‎…来让他们选择站队

1244
01:09:32,918 --> 01:09:34,879
There's lies here, and there's lies over there.
‎到处都是谎言

1245
01:09:34,962 --> 01:09:36,338
So they can keep the power,
‎这样他们就能保持住权力

1246
01:09:36,422 --> 01:09:39,967
so they can control everything.
‎这样他们就能控制一切

1247
01:09:40,050 --> 01:09:42,553
They can control our minds,
‎他们可以控制我们的思想 ‎

1248
01:09:42,636 --> 01:09:46,390
so that they can keep their secrets.
这样他们就可以保守他们的秘密

1249
01:09:44,638 --> 01:09:46,390
‎（质疑真相）

1250
01:09:46,473 --> 01:09:48,517
‎（疾控中心承认掩盖疫苗/自闭症）

1251
01:09:48,517 --> 01:09:50,895
Imagine a world where no one believes anything true.
‎想象一个没有人相信任何真相的世界

1252
01:09:50,978 --> 01:09:53,314
‎（疫苗不普适所有人 ‎我们的基因就是证据）

1253
01:09:52,897 --> 01:09:55,649
Everyone believes the government's lying to them.
‎所有人都相信 政府在骗他们

1254
01:09:56,317 --> 01:09:58,444
Everything is a conspiracy theory.
‎一切都是阴谋论

1255
01:09:58,527 --> 01:10:01,197
"I shouldn't trust anyone. I hate the other side."
‎“我不应该相信任何人 ‎我痛恨对立面”

1256
01:10:01,280 --> 01:10:02,698
That's where all this is heading.
‎一切正在向这个方向发展

1257
01:10:02,781 --> 01:10:06,160
The political earthquakes in Europe continue to rumble.
‎欧洲的政治地震 余震不止

1258
01:10:06,243 --> 01:10:08,412
This time, in Italy and Spain.
‎这一次 轮到了意大利和西班牙

1259
01:10:08,495 --> 01:10:11,999
Overall, Europe's traditional, centrist coalition lost its majority
‎整体上来说 欧洲传统的 ‎中间派联合政府失去了大多数人支持

1260
01:10:12,082 --> 01:10:15,002
while far right and far left populist parties made gains.
‎同时极左和极右民粹主义政党 ‎获得更多支持

1261
01:10:17,588 --> 01:10:19,048
‎（中心）

1262
01:10:19,757 --> 01:10:20,591
Back up.
‎退后

1263
01:10:21,300 --> 01:10:22,509
Okay, let's go.
‎好  我们走

1264
01:10:28,390 --> 01:10:31,268
These accounts were deliberately, specifically attempting
‎这些账户专门故意试图

1265
01:10:31,352 --> 01:10:34,355
to sow political discord in Hong Kong.
‎散播香港政治纷争信息

1266
01:10:38,609 --> 01:10:40,361
All right, Ben.
‎好 本

1267
01:10:42,863 --> 01:10:45,032
What does it look like to be a country
‎生活在一个全部信息来自于脸书

1268
01:10:45,115 --> 01:10:48,410
that's entire diet is Facebook and social media?
‎和社交媒体的国家 是什么感觉？

1269
01:10:48,953 --> 01:10:50,871
Democracy crumbled quickly.
‎民主迅速崩溃

1270
01:10:50,955 --> 01:10:51,830
Six months.
‎六个月

1271
01:10:51,914 --> 01:10:53,791
After that chaos in Chicago,
‎芝加哥的混乱发生后

1272
01:10:53,874 --> 01:10:57,086
violent clashes between protesters and supporters...
‎抗议者和支持者之间的暴力冲突…

1273
01:10:58,003 --> 01:11:01,632
Democracy is facing a crisis of confidence.
‎民主正面临着信心危机

1274
01:11:01,715 --> 01:11:04,343
What we're seeing is a global assault on democracy.
‎我们看到的 是对全球民主的攻击

1275
01:11:04,426 --> 01:11:05,427
‎（极端中心）

1276
01:11:05,511 --> 01:11:07,930
Most of the countries that are targeted are countries
‎多数目标国家 都是

1277
01:11:08,013 --> 01:11:09,723
that run democratic elections.
‎进行民主选举的国家

1278
01:11:10,641 --> 01:11:12,518
This is happening at scale.
‎大范围发生

1279
01:11:12,601 --> 01:11:15,562
By state actors, by people with millions of dollars saying,
‎国家行动者、家财万贯的富翁说

1280
01:11:15,646 --> 01:11:18,524
"I wanna destabilize Kenya. I wanna destabilize Cameroon.
‎“我想让肯尼亚动摇 ‎我想让喀麦隆动摇

1281
01:11:18,607 --> 01:11:20,651
Oh, Angola?  That only costs this much."
‎安哥拉？只要这么一点钱”

1282
01:11:20,734 --> 01:11:23,362
An extraordinary election took place Sunday in Brazil.
‎巴西上周日举行了一场特别的选举

1283
01:11:23,445 --> 01:11:25,823
With a campaign that's been powered by social media.
‎选举动员是社交媒体驱动的

1284
01:11:31,036 --> 01:11:33,956
We in the tech industry have created the tools
‎我们技术产业的人创造了

1285
01:11:34,039 --> 01:11:37,418
to destabilize and erode the fabric of society
‎动摇和侵蚀社会结构的工具

1286
01:11:37,501 --> 01:11:40,254
in every country,  all at once, everywhere.
‎所有国家都在同时发生 ‎世界各地都在发生

1287
01:11:40,337 --> 01:11:44,508
You have this in Germany, Spain, France, Brazil, Australia.
‎德国、西班牙、法国 ‎巴西、澳大利亚都有发生

1288
01:11:44,591 --> 01:11:47,261
Some of the most "developed nations" in the world
‎一些世界上最发达的国家

1289
01:11:47,344 --> 01:11:49,221
are now imploding on each other,
‎正在互相爆破

1290
01:11:49,305 --> 01:11:50,931
and what do they have in common?
‎他们有什么共同点？

1291
01:11:51,974 --> 01:11:52,975
Knowing what you know now,
‎基于你当前的了解

1292
01:11:53,058 --> 01:11:56,312
do you believe Facebook impacted the results of the 2016 election?
‎你相信脸书 ‎影响了2016年大选的结果吗？

1293
01:11:56,770 --> 01:11:58,814
Oh, that's... that is hard.
‎这个问题好难回答

1294
01:11:58,897 --> 01:12:00,691
You know,  it's... the...
‎你知道 这个…

1295
01:12:01,275 --> 01:12:04,653
the reality is, well, there were so many different forces at play.
‎事实是 ‎有很多不同的力量在产生影响

1296
01:12:04,737 --> 01:12:07,865
Representatives from Facebook, Twitter, and Google are back on Capitol Hill
‎脸书、推特和谷歌的代表们 ‎回到国会山

1297
01:12:07,948 --> 01:12:09,450
for a second day of testimony
‎对俄罗斯干预2016年大选问题

1298
01:12:09,533 --> 01:12:12,578
about Russia's interference in the 2016 election.
‎进行第二天的证词发言

1299
01:12:12,661 --> 01:12:17,291
The manipulation by third parties is not a hack.
‎第三方政党的操纵没有黑入

1300
01:12:18,500 --> 01:12:21,462
Right?  The Russians didn't hack Facebook.
‎对吧？俄罗斯没有黑入脸书

1301
01:12:21,545 --> 01:12:24,965
What they did was they used the tools that Facebook created
‎他们所做的是 利用脸书 ‎

1302
01:12:25,049 --> 01:12:27,843
for legitimate advertisers and legitimate users,
为合法广告商与合法用户创造的工具

1303
01:12:27,926 --> 01:12:30,346
and they applied it to a nefarious purpose.
‎用到了罪恶的用途中

1304
01:12:32,014 --> 01:12:34,391
It's like remote  control warfare.
‎就像是远程控制的战争

1305
01:12:34,475 --> 01:12:36,602
One country can manipulate another one
‎一个国家可以操纵另一个国家

1306
01:12:36,685 --> 01:12:39,229
without actually invading its physical borders.
‎都不用真正入侵实体边境

1307
01:12:39,605 --> 01:12:42,232
We're seeing violent images. It appears to be a dumpster
‎我们看到这些暴力的画面

1308
01:12:42,316 --> 01:12:43,317
being pushed around...
‎这是一个被推来推去的垃圾箱…

1309
01:12:43,400 --> 01:12:46,028
But it wasn't about who you wanted to vote for.
‎但问题不是你想投票给谁

1310
01:12:46,362 --> 01:12:50,574
It was about sowing total chaos and division in society.
‎问题是在社会中散播混乱和分歧

1311
01:12:50,657 --> 01:12:53,035
Now, this was in Huntington Beach. A march...
‎这是在霍廷顿海滩市的示威…

1312
01:12:53,118 --> 01:12:54,870
It's about making two sides
‎问题是制造了两个对立面

1313
01:12:54,953 --> 01:12:56,413
who couldn't hear each other anymore,
‎丝毫不再听取对方的观点

1314
01:12:56,497 --> 01:12:58,123
who didn't want to hear each other anymore,
‎不再想听对方的观点

1315
01:12:58,207 --> 01:12:59,875
who didn't trust each other anymore.
‎不再相信对方

1316
01:12:59,958 --> 01:13:03,212
This is a city where hatred was laid bare
‎这是仇恨被暴露出

1317
01:13:03,295 --> 01:13:05,464
and transformed into racial violence.
‎并转化成种族暴力的城市

1318
01:13:05,547 --> 01:13:07,925
‎（弗吉尼亚紧张局势 ‎暴力当天致三人遇害）

1319
01:13:20,145 --> 01:13:20,979
Ben!
‎本！

1320
01:13:21,605 --> 01:13:22,439
Cassandra!
‎卡桑德拉！

1321
01:13:22,981 --> 01:13:23,816
Cass!   Ben!
‎  卡桑！ ‎  本！

1322
01:13:23,899 --> 01:13:25,484
Come here! Come here!
‎过来！

1323
01:13:27,486 --> 01:13:31,156
Arms up. Arms up. Get down on your knees. Now, down.
‎举起手 膝盖跪地 快 跪下

1324
01:13:36,120 --> 01:13:37,204
Calm       Ben!
‎  冷静…… ‎  本！

1325
01:13:37,287 --> 01:13:38,664
Hey! Hands up!
‎喂！手举起来！

1326
01:13:39,623 --> 01:13:41,750
Turn around. On the ground.  On the ground!
‎转过去 趴地上

1327
01:13:56,723 --> 01:14:00,018
Do we want this system for sale to the highest bidder?
‎我们希望这个系统 ‎售卖出最高的竞价吗？

1328
01:14:01,437 --> 01:14:05,399
For democracy to be completely for sale, where you can reach any mind you want,
‎完全出售民主 ‎你可以控制任何你想控制的思想

1329
01:14:05,482 --> 01:14:09,069
target a lie to that specific population, and create culture wars?
‎对特定人群设定谎言 ‎制造文化战争？

1330
01:14:09,236 --> 01:14:10,237
Do we want that?
‎我们希望这样吗？

1331
01:14:14,700 --> 01:14:16,577
We are a nation of people...
‎我们这个国家的人民…

1332
01:14:16,952 --> 01:14:18,871
that no longer speak to each other.
‎不再和彼此说话了

1333
01:14:19,872 --> 01:14:23,000
We are a nation of people who have stopped being friends with people
‎我们这个国家的人民 ‎不再和彼此交友了

1334
01:14:23,083 --> 01:14:25,461
because of who they voted for in the last election.
‎只因为他们在上一次竞选中投票的人

1335
01:14:25,878 --> 01:14:28,422
We are a nation of people who have isolated ourselves
‎我们这个国家的人民孤立了自己

1336
01:14:28,505 --> 01:14:30,966
to only watch channels that tell us that we're right.
‎只看认同我们的那些频道

1337
01:14:32,259 --> 01:14:36,597
My message here today is that tribalism is ruining us.
‎我今天想传达的信息是 ‎部落主义正在毁掉我们

1338
01:14:37,347 --> 01:14:39,183
It is tearing our country apart.
‎它正在撕裂我们这个国家

1339
01:14:40,267 --> 01:14:42,811
It is no way for sane adults to act.
‎正常的成年人 不可能这样做

1340
01:14:43,187 --> 01:14:45,314
If everyone's entitled to their own facts,
‎如果每个人都有权执着于自己的真相

1341
01:14:45,397 --> 01:14:49,401
there's really no need for compromise, no need for people to come together.
‎就真没有必要妥协 ‎没有必要让人们团结了

1342
01:14:49,485 --> 01:14:51,695
In fact, there's really no need for people to interact.
‎事实上 真的没有必要让人们互动

1343
01:14:52,321 --> 01:14:53,530
We need to have...
‎我们需要

1344
01:14:53,989 --> 01:14:58,410
some shared understanding of reality. Otherwise, we aren't a country.
‎对现实有一些共同的理解  不然 我们就不是一个国家了

1345
01:14:58,952 --> 01:15:02,998
So, uh, long  term, the solution here is to build more AI tools
‎（扎克伯格先生）

1346
01:14:59,578 --> 01:15:02,998
‎所以长期来看 解决办法 ‎是建造更多的人工智能工具

1347
01:15:03,081 --> 01:15:08,128
that find patterns of people using the services that no real person would do.
‎找到人们使用这些服务的行为模式 ‎这是任何一个真人都做不到的

1348
01:15:08,212 --> 01:15:11,840
We are allowing the technologists to frame this as a problem
‎我们允许技术专家 把这个当做一个

1349
01:15:11,924 --> 01:15:13,884
that they're equipped to solve.
‎他们有能力解决的问题呈现

1350
01:15:15,135 --> 01:15:16,470
That is... That's a lie.
‎这是骗人的

1351
01:15:17,679 --> 01:15:20,724
People talk about AI as if it will know truth.
‎人们谈论人工智能 ‎好像人工智能知道真理一样

1352
01:15:21,683 --> 01:15:23,685
AI's not gonna solve these problems.
‎人工智能无法解决这些问题

1353
01:15:24,269 --> 01:15:27,189
AI cannot solve the problem of fake news.
‎人工智能无法解决虚假新闻的问题

1354
01:15:28,649 --> 01:15:31,026
Google doesn't have the option of saying,
‎谷歌没有选择去说

1355
01:15:31,109 --> 01:15:36,240
"Oh, is this conspiracy?  Is this truth? " Because they don't know what truth is.
‎“这是阴谋论？这是真相吗？”

1356
01:15:34,696 --> 01:15:36,240
‎因为它们不知道 真相是什么

1357
01:15:36,782 --> 01:15:37,783
They don't have a...
‎它们没有

1358
01:15:37,908 --> 01:15:40,827
They don't have a proxy for truth that's better than a click.
真相的代理服务器 ‎只有点击

1359
01:15:41,870 --> 01:15:45,123
If we don't agree on what is true
‎如果我们不同意真相

1360
01:15:45,207 --> 01:15:47,584
or that there is such a thing as truth,
‎或者不同意存在真相

1361
01:15:48,293 --> 01:15:49,294
we're toast.
‎我们就完蛋了

1362
01:15:49,753 --> 01:15:52,089
This is the problem beneath other problems
‎这是其他问题之下的问题

1363
01:15:52,172 --> 01:15:54,424
because if we can't agree on what's true,
‎因为如果我们不能认同真相

1364
01:15:55,092 --> 01:15:57,803
then we can't navigate out of any of our problems.
‎那我们就无法找到 ‎我们任何一个问题的解决方法

1365
01:16:05,435 --> 01:16:07,729
We should suggest Flat Earth Football Club.
‎我们应该建议他 ‎关注平面地球足球俱乐部

1366
01:16:07,813 --> 01:16:10,566
Don't show him sports updates. He doesn't engage.
‎别再给他展示运动消息了 ‎他不感兴趣

1367
01:16:39,886 --> 01:16:42,764
A lot of people in Silicon Valley subscribe to some kind of theory
‎硅谷的很多人相信一种理论

1368
01:16:42,848 --> 01:16:45,142
that we're building some global super brain,
‎我们正在建造一些全球的超级大脑

1369
01:16:45,309 --> 01:16:48,020
and all of our users are just interchangeable little neurons,
‎我们所有的用户 ‎都只是可交互的神经元

1370
01:16:48,103 --> 01:16:49,563
no one of which is important.
‎他们一点都不重要

1371
01:16:50,230 --> 01:16:53,150
And it subjugates people into this weird role
‎它让人们服从于这样一个奇怪的角色

1372
01:16:53,233 --> 01:16:56,069
where you're just, like, this little computing element
‎你就像是一个小的编程元素

1373
01:16:56,153 --> 01:16:58,905
that we're programming through our behavior manipulation
‎我们通过我们的行为操纵去 编程

1374
01:16:58,989 --> 01:17:02,367
for the service of this giant brain, and you don't matter.
‎为了服务于这个巨型大脑 ‎你根本不重要

1375
01:17:02,451 --> 01:17:04,911
You're not gonna get paid. You're not gonna get acknowledged.
‎不会给你钱 不会告诉你真相

1376
01:17:04,995 --> 01:17:06,455
You don't have self  determination.
‎你没有自主权

1377
01:17:06,538 --> 01:17:09,416
We'll sneakily just manipulate you because you're a computing node,
‎我们会鬼祟地操纵你 ‎因为你是编程中的结点

1378
01:17:09,499 --> 01:17:12,336
so we need to program you  'cause that's what you do with computing nodes.
‎所以我们需要将你编程 ‎因为我们就要这样对待编程中的结点

1379
01:17:20,093 --> 01:17:21,845
Oh, man.
‎天啊

1380
01:17:21,928 --> 01:17:25,390
When you think about technology and it being an existential threat,
‎当你想到技术 ‎技术是一种人类存亡的威胁

1381
01:17:25,474 --> 01:17:28,060
you know, that's a big claim, and...
‎这个指控很严重…

1382
01:17:29,603 --> 01:17:33,982
it's easy to then, in your mind, think, "Okay, so, there I am with the phone...
‎然后你的脑中就会很容易想 ‎“好 我正在拿着手机

1383
01:17:35,609 --> 01:17:37,235
scrolling, clicking, using it.
‎互动、点击、使用

1384
01:17:37,319 --> 01:17:39,196
Like, where's the existential threat?
‎人类存亡的威胁在哪里？

1385
01:17:40,280 --> 01:17:41,615
Okay, there's the supercomputer.
‎好 有一个超级电脑

1386
01:17:41,698 --> 01:17:43,950
The other side of the screen, pointed at my brain,
‎在屏幕的另一端 正指向我的大脑

1387
01:17:44,409 --> 01:17:47,537
got me to watch one more video. Where's the existential threat? "
‎让我再看一个视频 ‎人类存亡的威胁在哪里？”

1388
01:17:54,252 --> 01:17:59,341
It's not about the technology being the existential threat.
‎技术并不是人类存亡的威胁

1389
01:18:02,636 --> 01:18:04,346
‎（劝服性技术 美国参议院听政会）

1390
01:18:03,679 --> 01:18:06,264
It's the technology's ability
‎是技术能够把

1391
01:18:06,348 --> 01:18:09,476
to bring out the worst in society...
‎社会中最坏的东西带出来的能力

1392
01:18:09,559 --> 01:18:13,522
...and the worst in society being the existential threat.
‎社会中最坏的东西 ‎才是人类存亡的威胁

1393
01:18:13,605 --> 01:18:15,899
‎（美国参议院）

1394
01:18:18,819 --> 01:18:20,570
If technology creates...
‎如果技术创造了

1395
01:18:21,697 --> 01:18:23,115
mass chaos,
公众混乱

1396
01:18:23,198 --> 01:18:24,533
outrage, incivility,
‎愤怒、无礼

1397
01:18:24,616 --> 01:18:26,326
lack of trust in each other,
‎彼此缺乏信任

1398
01:18:27,452 --> 01:18:30,414
loneliness, alienation, more polarization,
‎孤独、疏远、更加两极分化

1399
01:18:30,706 --> 01:18:33,333
more election hacking, more populism,
‎更多大选黑入、更多平民政治

1400
01:18:33,917 --> 01:18:36,962
more distraction and inability to focus on the real issues...
‎让人更加分散注意力 ‎无法集中在真正的问题上…

1401
01:18:37,963 --> 01:18:39,715
that's just society.
‎那只是社会

1402
01:18:40,340 --> 01:18:46,388
And now society is incapable of healing itself
‎现在社会无法自愈

1403
01:18:46,471 --> 01:18:48,515
and just devolving into a kind of chaos.
‎转移成了一种混乱的形式

1404
01:18:51,977 --> 01:18:54,938
This affects everyone, even if you don't use these products.
‎这影响着每一个人 ‎即使你不使用这些产品

1405
01:18:55,397 --> 01:18:57,524
These things have become digital Frankensteins
‎这些事情变成了数码的科学怪人

1406
01:18:57,607 --> 01:19:00,068
that are  terraforming the world in their image,
‎让这个世界变成他们现象中的样子

1407
01:19:00,152 --> 01:19:01,862
whether it's the mental health of children
‎不论是儿童的心理健康

1408
01:19:01,945 --> 01:19:04,489
or our politics and our political discourse,
‎还是我们的政治 我们的政治演说

1409
01:19:04,573 --> 01:19:07,492
without taking responsibility for taking over the public square.
‎而不用因为控制公众舆论 ‎承担责任

1410
01:19:07,576 --> 01:19:10,579
So, again, it comes back to       And who do you think's responsible?
‎  所以 还是要回到… ‎  你觉得这一切怪谁？

1411
01:19:10,662 --> 01:19:13,582
I think we have to have the platforms be responsible
‎我认为我们必须让平台负起责任

1412
01:19:13,665 --> 01:19:15,584
for when they take over election advertising,
‎因为他们接管大选广告的时候

1413
01:19:15,667 --> 01:19:17,794
they're responsible for protecting elections.
‎就要负责保护大选

1414
01:19:17,878 --> 01:19:20,380
When they take over mental health of kids or Saturday morning,
‎当他们接管儿童心理健康 ‎或是儿童频道的时候

1415
01:19:20,464 --> 01:19:22,841
they're responsible for protecting Saturday morning.
‎他们就有责任保护好儿童频道

1416
01:19:23,592 --> 01:19:27,929
The race to keep people's attention isn't going away.
‎保持人们关注的竞争不会结束

1417
01:19:28,388 --> 01:19:31,850
Our technology's gonna become more integrated into our lives, not less.
‎我们的技术会在我们生活中更加集成 ‎而不会减少

1418
01:19:31,933 --> 01:19:34,895
The AIs are gonna get better at predicting what keeps us on the screen,
‎人工智能会更加擅长预判 ‎什么内容能让我们持续盯着屏幕

1419
01:19:34,978 --> 01:19:37,105
not worse at predicting what keeps us on the screen.
‎而不是做出更差的预判

1420
01:19:38,940 --> 01:19:42,027
I... I am 62 years old,
‎我已经62岁了

1421
01:19:42,110 --> 01:19:44,821
getting older every minute, the more this conversation goes on...
‎随着这个对话继续进行 ‎我每一分钟都在变老

1422
01:19:44,905 --> 01:19:48,033
...but... but I will tell you that, um...
‎但我会告诉你

1423
01:19:48,700 --> 01:19:52,370
I'm probably gonna be dead and gone, and I'll probably be thankful for it,
‎到时候我可能已经死了 不在了 ‎但我可能会为此感恩

1424
01:19:52,454 --> 01:19:54,331
when all this shit comes to fruition.
‎因为当这些恐怖的东西结出恶果

1425
01:19:54,790 --> 01:19:59,586
Because... Because I think that this scares me to death.
‎我觉得能吓死我

1426
01:20:00,754 --> 01:20:03,048
Do... Do you... Do you see it the same way?
‎你也这样看吗？

1427
01:20:03,548 --> 01:20:06,885
Or am I overreacting to a situation that I don't know enough about?
‎还是我对一个我不够了解的情况 ‎过度反应了？

1428
01:20:09,805 --> 01:20:11,598
What are you most worried about?
‎你最担心什么？

1429
01:20:13,850 --> 01:20:18,480
I think, in the... in the shortest time horizon...
‎我认为在最短的时间范围内…

1430
01:20:19,523 --> 01:20:20,524
civil war.
‎是内战

1431
01:20:24,444 --> 01:20:29,908
If we go down the current status quo for, let's say, another 20 years...
‎如果现在的常态继续下去 ‎我们说再过20年

1432
01:20:31,117 --> 01:20:34,579
we probably destroy our civilization through willful ignorance.
‎我们很可能会因为故意无知 ‎毁掉我们的文明

1433
01:20:34,663 --> 01:20:37,958
We probably fail to meet the challenge of climate change.
‎我们或许会无法应对气候变化的挑战

1434
01:20:38,041 --> 01:20:42,087
We probably degrade the world's democracies
‎我们或许会瓦解世界的民主

1435
01:20:42,170 --> 01:20:46,132
so that they fall into some sort of bizarre autocratic dysfunction.
‎最终衰落成一种奇怪的独裁机能障碍

1436
01:20:46,216 --> 01:20:48,426
We probably ruin the global economy.
‎我们或许会毁掉全球经济

1437
01:20:48,760 --> 01:20:52,264
Uh, we probably, um, don't survive.
‎我们或许会无法存活

1438
01:20:52,347 --> 01:20:54,808
You know, I... I really do view it as existential.
‎我真的把它看做 ‎人类生死存亡的大问题

1439
01:21:02,524 --> 01:21:04,985
Is this the last generation of people
‎这会是知道在这种幻象发生之前

1440
01:21:05,068 --> 01:21:08,488
that are gonna know what it was like before this illusion took place?
‎世界是什么样的最后一代人吗？

1441
01:21:11,074 --> 01:21:14,578
Like, how do you wake up from the matrix when you don't know you're in the matrix?
‎如果你不知道自己在矩阵中 ‎你要怎么从矩阵中醒来？

1442
01:21:17,747 --> 01:21:23,253
‎（“不论是乌托邦还是毁灭 ‎都是一场一触即发的接力赛…）

1443
01:21:23,336 --> 01:21:27,299
‎（直接通往最后一刻…” ‎——巴克敏斯特·富勒）

1444
01:21:27,382 --> 01:21:30,635
A lot of what we're saying sounds like it's just this...
‎你知道 我们说的很多话 ‎听起来像是…

1445
01:21:31,511 --> 01:21:33,680
one  sided doom and gloom.
‎片面的悲观

1446
01:21:33,763 --> 01:21:36,808
Like, "Oh, my God, technology's just ruining the world
‎“天啊 技术正在毁灭世界

1447
01:21:36,892 --> 01:21:38,059
and it's ruining kids,"
‎正在毁灭孩子们”

1448
01:21:38,143 --> 01:21:40,061
and it's like... "No."
‎不是这样的

1449
01:21:40,228 --> 01:21:45,565
It's confusing because it's simultaneous utopia...and dystopia.
‎这很困惑 ‎因为这同时是乌托邦和毁灭

1450
01:21:45,942 --> 01:21:50,447
Like, I could hit a button on my phone, and a car shows up in 30 seconds,
‎我可以在手机按一个按钮 ‎30秒后就能出现一辆车

1451
01:21:50,530 --> 01:21:52,699
and I can go exactly where I need to go.
‎我就可以去想去的任何地方

1452
01:21:52,782 --> 01:21:55,660
That is magic. That's amazing.
‎这简直是魔法 太神奇了

1453
01:21:56,161 --> 01:21:57,662
When we were making the like button,
‎我们制作“点赞”按钮的时候

1454
01:21:57,746 --> 01:22:01,499
our entire motivation was, "Can we spread positivity and love in the world? "
‎我们全部的动机是 “我们可以 ‎在世界中传播积极和爱吗？”

1455
01:22:01,583 --> 01:22:05,003
The idea that, fast  forward to today, and teens would be getting depressed
‎时间快进到当下 ‎青少年会因为没有得到足够多的点赞

1456
01:22:05,086 --> 01:22:06,421
when they don't have enough likes,
‎而抑郁 这个想法

1457
01:22:06,504 --> 01:22:08,632
or it could be leading to political polarization
‎或者会导致政治两极分化

1458
01:22:08,715 --> 01:22:09,883
was nowhere on our radar.
‎当时是完全无法想象的

1459
01:22:09,966 --> 01:22:12,135
I don't think these guys set out to be evil.
‎我不认为这些人 ‎最开始的目标是邪恶的

1460
01:22:13,511 --> 01:22:15,764
It's just the business model that has a problem.
‎只是这个商业模式有问题

1461
01:22:15,847 --> 01:22:20,226
You could shut down the service and destroy whatever it is
‎你可以关掉服务 毁掉不管这是什么

1462
01:22:20,310 --> 01:22:24,522
$20 billion of shareholder value     and get sued and...
‎200亿美元的股东利益 被起诉…

1463
01:22:24,606 --> 01:22:27,108
But you can't, in practice, put the genie back in the bottle.
‎但现实是 覆水难收

1464
01:22:27,192 --> 01:22:30,403
You can make some tweaks, but at the end of the day,
‎你可以做出一些小的调整 但是最终

1465
01:22:30,487 --> 01:22:34,032
you've gotta grow revenue and usage, quarter over quarter. It's...
‎你要增加收益和使用 ‎每一个季度都要增加

1466
01:22:34,658 --> 01:22:37,535
The bigger it gets, the harder it is for anyone to change.
‎规模做得越大 越难让任何人改变

1467
01:22:38,495 --> 01:22:43,458
What I see is a bunch of people who are trapped by a business model,
‎我看到的是一群被困住的人 ‎被商业模式

1468
01:22:43,541 --> 01:22:46,169
an economic incentive, and shareholder pressure
‎经济奖励和股东压力困住

1469
01:22:46,252 --> 01:22:48,922
that makes it almost impossible to do something else.
‎几乎无法做其他的任何事情

1470
01:22:49,005 --> 01:22:50,924
I think we need to accept that it's okay
‎我认为我们应该接受

1471
01:22:51,007 --> 01:22:53,176
for companies to be focused on making money.
‎公司专注于挣钱 是合情合理的

1472
01:22:53,259 --> 01:22:55,637
What's not okay is when there's no regulation, no rules,
‎不合情合理的是 ‎当没有监管、没有规定、

1473
01:22:55,720 --> 01:22:56,888
and no competition,
没有竞争

1474
01:22:56,972 --> 01:23:00,850
and the companies are acting as sort of  de facto governments.
‎公司在充当实际政府部门

1475
01:23:00,934 --> 01:23:03,353
And then they're saying, "Well, we can regulate ourselves."
‎然后他们说“我们可以监管自己”

1476
01:23:03,436 --> 01:23:05,981
I mean, that's just a lie. That's just ridiculous.
‎这肯定是骗人的 怎么可能呢

1477
01:23:06,064 --> 01:23:08,650
Financial incentives kind of run the world,
‎经济奖励可以说运营着世界

1478
01:23:08,733 --> 01:23:12,529
so any solution to this problem
‎所以这个问题的任何解决方案

1479
01:23:12,612 --> 01:23:15,573
has to realign the financial incentives.
‎一定要符合经济奖励

1480
01:23:16,074 --> 01:23:18,785
There's no fiscal reason for these companies to change.
‎这些公司没有需要改变的财政理由

1481
01:23:18,868 --> 01:23:21,329
And that is why I think we need regulation.
‎所以我才认为 我们需要监管

1482
01:23:21,413 --> 01:23:24,290
The phone company has tons of sensitive data about you,
‎手机公司有无数的关于你的敏感数据

1483
01:23:24,374 --> 01:23:27,544
and we have a lot of laws that make sure they don't do the wrong things.
‎我们有很多法律去保证 ‎他们不会利用这些数据做错事

1484
01:23:27,627 --> 01:23:31,506
We have almost no laws around digital privacy, for example.
‎在数码隐私上 我们几乎没有立法

1485
01:23:31,589 --> 01:23:34,426
We could tax data collection and processing
‎我们可以对数据收集和处理收税

1486
01:23:34,509 --> 01:23:37,554
the same way that you, for example, pay your water bill
‎原理等同于你交水费

1487
01:23:37,637 --> 01:23:39,723
by monitoring the amount of water that you use.
‎监控你自己的用水量

1488
01:23:39,806 --> 01:23:43,226
You tax these companies on the data assets that they have.
‎让这些公司因为持有的数据资产交税

1489
01:23:43,309 --> 01:23:44,769
It gives them a fiscal reason
‎就能给他们一个财政理由

1490
01:23:44,853 --> 01:23:47,856
to not acquire every piece of data on the planet.
‎不去获取地球上每一条数据

1491
01:23:47,939 --> 01:23:50,567
The law runs way behind on these things,
‎立法在这方面太落后了

1492
01:23:50,650 --> 01:23:55,864
but what I know is the current situation exists not for the protection of users,
‎但据我所知 当前状况的存在 ‎不是为了保护用户

1493
01:23:55,947 --> 01:23:58,700
but for the protection of the rights and privileges
‎而是为了保护这些巨型的

1494
01:23:58,783 --> 01:24:01,453
of these gigantic, incredibly wealthy companies.
‎超级富有公司的权利和特权

1495
01:24:02,245 --> 01:24:05,832
Are we always gonna defer to the richest, most powerful people?
‎我们要一直听从最有钱 ‎最有权力的人吗？

1496
01:24:05,915 --> 01:24:07,417
Or are we ever gonna say,
‎还是我们要说

1497
01:24:07,959 --> 01:24:12,047
"You know, there are times when there is a national interest.
‎“有时候 确实是有国家利益

1498
01:24:12,130 --> 01:24:15,592
There are times when the interests of people, of users,
‎有时候 人民的利益 用户利益

1499
01:24:15,675 --> 01:24:17,385
is actually more important
‎其实比一个

1500
01:24:18,011 --> 01:24:21,473
than the profits of somebody who's already a billionaire"?
‎已经是亿万富翁的人的利益 ‎更加重要？”

1501
01:24:21,556 --> 01:24:26,603
These markets undermine democracy, and they undermine freedom,
‎这些市场削弱了民主 削弱了自由

1502
01:24:26,686 --> 01:24:28,521
and they should be outlawed.
‎应该对他们进行法律的制裁

1503
01:24:29,147 --> 01:24:31,816
This is not a radical proposal.
‎这不是激进的提议

1504
01:24:31,900 --> 01:24:34,194
There are other markets that we outlaw.
‎我们有法律制裁的其他市场

1505
01:24:34,277 --> 01:24:36,988
We outlaw markets in human organs.
‎我们制裁人类器官贩卖市场

1506
01:24:37,072 --> 01:24:39,491
We outlaw markets in human slaves.
‎我们制裁人类奴隶市场

1507
01:24:39,949 --> 01:24:44,037
Because they have inevitable destructive consequences.
‎因为它们都有不可避免的破坏性后果

1508
01:24:44,537 --> 01:24:45,830
We live in a world
‎我们生活的世界

1509
01:24:45,914 --> 01:24:50,001
in which a tree is worth more, financially, dead than alive,
‎死去的树比活着的树更有经济价值

1510
01:24:50,085 --> 01:24:53,838
in a world in which a whale is worth more dead than alive.
‎这个世界 死去的鲸 ‎比活着的鲸更有价值

1511
01:24:53,922 --> 01:24:56,341
For so long as our economy works in that way
‎只要我们的经济这样运转

1512
01:24:56,424 --> 01:24:58,134
and corporations go unregulated,
‎公司不受监管

1513
01:24:58,218 --> 01:25:00,678
they're going to continue to destroy trees,
‎它们就会继续破坏树木

1514
01:25:00,762 --> 01:25:01,763
to kill whales,
‎继续捕杀鲸

1515
01:25:01,846 --> 01:25:06,101
to mine the earth, and to continue to pull oil out of the ground,
‎在地球上挖矿 从地下抽石油

1516
01:25:06,184 --> 01:25:08,394
even though we know it is destroying the planet
‎虽然我们知道 这样做会破坏地球

1517
01:25:08,478 --> 01:25:12,148
and we know that it's going to leave a worse world for future generations.
‎我们知道 这样做会为未来几代人 ‎留下一个更不堪的世界

1518
01:25:12,232 --> 01:25:13,858
This is short  term thinking
‎这是目光短浅

1519
01:25:13,942 --> 01:25:16,694
based on this religion of profit at all costs,
‎为了利益牺牲一切的信仰

1520
01:25:16,778 --> 01:25:20,156
as if somehow, magically, each corporation acting in its selfish interest
‎指望每个 ‎只顾自己私利的公司会突然神奇地

1521
01:25:20,240 --> 01:25:21,950
is going to produce the best result.
‎去产生最好的结果

1522
01:25:22,033 --> 01:25:24,494
This has been affecting the environment for a long time.
‎这已经影响环境很久了

1523
01:25:24,577 --> 01:25:27,288
What's frightening, and what hopefully is the last straw
‎恐怖的是 希望这是 ‎压倒骆驼的最后一根稻草

1524
01:25:27,372 --> 01:25:29,207
that will make us wake up as a civilization
‎让我们作为文明的种族 去幡然醒悟

1525
01:25:29,290 --> 01:25:31,709
to how flawed this theory has been in the first place
‎这个理论最初就有很多缺点

1526
01:25:31,793 --> 01:25:35,004
is to see that now we're the tree, we're the whale.
‎让我们看到 我们现在就是树 就是鲸

1527
01:25:35,088 --> 01:25:37,048
Our attention can be mined.
‎我们的关注就是被挖掘的矿产

1528
01:25:37,132 --> 01:25:39,134
We are more profitable to a corporation
‎如果我们花时间盯着一个屏幕

1529
01:25:39,217 --> 01:25:41,594
if we're spending time staring at a screen,
‎盯着一个广告 对公司而言

1530
01:25:41,678 --> 01:25:42,971
staring at an ad,
‎比我们用这个时间

1531
01:25:43,054 --> 01:25:45,890
than if we're spending that time living our life in a rich way.
‎过自己丰富的生活 更加有利可图

1532
01:25:45,974 --> 01:25:47,559
And so, we're seeing the results of that.
‎我们看到了这样的后果

1533
01:25:47,642 --> 01:25:50,687
We're seeing corporations using powerful artificial intelligence
‎我们看到公司利用强大的人工智能

1534
01:25:50,770 --> 01:25:53,648
to outsmart us and figure out how to pull our attention
‎凌驾在我们的智能之上 ‎研究怎样拉拢我们的关注

1535
01:25:53,731 --> 01:25:55,358
toward the things they want us to look at,
‎让我们去看 他们想让我们看的东西

1536
01:25:55,441 --> 01:25:57,277
rather than the things that are most consistent
‎而不是让我们看 ‎与我们目标、价值观

1537
01:25:57,360 --> 01:25:59,237
with our goals and our values and our lives.
‎与我们的生活最为一致的东西

1538
01:26:02,991 --> 01:26:04,450
‎（史蒂夫·乔布斯 今日演讲者）

1539
01:26:05,535 --> 01:26:06,911
What a computer is,
‎电脑对我来说 是…

1540
01:26:06,995 --> 01:26:10,290
is it's the most remarkable tool that we've ever come up with.
‎是我们人类历史上 最神奇的发明

1541
01:26:11,124 --> 01:26:13,877
And it's the equivalent of a bicycle for our minds.
‎相当于是我们思想的自行车

1542
01:26:15,628 --> 01:26:20,091
The idea of humane technology, that's where Silicon Valley got its start.
‎人道技术的创想 是硅谷最初的目标

1543
01:26:21,050 --> 01:26:25,722
And we've lost sight of it because it became the cool thing to do,
‎我们已经背离了这个目标 ‎因为这样做比较酷

1544
01:26:25,805 --> 01:26:27,265
as opposed to the right thing to do.
‎而不是这样做比较正确

1545
01:26:27,348 --> 01:26:29,726
The Internet was, like, a weird, wacky place.
‎网络就是一个奇怪的、可笑的地方

1546
01:26:29,809 --> 01:26:31,394
It was experimental.
‎它是实验性的

1547
01:26:31,477 --> 01:26:34,731
Creative things happened on the Internet, and certainly, they do still,
‎网络上发生着有创意的事情 ‎当然现在也有发生

1548
01:26:34,814 --> 01:26:38,610
but, like, it just feels like this, like, giant mall.
‎但是感觉像一个巨大的商场

1549
01:26:38,693 --> 01:26:42,071
You know, it's just like, "God, there's gotta be...
‎就是“天啊 ‎

1550
01:26:42,155 --> 01:26:44,157
there's gotta be more to it than that."
肯定不止表面上这么简单”

1551
01:26:46,659 --> 01:26:48,411
I guess I'm just an optimist.
‎我想我只是一个乐观主义者

1552
01:26:48,494 --> 01:26:52,040
'Cause I think we can change what social media looks like and means.
‎因为我认为 ‎我们可以改变社交媒体的样子和方式

1553
01:26:54,083 --> 01:26:56,711
The way the technology works is not a law of physics.
‎技术的工作方式不是物理学定律 ‎

1554
01:26:56,794 --> 01:26:57,921
It is not set in stone.
它不是一成不变的

1555
01:26:58,004 --> 01:27:02,175
These are choices that human beings like myself have been making.
‎这些都是像我这样的人类 ‎做出的选择

1556
01:27:02,759 --> 01:27:05,345
And human beings can change those technologies.
‎人类可以改变这些技术

1557
01:27:06,971 --> 01:27:09,974
And the question now is whether or not we're willing to admit
‎现在的问题是 我们是否愿意承认

1558
01:27:10,475 --> 01:27:15,438
that those bad outcomes are coming directly as a product of our work.
‎这些后果 是我们杰作的直接产物

1559
01:27:21,027 --> 01:27:24,864
It's that we built these things, and we have a responsibility to change it.
‎这些东西是我们建立起来的 ‎我们有责任去改变它们

1560
01:27:37,085 --> 01:27:38,711
The attention extraction model
‎提取关注模型 ‎

1561
01:27:38,795 --> 01:27:42,298
is not how we want to treat human beings.
不是我们想对待人类的方式

1562
01:27:45,343 --> 01:27:48,137
Is it just me or...
‎只有我这样想吗？还是…

1563
01:27:49,722 --> 01:27:51,099
Poor sucker.
‎可悲的人

1564
01:27:51,516 --> 01:27:53,226
The fabric of a healthy society
‎一个健康社会的结构

1565
01:27:53,309 --> 01:27:56,145
depends on us getting off this corrosive business model.
‎要依靠我们脱离这种 ‎有破坏性的商业模型

1566
01:28:04,696 --> 01:28:08,157
We can demand that these products be designed humanely.
‎我们可以要求 ‎这些产品进行人道设计

1567
01:28:09,409 --> 01:28:13,121
We can demand to not be treated as an extractable resource.
‎我们可以要求 ‎不被当做可以提取的资源对待

1568
01:28:15,164 --> 01:28:18,334
The intention could be: "How do we make the world better? "
‎我们的动机可以是 ‎“我们怎样让这个世界变得更好？”

1569
01:28:20,336 --> 01:28:21,504
Throughout history,
‎在整个人类历史中

1570
01:28:21,587 --> 01:28:23,798
every single time something's gotten better,
‎每一次有事物变得更好

1571
01:28:23,881 --> 01:28:26,342
it's because somebody has come along to say,
‎都是因为有人站出来说

1572
01:28:26,426 --> 01:28:28,428
"This is stupid. We can do better."
‎“这太蠢了 我们可以做得更好”

1573
01:28:29,178 --> 01:28:32,557
Like, it's the critics that drive improvement.
‎是批判者驱动着改进

1574
01:28:33,141 --> 01:28:35,393
It's the critics who are the true optimists.
‎批判者才是真正的乐观主义者

1575
01:28:37,020 --> 01:28:39,147
Hello.
‎你好

1576
01:28:46,195 --> 01:28:47,697
I mean, it seems kind of crazy, right?
‎感觉有点疯狂 是吧？

1577
01:28:47,780 --> 01:28:51,534
It's like the fundamental way that this stuff is designed...
‎这东西的设计基本方式

1578
01:28:52,994 --> 01:28:55,163
isn't going in a good direction.
‎就不会朝着好的方向发展

1579
01:28:55,246 --> 01:28:56,873
Like, the entire thing.
‎整个社交媒体

1580
01:28:56,956 --> 01:29:00,626
So, it sounds crazy to say we need to change all that,
‎我说要改变这一切 ‎听起来有点疯狂

1581
01:29:01,169 --> 01:29:02,670
but that's what we need to do.
‎但我们需要这样做

1582
01:29:04,297 --> 01:29:05,923
Think we're gonna get there?
‎你觉得这一天能实现吗？

1583
01:29:07,383 --> 01:29:08,301
We have to.
‎必须要实现

1584
01:29:20,646 --> 01:29:24,942
Um, it seems like you're very optimistic.
‎你似乎非常乐观

1585
01:29:26,194 --> 01:29:27,570
Is that how I sound?
‎听起来是这样吗？

1586
01:29:27,653 --> 01:29:28,905
Yeah, I mean...
‎是 我是说… ‎我无法相信你一直这样说

1587
01:29:28,988 --> 01:29:31,449
I can't believe you keep saying that, because I'm like, "Really?
‎因为我在想：“真的吗？”

1588
01:29:31,532 --> 01:29:33,409
I feel like we're headed toward dystopia.
我感觉 ‎我们正走向毁灭而不是乌托邦

1589
01:29:33,493 --> 01:29:35,328
I feel like we're on the fast track to dystopia,
‎我感觉我们正在飞速走向毁灭

1590
01:29:35,411 --> 01:29:37,830
and it's gonna take a miracle to get us out of it."
‎需要一个奇迹 ‎才能让我们走下这条路“

1591
01:29:37,914 --> 01:29:40,291
And that miracle is, of course, collective will.
‎这个奇迹当然是集体意识

1592
01:29:41,000 --> 01:29:44,587
I am optimistic that we're going to figure it out,
‎可我是乐观主义者 ‎我们一定会有办法解决

1593
01:29:44,670 --> 01:29:47,048
but I think it's gonna take a long time.
‎但我认为 可能会用很久的时间

1594
01:29:47,131 --> 01:29:50,385
Because not everybody recognizes that this is a problem.
‎因为不是所有的人都意识到了 ‎这是一个问题

1595
01:29:50,468 --> 01:29:55,890
I think one of the big failures in technology today
‎我认为当今技术最大的一个失败

1596
01:29:55,973 --> 01:29:58,643
is a real failure of leadership,
‎是领导力的真正失败

1597
01:29:58,726 --> 01:30:01,979
of, like, people coming out and having these open conversations
‎人们站出来 去公开讨论

1598
01:30:02,063 --> 01:30:05,900
about things that... not just what went well, but what isn't perfect
‎不仅是哪些地方进行得好 ‎还应该讨论哪里不完美

1599
01:30:05,983 --> 01:30:08,194
so that someone can come in and build something new.
‎才能让有人介入 构建一些新的东西

1600
01:30:08,277 --> 01:30:10,321
At the end of the day, you know,
‎最终 你知道

1601
01:30:10,405 --> 01:30:14,617
this machine isn't gonna turn around until there's massive public pressure.
‎在有足够的公众压力之前 ‎这台机器是绝对不会回头的

1602
01:30:14,700 --> 01:30:18,329
By having these conversations and... and voicing your opinion,
‎通过这些对话 发出你的声音

1603
01:30:18,413 --> 01:30:21,082
in some cases through these very technologies,
‎在一些情况下 通过某些特定的技术

1604
01:30:21,165 --> 01:30:24,252
we can start to change the tide. We can start to change the conversation.
‎我们可以开始改变趋势 ‎我们可以开始改变对话

1605
01:30:24,335 --> 01:30:27,004
It might sound strange, but it's my world. It's my community.
‎听起来可能有点奇怪 ‎但这是我的世界 是我生活的环境

1606
01:30:27,088 --> 01:30:29,632
I don't hate them. I don't wanna do any harm to Google or Facebook.
‎我不恨他们 ‎我不想伤害谷歌或者脸书

1607
01:30:29,715 --> 01:30:32,885
I just want to reform them so they don't destroy the world. You know?
‎我只是想改革它们 ‎别让他们毁了世界 你知道吗？

1608
01:30:32,969 --> 01:30:35,513
I've uninstalled a ton of apps from my phone
‎我在手机上卸载了很多程序

1609
01:30:35,596 --> 01:30:37,723
that I felt were just wasting my time.
‎我感觉那些都是浪费时间

1610
01:30:37,807 --> 01:30:40,685
All the social media apps, all the news apps,
‎所有的社交媒体程序 所有的新程序

1611
01:30:40,768 --> 01:30:42,520
and I've turned off notifications
‎我关掉了通知

1612
01:30:42,603 --> 01:30:45,815
on anything that was vibrating my leg with information
‎所有那些让我手机震动的通知

1613
01:30:45,898 --> 01:30:48,943
that wasn't timely and important to me right now.
‎不够及时 对现在我来说 ‎并不重要的信息

1614
01:30:49,026 --> 01:30:51,279
It's for the same reason I don't keep cookies in my pocket.
‎也正是因为同样的理由 ‎我兜里不放饼干

1615
01:30:51,362 --> 01:30:53,197
Reduce the number of notifications you get.
‎减少你收到的通知数量

1616
01:30:53,281 --> 01:30:54,449
Turn off notifications.
‎关掉通知

1617
01:30:54,532 --> 01:30:55,950
Turning off all notifications.
‎关掉所有应用的通知

1618
01:30:56,033 --> 01:30:58,536
I'm not using Google anymore, I'm using Qwant,
‎我已经不再用谷歌了 ‎我用Qwant搜索引擎

1619
01:30:58,619 --> 01:31:01,497
which doesn't store your search history.
‎这个引擎不会存储你的搜索历史

1620
01:31:01,581 --> 01:31:04,459
Never accept a video recommended to you on YouTube.
‎永远不要接受 ‎YouTube上给你推荐的视频

1621
01:31:04,542 --> 01:31:07,003
Always choose. That's another way to fight.
‎永远自己去选择 ‎这是另一个抗争的方式

1622
01:31:07,086 --> 01:31:12,133
There are tons of Chrome extensions that remove recommendations.
‎谷歌浏览器有无数扩展程序 ‎可以移走推荐

1623
01:31:12,216 --> 01:31:15,178
You're recommending something to undo what you made.
‎我很喜欢你推荐一个 ‎撤销你所做东西的东西

1624
01:31:15,261 --> 01:31:16,554
Yep.
‎对

1625
01:31:16,929 --> 01:31:21,642
Before you share, fact  check, consider the source, do that extra Google.
‎在你分享之前 查找一下事实 ‎思考一下信息来源 谷歌搜索一下

1626
01:31:21,726 --> 01:31:25,104
If it seems like it's something designed to really push your emotional buttons,
‎如果这个东西感觉像是 ‎以触发你的情感按钮为目标

1627
01:31:25,188 --> 01:31:26,314
like, it probably is.
‎很可能确实是

1628
01:31:26,397 --> 01:31:29,025
Essentially, you vote with your clicks.
‎基本可以说 你用点击去投票

1629
01:31:29,108 --> 01:31:30,359
If you click on clickbait,
‎如果你点击了钓鱼链接

1630
01:31:30,443 --> 01:31:33,779
you're creating a financial incentive that perpetuates this existing system.
‎你就是在创造一个经济奖励 ‎延续这个已经存在的体系

1631
01:31:33,863 --> 01:31:36,949
Make sure that you get lots of different kinds of information
‎在你的生活中 一定要获得

1632
01:31:37,033 --> 01:31:37,909
in your own life.
‎各种不同的信息

1633
01:31:37,992 --> 01:31:40,995
I follow people on Twitter that I disagree with
‎我会在推特上关注我不认同的人

1634
01:31:41,078 --> 01:31:44,207
because I want to be exposed to different points of view.
‎因为我想看到不同的观点

1635
01:31:44,665 --> 01:31:46,584
Notice that many people in the tech industry
‎要知道 技术行业中的很多人

1636
01:31:46,667 --> 01:31:49,045
don't give these devices to their own children.
‎不会把这些设备给他们自己的小孩用

1637
01:31:49,128 --> 01:31:51,047
My kids don't use social media at all.
‎我的孩子们完全不使用社交媒体

1638
01:31:51,839 --> 01:31:53,549
Is that a rule, or is that a...
‎这是规定 还是…

1639
01:31:53,633 --> 01:31:54,509
That's a rule.
‎家规

1640
01:31:55,092 --> 01:31:57,845
We are zealots about it.
‎我们对它很狂热

1641
01:31:57,929 --> 01:31:59,222
We're... We're crazy.
‎我们很疯狂

1642
01:31:59,305 --> 01:32:05,603
And we don't let our kids have really any screen time.
‎我们不会让我们的孩子 ‎拥有任何看屏幕的时间

1643
01:32:05,686 --> 01:32:08,564
I've worked out what I think are three simple rules, um,
‎我想出了 ‎我自己认为的三个简单原则

1644
01:32:08,648 --> 01:32:12,610
that make life a lot easier for families and that are justified by the research.
‎能让生活对家人来说更容易 ‎这是经过研究验证的

1645
01:32:12,693 --> 01:32:15,571
So, the first rule is all devices out of the bedroom
‎第一个原则是 ‎在每晚的固定时间 所有设备

1646
01:32:15,655 --> 01:32:17,281
at a fixed time every night.
‎不能进入卧室

1647
01:32:17,365 --> 01:32:20,535
Whatever the time is, half an hour before bedtime, all devices out.
‎不管是什么时间 睡前半小时 ‎所有设备全都拿出去

1648
01:32:20,618 --> 01:32:24,038
The second rule is no social media until high school.
‎第二个原则是 ‎高中之前禁止使用社交媒体

1649
01:32:24,121 --> 01:32:26,374
Personally, I think the age should be 16.
‎我个人认为 这个年龄应该是16岁

1650
01:32:26,457 --> 01:32:28,960
Middle school's hard enough. Keep it out until high school.
‎初中已经够难了 上高中之前别用了

1651
01:32:29,043 --> 01:32:32,964
And the third rule is work out a time budget with your kid.
‎第三个原则是 ‎和你的孩子研究出一个时间预算

1652
01:32:33,047 --> 01:32:34,757
And if you talk with them and say,
‎如果你和他们聊 去说

1653
01:32:34,840 --> 01:32:37,927
"Well, how many hours a day do you wanna spend on your device?
‎“你每天想在你的设备上花多少时间

1654
01:32:38,010 --> 01:32:39,637
What do you think is a good amount? "
‎你觉得适合的时长是多少”

1655
01:32:39,720 --> 01:32:41,597
they'll often say something pretty reasonable.
‎他们通常会说出一个很合理的时长

1656
01:32:42,056 --> 01:32:44,642
Well, look, I know perfectly well
‎看 我非常清楚

1657
01:32:44,725 --> 01:32:48,563
that I'm not gonna get everybody to delete their social media accounts,
‎我无法让所有人删除社交媒体账号

1658
01:32:48,646 --> 01:32:50,439
but I think I can get a few.
‎但我想我可以让几个人这样做

1659
01:32:50,523 --> 01:32:54,402
And just getting a few people to delete their accounts matters a lot,
‎让几个人删除账号 ‎就已经能产生很大影响了

1660
01:32:54,485 --> 01:32:58,406
and the reason why is that that creates the space for a conversation
‎理由是 这样能创造一个对话的空间

1661
01:32:58,489 --> 01:33:00,908
because I want there to be enough people out in the society
‎因为我想让社会中有足够的人

1662
01:33:00,992 --> 01:33:05,204
who are free of the manipulation engines to have a societal conversation
‎这些人不受到引擎的操纵 ‎能够进行社交对话

1663
01:33:05,288 --> 01:33:07,540
that isn't bounded by the manipulation engines.
‎没有受到操纵引擎的牵制

1664
01:33:07,623 --> 01:33:10,126
So, do it! Get out of the system.
‎这样做吧！退出这个体系

1665
01:33:10,209 --> 01:33:12,503
Yeah, delete. Get off the stupid stuff.
‎对 删掉 下线这个愚蠢的东西

1666
01:33:13,546 --> 01:33:16,507
The world's beautiful. Look. Look, it's great out there.
‎世界很美丽 你们看 ‎外面的世界很美好


