
00:00:15.840 --> 00:00:17.920
[eerie instrumental music playing]

00:00:31.120 --> 00:00:34.640
[interviewer] Why don't you go ahead?
Sit down and see if you can get comfy.

00:00:37.560 --> 00:00:39.800
-You good? All right.
-Yeah. [exhales]

00:00:39.920 --> 00:00:42.120
-[interviewer] Um...
-[cell phone vibrates]

00:00:43.040 --> 00:00:44.800
[crew member] Take one, marker.

00:00:46.800 --> 00:00:48.800
[interviewer] Wanna start
by introducing yourself?

00:00:48.880 --> 00:00:49.800
[crew member coughs]

00:00:50.480 --> 00:00:53.320
Hello, world. Bailey. Take three.

00:00:53.960 --> 00:00:56.360
-[interviewer] You good?
-This is the worst part, man.

00:00:56.880 --> 00:00:59.520
[chuckling] I don't like this.

00:00:59.840 --> 00:01:02.240
I worked at Facebook in 2011 and 2012.

00:01:02.320 --> 00:01:05.200
I was one of the really early employees
at Instagram.

00:01:05.280 --> 00:01:08.680
[man 1] I worked at, uh, Google,
uh, YouTube.

00:01:08.760 --> 00:01:11.680
[woman] Apple, Google, Twitter, Palm.

00:01:12.720 --> 00:01:15.520
I helped start Mozilla Labs
and switched over to the Firefox side.

00:01:15.600 --> 00:01:18.120
-[interviewer] Are we rolling? Everybody?
-[crew members reply]

00:01:18.200 --> 00:01:19.160
[interviewer] Great.

00:01:21.200 --> 00:01:22.600
[man 2] I worked at Twitter.

00:01:23.040 --> 00:01:23.920
My last job there

00:01:24.000 --> 00:01:26.160
was the senior vice president
of engineering.

00:01:27.320 --> 00:01:29.240
-[man 3] I was the president of Pinterest.
-[sips]

00:01:29.320 --> 00:01:32.720
Before that, um,
I was the... the director of monetization

00:01:32.800 --> 00:01:34.240
at Facebook for five years.

00:01:34.320 --> 00:01:37.960
While at Twitter, I spent a number
of years running their developer platform,

00:01:38.040 --> 00:01:40.200
and then I became
head of consumer product.

00:01:40.320 --> 00:01:44.280
I was the coinventor of Google Drive,
Gmail Chat,

00:01:44.360 --> 00:01:46.680
Facebook Pages,
and the Facebook like button.

00:01:47.440 --> 00:01:50.760
Yeah. This is... This is why I spent,
like, eight months

00:01:50.840 --> 00:01:52.760
talking back and forth with lawyers.

00:01:54.080 --> 00:01:55.400
This freaks me out.

00:01:58.400 --> 00:01:59.680
[man 2] When I was there,

00:01:59.800 --> 00:02:02.920
I always felt like,
fundamentally, it was a force for good.

00:02:03.400 --> 00:02:05.360
I don't know if I feel that way anymore.

00:02:05.440 --> 00:02:10.600
I left Google in June 2017, uh,
due to ethical concerns.

00:02:10.680 --> 00:02:14.120
And... And not just at Google
but within the industry at large.

00:02:14.200 --> 00:02:15.360
I'm very concerned.

00:02:16.640 --> 00:02:17.680
I'm very concerned.

00:02:19.080 --> 00:02:21.800
It's easy today to lose sight of the fact

00:02:21.880 --> 00:02:27.800
that these tools actually have created
some wonderful things in the world.

00:02:27.880 --> 00:02:31.920
They've reunited lost family members.
They've found organ donors.

00:02:32.040 --> 00:02:36.560
I mean, there were meaningful,
systemic changes happening

00:02:36.640 --> 00:02:39.160
around the world
because of these platforms

00:02:39.240 --> 00:02:40.280
that were positive!

00:02:40.840 --> 00:02:44.520
I think we were naive
about the flip side of that coin.

00:02:45.520 --> 00:02:48.560
Yeah, these things, you release them,
and they take on a life of their own.

00:02:48.680 --> 00:02:52.000
And how they're used is pretty different
than how you expected.

00:02:52.080 --> 00:02:56.520
Nobody, I deeply believe,
ever intended any of these consequences.

00:02:56.600 --> 00:02:59.560
There's no one bad guy.
No. Absolutely not.

00:03:01.600 --> 00:03:03.960
[interviewer] So, then,
what's the... what's the problem?

00:03:09.160 --> 00:03:11.480
[interviewer] Is there a problem,
and what is the problem?

00:03:12.120 --> 00:03:13.040
[swallows]

00:03:17.600 --> 00:03:20.000
[clicks tongue] Yeah, it is hard
to give a single, succinct...

00:03:20.080 --> 00:03:22.120
I'm trying to touch on
many different problems.

00:03:22.520 --> 00:03:23.960
[interviewer] What is the problem?

00:03:24.600 --> 00:03:25.920
[clicks tongue, chuckles]

00:03:27.920 --> 00:03:29.480
[birds singing]

00:03:31.160 --> 00:03:32.680
[dog barking in distance]

00:03:33.440 --> 00:03:35.320
[reporter 1]
Despite facing mounting criticism,

00:03:35.400 --> 00:03:37.680
the so-called Big Tech names
are getting bigger.

00:03:37.760 --> 00:03:40.920
The entire tech industry is
under a new level of scrutiny.

00:03:41.000 --> 00:03:43.800
And a new study sheds light on the link

00:03:43.880 --> 00:03:46.120
between mental health
and social media use.

00:03:46.240 --> 00:03:48.680
[on TV]
Here to talk about the latest research...

00:03:48.760 --> 00:03:51.400
[Tucker Carlson] ...is going on
that gets no coverage at all.

00:03:51.480 --> 00:03:54.120
Tens of millions of Americans
are hopelessly addicted

00:03:54.200 --> 00:03:56.320
to their electronic devices.

00:03:56.400 --> 00:03:58.000
[reporter 2] It's exacerbated by the fact

00:03:58.080 --> 00:04:00.680
that you can literally isolate yourself
now

00:04:00.760 --> 00:04:02.720
in a bubble, thanks to our technology.

00:04:02.840 --> 00:04:04.560
Fake news is becoming more advanced

00:04:04.640 --> 00:04:06.800
and threatening societies
around the world.

00:04:06.880 --> 00:04:10.240
We weren't expecting any of this
when we created Twitter over 12 years ago.

00:04:10.320 --> 00:04:12.480
White House officials say
they have no reason to believe

00:04:12.560 --> 00:04:14.760
the Russian cyberattacks will stop.

00:04:14.840 --> 00:04:18.120
YouTube is being forced
to concentrate on cleansing the site.

00:04:18.200 --> 00:04:21.560
[reporter 3] TikTok,
if you talk to any tween out there...

00:04:21.640 --> 00:04:24.000
[on TV] ...there's no chance
they'll delete this thing...

00:04:24.080 --> 00:04:26.200
Hey, Isla,
can you get the table ready, please?

00:04:26.320 --> 00:04:28.600
[reporter 4] There's a question
about whether social media

00:04:28.680 --> 00:04:29.960
is making your child depressed.

00:04:30.040 --> 00:04:32.080
[mom] Isla,
can you set the table, please?

00:04:32.200 --> 00:04:35.320
[reporter 5] These cosmetic procedures
are becoming so popular with teens,

00:04:35.400 --> 00:04:37.880
plastic surgeons have coined
a new syndrome for it,

00:04:38.000 --> 00:04:40.800
"Snapchat dysmorphia,"
with young patients wanting surgery

00:04:40.880 --> 00:04:43.720
so they can look more like they do
in filtered selfies.

00:04:43.800 --> 00:04:45.920
Still don't see why you let her have
that thing.

00:04:46.000 --> 00:04:47.400
What was I supposed to do?

00:04:47.480 --> 00:04:49.560
I mean, every other kid
in her class had one.

00:04:50.160 --> 00:04:51.160
She's only 11.

00:04:51.240 --> 00:04:52.960
Cass, no one's forcing you to get one.

00:04:53.040 --> 00:04:55.080
You can stay disconnected
as long as you want.

00:04:55.160 --> 00:04:59.320
Hey, I'm connected without a cell phone,
okay? I'm on the Internet right now.

00:04:59.400 --> 00:05:03.080
Also, that isn't even actual connection.
It's just a load of sh--

00:05:03.160 --> 00:05:05.000
Surveillance capitalism has come to shape

00:05:05.080 --> 00:05:07.760
our politics and culture
in ways many people don't perceive.

00:05:07.840 --> 00:05:10.080
[reporter 6]
ISIS inspired followers online,

00:05:10.160 --> 00:05:12.800
and now white supremacists
are doing the same.

00:05:12.880 --> 00:05:14.160
Recently in India,

00:05:14.240 --> 00:05:17.440
Internet lynch mobs have killed
a dozen people, including these five...

00:05:17.520 --> 00:05:20.360
[reporter 7] It's not just fake news;
it's fake news with consequences.

00:05:20.440 --> 00:05:24.080
[reporter 8] How do you handle an epidemic
in the age of fake news?

00:05:24.160 --> 00:05:27.000
Can you get the coronavirus
by eating Chinese food?

00:05:27.520 --> 00:05:32.520
We have gone from the information age
into the disinformation age.

00:05:32.600 --> 00:05:34.680
Our democracy is under assault.

00:05:34.760 --> 00:05:36.920
[man 4] What I said was,
"I think the tools

00:05:37.000 --> 00:05:39.000
that have been created today are starting

00:05:39.080 --> 00:05:41.800
to erode the social fabric
of how society works."

00:05:41.880 --> 00:05:44.440
[eerie instrumental music continues]

00:05:55.960 --> 00:05:58.480
-[music fades]
-[indistinct chatter]

00:05:58.560 --> 00:05:59.440
[crew member] Fine.

00:06:00.160 --> 00:06:03.440
[stage manager] Aza does
welcoming remarks. We play the video.

00:06:04.200 --> 00:06:07.320
And then, "Ladies and gentlemen,
Tristan Harris."

00:06:07.400 --> 00:06:08.880
-Right.
-[stage manager] Great.

00:06:08.960 --> 00:06:12.040
So, I come up, and...

00:06:13.840 --> 00:06:17.120
basically say, "Thank you all for coming."
Um...

00:06:17.920 --> 00:06:22.040
So, today, I wanna talk about a new agenda
for technology.

00:06:22.120 --> 00:06:25.480
And why we wanna do that
is because if you ask people,

00:06:25.560 --> 00:06:27.800
"What's wrong in the tech industry
right now?"

00:06:28.240 --> 00:06:31.640
there's a cacophony of grievances
and scandals,

00:06:31.720 --> 00:06:33.880
and "They stole our data."
And there's tech addiction.

00:06:33.960 --> 00:06:35.960
And there's fake news.
And there's polarization

00:06:36.040 --> 00:06:37.840
and some elections
that are getting hacked.

00:06:38.200 --> 00:06:41.600
But is there something
that is beneath all these problems

00:06:41.680 --> 00:06:44.600
that's causing all these things
to happen at once?

00:06:44.800 --> 00:06:46.360
[stage manager speaking indistinctly]

00:06:46.440 --> 00:06:48.400
-Does this feel good?
-Very good. Yeah.

00:06:49.040 --> 00:06:50.000
Um... [sighs]

00:06:50.720 --> 00:06:52.960
I'm just trying to...
Like, I want people to see...

00:06:53.040 --> 00:06:55.120
Like, there's a problem happening
in the tech industry,

00:06:55.200 --> 00:06:56.720
and it doesn't have a name,

00:06:56.800 --> 00:07:00.200
and it has to do with one source,
like, one...

00:07:00.800 --> 00:07:03.600
[eerie instrumental music playing]

00:07:05.080 --> 00:07:09.400
[Tristan] When you look around you,
it feels like the world is going crazy.

00:07:12.760 --> 00:07:15.320
You have to ask yourself, like,
"Is this normal?

00:07:16.080 --> 00:07:18.760
Or have we all fallen under some kind
of spell?"

00:07:28.000 --> 00:07:30.480
I wish more people could understand
how this works

00:07:30.560 --> 00:07:34.040
because it shouldn't be something
that only the tech industry knows.

00:07:34.120 --> 00:07:36.240
It should be something
that everybody knows.

00:07:36.320 --> 00:07:38.720
[backpack zips]

00:07:41.400 --> 00:07:42.360
[softly] Bye.

00:07:43.640 --> 00:07:44.880
[guard] Here you go, sir.

00:07:47.360 --> 00:07:48.680
-[employee] Hello!
-[Tristan] Hi.

00:07:48.760 --> 00:07:50.680
-Tristan. Nice to meet you.
-It's Tris-tan, right?

00:07:50.760 --> 00:07:51.720
-Yes.
-Awesome. Cool.

00:07:53.160 --> 00:07:55.920
[presenter] Tristan Harris
is a former design ethicist for Google

00:07:56.000 --> 00:07:59.400
and has been called the closest thing
Silicon Valley has to a conscience.

00:07:59.480 --> 00:08:00.720
[reporter] He's asking tech

00:08:00.800 --> 00:08:04.200
to bring what he calls "ethical design"
to its products.

00:08:04.280 --> 00:08:06.880
[Anderson Cooper] It's rare
for a tech insider to be so blunt,

00:08:07.000 --> 00:08:10.120
but Tristan Harris believes
someone needs to be.

00:08:11.320 --> 00:08:12.680
[Tristan] When I was at Google,

00:08:12.760 --> 00:08:16.040
I was on the Gmail team,
and I just started getting burnt out

00:08:16.120 --> 00:08:18.360
'cause we'd had
so many conversations about...

00:08:19.440 --> 00:08:23.160
you know, what the inbox should look like
and what color it should be, and...

00:08:23.240 --> 00:08:25.880
And I, you know, felt personally addicted
to e-mail,

00:08:26.280 --> 00:08:27.640
and I found it fascinating

00:08:27.720 --> 00:08:31.520
there was no one at Gmail working
on making it less addictive.

00:08:31.960 --> 00:08:34.520
And I was like,
"Is anybody else thinking about this?

00:08:34.600 --> 00:08:36.400
I haven't heard anybody talk about this."

00:08:36.840 --> 00:08:39.680
-And I was feeling this frustration...
-[sighs]

00:08:39.760 --> 00:08:41.240
...with the tech industry, overall,

00:08:41.320 --> 00:08:43.160
that we'd kind of, like, lost our way.

00:08:43.240 --> 00:08:46.440
-[ominous instrumental music playing]
-[message alerts chiming]

00:08:46.800 --> 00:08:49.800
[Tristan] You know, I really struggled
to try and figure out

00:08:49.880 --> 00:08:52.560
how, from the inside, we could change it.

00:08:52.920 --> 00:08:55.120
[energetic piano music playing]

00:08:55.200 --> 00:08:58.120
[Tristan] And that was when I decided
to make a presentation,

00:08:58.200 --> 00:08:59.480
kind of a call to arms.

00:09:01.000 --> 00:09:04.960
Every day, I went home and I worked on it
for a couple hours every single night.

00:09:05.040 --> 00:09:06.080
[typing]

00:09:06.160 --> 00:09:08.560
[Tristan] It basically just said,
you know,

00:09:08.640 --> 00:09:11.880
never before
in history have 50 designers--

00:09:12.440 --> 00:09:15.240
20- to 35-year-old white guys
in California--

00:09:15.880 --> 00:09:19.720
made decisions that would have an impact
on two billion people.

00:09:21.000 --> 00:09:24.440
Two billion people will have thoughts
that they didn't intend to have

00:09:24.520 --> 00:09:28.400
because a designer at Google said,
"This is how notifications work

00:09:28.480 --> 00:09:30.760
on that screen that you wake up to
in the morning."

00:09:31.200 --> 00:09:35.280
And we have a moral responsibility,
as Google, for solving this problem.

00:09:36.080 --> 00:09:37.720
And I sent this presentation

00:09:37.840 --> 00:09:41.800
to about 15, 20 of my closest colleagues
at Google,

00:09:41.880 --> 00:09:44.960
and I was very nervous about it.
I wasn't sure how it was gonna land.

00:09:46.440 --> 00:09:48.040
When I went to work the next day,

00:09:48.120 --> 00:09:50.440
most of the laptops
had the presentation open.

00:09:52.120 --> 00:09:54.560
Later that day, there was, like,
400 simultaneous viewers,

00:09:54.640 --> 00:09:56.040
so it just kept growing and growing.

00:09:56.120 --> 00:10:00.280
I got e-mails from all around the company.
I mean, people in every department saying,

00:10:00.360 --> 00:10:02.840
"I totally agree."
"I see this affecting my kids."

00:10:02.920 --> 00:10:04.960
"I see this affecting
the people around me."

00:10:05.040 --> 00:10:06.920
"We have to do something about this."

00:10:07.480 --> 00:10:10.800
It felt like I was sort of launching
a revolution or something like that.

00:10:11.840 --> 00:10:15.200
Later, I found out Larry Page
had been notified about this presentation

00:10:15.280 --> 00:10:17.920
-in three separate meetings that day.
-[indistinct chatter]

00:10:18.000 --> 00:10:20.280
[Tristan] And so, it created
this kind of cultural moment

00:10:20.880 --> 00:10:24.400
-that Google needed to take seriously.
-[whooshing]

00:10:26.000 --> 00:10:28.880
-[Tristan] And then... nothing.
-[whooshing fades]

00:10:32.680 --> 00:10:34.200
[message alerts chiming]

00:10:34.280 --> 00:10:36.120
[Tim] Everyone in 2006...

00:10:37.200 --> 00:10:39.200
including all of us at Facebook,

00:10:39.280 --> 00:10:43.400
just had total admiration for Google
and what Google had built,

00:10:43.480 --> 00:10:47.400
which was this incredibly useful service

00:10:47.480 --> 00:10:51.440
that did, far as we could tell,
lots of goodness for the world,

00:10:51.520 --> 00:10:54.680
and they built
this parallel money machine.

00:10:55.400 --> 00:11:00.040
We had such envy for that,
and it seemed so elegant to us...

00:11:00.840 --> 00:11:02.160
and so perfect.

00:11:02.960 --> 00:11:05.280
Facebook had been around
for about two years,

00:11:05.360 --> 00:11:08.360
um, and I was hired to come in
and figure out

00:11:08.440 --> 00:11:10.600
what the business model was gonna be
for the company.

00:11:10.680 --> 00:11:13.400
I was the director of monetization.
The point was, like,

00:11:13.520 --> 00:11:17.040
"You're the person who's gonna figure out
how this thing monetizes."

00:11:17.120 --> 00:11:19.800
And there were a lot of people
who did a lot of the work,

00:11:19.880 --> 00:11:25.480
but I was clearly one of the people
who was pointing towards...

00:11:26.760 --> 00:11:28.560
"Well, we have to make money, A...

00:11:29.320 --> 00:11:33.640
and I think this advertising model
is probably the most elegant way.

00:11:36.280 --> 00:11:38.280
[bright instrumental music playing]

00:11:42.240 --> 00:11:44.360
Uh-oh. What's this video Mom just sent us?

00:11:44.440 --> 00:11:46.760
Oh, that's from a talk show,
but that's pretty good.

00:11:46.840 --> 00:11:47.880
Guy's kind of a genius.

00:11:47.960 --> 00:11:50.560
He's talking all about deleting
social media, which you gotta do.

00:11:50.680 --> 00:11:52.880
I might have to start blocking
her e-mails.

00:11:52.960 --> 00:11:54.880
I don't even know
what she's talking about, man.

00:11:54.960 --> 00:11:56.080
She's worse than I am.

00:11:56.160 --> 00:11:58.520
-No, she only uses it for recipes.
-Right, and work.

00:11:58.600 --> 00:12:00.560
-And workout videos.
-[guy] And to check up on us.

00:12:00.640 --> 00:12:03.040
And everyone else she's ever met
in her entire life.

00:12:04.920 --> 00:12:07.880
If you are scrolling through
your social media feed

00:12:07.960 --> 00:12:11.720
while you're watchin' us, you need to put
the damn phone down and listen up

00:12:11.800 --> 00:12:14.800
'cause our next guest has written
an incredible book

00:12:14.880 --> 00:12:18.120
about how much it's wrecking our lives.

00:12:18.200 --> 00:12:19.440
Please welcome author

00:12:19.520 --> 00:12:23.960
of Ten Arguments for Deleting
Your Social Media Accounts Right Now...

00:12:24.040 --> 00:12:26.280
-[Sunny Hostin] Uh-huh.
-...Jaron Lanier.

00:12:26.360 --> 00:12:27.920
[cohosts speaking indistinctly]

00:12:28.000 --> 00:12:31.840
[Jaron] Companies like Google and Facebook
are some of the wealthiest

00:12:31.920 --> 00:12:33.520
and most successful of all time.

00:12:33.720 --> 00:12:36.840
Uh, they have relatively few employees.

00:12:36.920 --> 00:12:41.440
They just have this giant computer
that rakes in money, right? Uh...

00:12:41.520 --> 00:12:42.960
Now, what are they being paid for?

00:12:43.040 --> 00:12:45.200
[chuckles]
That's a really important question.

00:12:47.320 --> 00:12:50.320
[Roger] So, I've been an investor
in technology for 35 years.

00:12:51.000 --> 00:12:54.360
The first 50 years of Silicon Valley,
the industry made products--

00:12:54.440 --> 00:12:55.560
hardware, software--

00:12:55.640 --> 00:12:58.400
sold 'em to customers.
Nice, simple business.

00:12:58.480 --> 00:13:01.440
For the last ten years,
the biggest companies in Silicon Valley

00:13:01.520 --> 00:13:03.880
have been in the business
of selling their users.

00:13:03.960 --> 00:13:05.920
It's a little even trite to say now,

00:13:06.000 --> 00:13:09.200
but... because we don't pay
for the products that we use,

00:13:09.280 --> 00:13:12.160
advertisers pay
for the products that we use.

00:13:12.240 --> 00:13:14.200
Advertisers are the customers.

00:13:14.720 --> 00:13:16.080
We're the thing being sold.

00:13:16.160 --> 00:13:17.640
The classic saying is:

00:13:17.720 --> 00:13:21.600
"If you're not paying for the product,
then you are the product."

00:13:23.400 --> 00:13:27.200
A lot of people think, you know,
"Oh, well, Google's just a search box,

00:13:27.320 --> 00:13:29.840
and Facebook's just a place to see
what my friends are doing

00:13:29.920 --> 00:13:31.080
and see their photos."

00:13:31.160 --> 00:13:35.480
But what they don't realize
is they're competing for your attention.

00:13:36.520 --> 00:13:41.120
So, you know, Facebook, Snapchat,
Twitter, Instagram, YouTube,

00:13:41.200 --> 00:13:45.680
companies like this, their business model
is to keep people engaged on the screen.

00:13:46.280 --> 00:13:49.560
Let's figure out how to get
as much of this person's attention

00:13:49.640 --> 00:13:50.960
as we possibly can.

00:13:51.440 --> 00:13:53.360
How much time can we get you to spend?

00:13:53.880 --> 00:13:56.680
How much of your life can we get you
to give to us?

00:13:58.640 --> 00:14:01.080
[Justin] When you think about
how some of these companies work,

00:14:01.160 --> 00:14:02.400
it starts to make sense.

00:14:03.040 --> 00:14:06.080
There are all these services
on the Internet that we think of as free,

00:14:06.160 --> 00:14:09.480
but they're not free.
They're paid for by advertisers.

00:14:09.560 --> 00:14:11.560
Why do advertisers pay those companies?

00:14:11.640 --> 00:14:14.680
They pay in exchange for showing their ads
to us.

00:14:14.760 --> 00:14:18.360
We're the product. Our attention
is the product being sold to advertisers.

00:14:18.800 --> 00:14:20.440
That's a little too simplistic.

00:14:20.840 --> 00:14:23.640
It's the gradual, slight,
imperceptible change

00:14:23.720 --> 00:14:26.560
in your own behavior and perception
that is the product.

00:14:27.640 --> 00:14:30.240
And that is the product.
It's the only possible product.

00:14:30.320 --> 00:14:34.080
There's nothing else on the table
that could possibly be called the product.

00:14:34.160 --> 00:14:37.000
That's the only thing there is
for them to make money from.

00:14:37.680 --> 00:14:39.240
Changing what you do,

00:14:39.320 --> 00:14:41.720
how you think, who you are.

00:14:42.640 --> 00:14:45.280
It's a gradual change. It's slight.

00:14:45.360 --> 00:14:48.960
If you can go to somebody and you say,
"Give me $10 million,

00:14:49.040 --> 00:14:54.320
and I will change the world one percent
in the direction you want it to change..."

00:14:54.840 --> 00:14:58.200
It's the world! That can be incredible,
and that's worth a lot of money.

00:14:59.320 --> 00:15:00.160
Okay.

00:15:00.680 --> 00:15:04.560
[Shoshana] This is what every business
has always dreamt of:

00:15:04.640 --> 00:15:10.920
to have a guarantee that if it places
an ad, it will be successful.

00:15:11.320 --> 00:15:12.800
That's their business.

00:15:12.880 --> 00:15:14.400
They sell certainty.

00:15:15.000 --> 00:15:17.600
In order to be successful
in that business,

00:15:17.720 --> 00:15:19.800
you have to have great predictions.

00:15:20.080 --> 00:15:24.160
Great predictions begin
with one imperative:

00:15:25.200 --> 00:15:26.920
you need a lot of data.

00:15:29.120 --> 00:15:31.280
Many people call this
surveillance capitalism,

00:15:31.640 --> 00:15:34.360
capitalism profiting
off of the infinite tracking

00:15:34.440 --> 00:15:38.040
of everywhere everyone goes
by large technology companies

00:15:38.120 --> 00:15:40.360
whose business model is to make sure

00:15:40.440 --> 00:15:42.840
that advertisers are as successful
as possible.

00:15:42.920 --> 00:15:45.560
This is a new kind of marketplace now.

00:15:45.640 --> 00:15:48.080
It's a marketplace
that never existed before.

00:15:48.800 --> 00:15:55.360
And it's a marketplace
that trades exclusively in human futures.

00:15:56.080 --> 00:16:01.560
Just like there are markets that trade
in pork belly futures or oil futures.

00:16:02.120 --> 00:16:07.600
We now have markets
that trade in human futures at scale,

00:16:08.160 --> 00:16:13.480
and those markets have produced
the trillions of dollars

00:16:14.000 --> 00:16:19.280
that have made the Internet companies
the richest companies

00:16:19.360 --> 00:16:22.360
in the history of humanity.

00:16:23.360 --> 00:16:25.360
[indistinct chatter]

00:16:27.360 --> 00:16:31.000
[Jeff] What I want people to know
is that everything they're doing online

00:16:31.080 --> 00:16:34.320
is being watched, is being tracked,
is being measured.

00:16:35.040 --> 00:16:39.600
Every single action you take
is carefully monitored and recorded.

00:16:39.720 --> 00:16:43.840
Exactly what image you stop and look at,
for how long you look at it.

00:16:43.920 --> 00:16:45.800
Oh, yeah, seriously,
for how long you look at it.

00:16:45.880 --> 00:16:47.880
[monitors beeping]

00:16:50.520 --> 00:16:52.200
[Tristan] They know
when people are lonely.

00:16:52.280 --> 00:16:53.800
They know when people are depressed.

00:16:53.880 --> 00:16:57.080
They know when people are looking
at photos of your ex-romantic partners.

00:16:57.160 --> 00:17:00.840
They know what you're doing late at night.
They know the entire thing.

00:17:01.280 --> 00:17:03.240
Whether you're an introvert
or an extrovert,

00:17:03.320 --> 00:17:06.800
or what kind of neuroses you have,
what your personality type is like.

00:17:08.200 --> 00:17:11.600
[Shoshana] They have more information
about us

00:17:11.680 --> 00:17:14.320
than has ever been imagined
in human history.

00:17:14.960 --> 00:17:16.360
It is unprecedented.

00:17:18.560 --> 00:17:22.800
And so, all of this data that we're...
that we're just pouring out all the time

00:17:22.880 --> 00:17:26.760
is being fed into these systems
that have almost no human supervision

00:17:27.440 --> 00:17:30.880
and that are making better and better
and better and better predictions

00:17:30.960 --> 00:17:33.560
about what we're gonna do
and... and who we are.

00:17:33.640 --> 00:17:35.640
[indistinct chatter]

00:17:36.280 --> 00:17:39.360
[Aza] People have the misconception
it's our data being sold.

00:17:40.360 --> 00:17:43.200
It's not in Facebook's business interest
to give up the data.

00:17:45.520 --> 00:17:47.120
What do they do with that data?

00:17:49.400 --> 00:17:51.000
[console whirring]

00:17:51.080 --> 00:17:54.480
[Aza] They build models
that predict our actions,

00:17:54.560 --> 00:17:57.600
and whoever has the best model wins.

00:18:02.720 --> 00:18:04.040
His scrolling speed is slowing.

00:18:04.120 --> 00:18:06.080
Nearing the end
of his average session length.

00:18:06.160 --> 00:18:07.000
Decreasing ad load.

00:18:07.080 --> 00:18:08.320
Pull back on friends and family.

00:18:09.600 --> 00:18:11.320
[Tristan] On the other side of the screen,

00:18:11.400 --> 00:18:15.480
it's almost as if they had
this avatar voodoo doll-like model of us.

00:18:16.840 --> 00:18:18.160
All of the things we've ever done,

00:18:18.240 --> 00:18:19.480
all the clicks we've ever made,

00:18:19.560 --> 00:18:21.640
all the videos we've watched,
all the likes,

00:18:21.720 --> 00:18:25.360
that all gets brought back into building
a more and more accurate model.

00:18:25.880 --> 00:18:27.480
The model, once you have it,

00:18:27.560 --> 00:18:29.840
you can predict the kinds of things
that person does.

00:18:29.920 --> 00:18:31.760
Right, let me just test.

00:18:32.560 --> 00:18:35.000
[Tristan] Where you'll go.
I can predict what kind of videos

00:18:35.080 --> 00:18:36.120
will keep you watching.

00:18:36.200 --> 00:18:39.160
I can predict what kinds of emotions tend
to trigger you.

00:18:39.240 --> 00:18:40.400
[blue AI] Yes, perfect.

00:18:41.560 --> 00:18:43.360
The most epic fails of the year.

00:18:46.120 --> 00:18:47.520
-[crowd groans on video]
-[whooshes]

00:18:48.640 --> 00:18:51.080
-Perfect. That worked.
-Following with another video.

00:18:51.160 --> 00:18:54.040
Beautiful. Let's squeeze in a sneaker ad
before it starts.

00:18:56.440 --> 00:18:58.160
[Tristan] At a lot
of technology companies,

00:18:58.240 --> 00:18:59.720
there's three main goals.

00:18:59.800 --> 00:19:01.360
There's the engagement goal:

00:19:01.440 --> 00:19:03.680
to drive up your usage,
to keep you scrolling.

00:19:04.600 --> 00:19:06.120
There's the growth goal:

00:19:06.240 --> 00:19:08.680
to keep you coming back
and inviting as many friends

00:19:08.760 --> 00:19:10.800
and getting them to invite more friends.

00:19:11.640 --> 00:19:13.160
And then there's the advertising goal:

00:19:13.240 --> 00:19:15.000
to make sure that,
as all that's happening,

00:19:15.080 --> 00:19:17.400
we're making as much money as possible
from advertising.

00:19:18.120 --> 00:19:19.160
[console beeps]

00:19:19.240 --> 00:19:22.000
Each of these goals are powered
by algorithms

00:19:22.080 --> 00:19:24.440
whose job is to figure out
what to show you

00:19:24.520 --> 00:19:26.160
to keep those numbers going up.

00:19:26.600 --> 00:19:29.920
We often talked about, at Facebook,
this idea

00:19:30.000 --> 00:19:34.000
of being able to just dial that as needed.

00:19:34.680 --> 00:19:38.600
And, you know, we talked
about having Mark have those dials.

00:19:41.280 --> 00:19:44.480
"Hey, I want more users in Korea today."

00:19:45.680 --> 00:19:46.600
"Turn the dial."

00:19:47.440 --> 00:19:49.200
"Let's dial up the ads a little bit."

00:19:49.960 --> 00:19:51.880
"Dial up monetization, just slightly."

00:19:52.840 --> 00:19:55.440
And so, that happ--

00:19:55.520 --> 00:19:59.240
I mean, at all of these companies,
there is that level of precision.

00:20:00.000 --> 00:20:02.400
-Dude, how--
-I don't know how I didn't get carded.

00:20:02.480 --> 00:20:05.680
-That ref just, like, sucked or something.
-You got literally all the way...

00:20:05.800 --> 00:20:07.960
-That's Rebecca. Go talk to her.
-I know who it is.

00:20:08.040 --> 00:20:10.840
-Dude, yo, go talk to her.
-[guy] I'm workin' on it.

00:20:10.920 --> 00:20:14.160
His calendar says he's on a break
right now. We should be live.

00:20:14.760 --> 00:20:16.440
[sighs] Want me to nudge him?

00:20:17.120 --> 00:20:18.040
Yeah, nudge away.

00:20:18.120 --> 00:20:19.080
[console beeps]

00:20:21.640 --> 00:20:24.160
"Your friend Tyler just joined.
Say hi with a wave."

00:20:26.000 --> 00:20:27.160
[Engagement AI] Come on, Ben.

00:20:27.280 --> 00:20:29.320
Send a wave. [sighs]

00:20:29.400 --> 00:20:32.600
-You're not... Go talk to her, dude.
-[phone vibrates, chimes]

00:20:33.840 --> 00:20:35.480
-[Ben sighs]
-[cell phone chimes]

00:20:36.880 --> 00:20:38.000
[console beeps]

00:20:38.080 --> 00:20:40.440
New link! All right, we're on. [exhales]

00:20:40.960 --> 00:20:46.080
Follow that up with a post
from User 079044238820, Rebecca.

00:20:46.160 --> 00:20:49.800
Good idea. GPS coordinates indicate
that they're in close proximity.

00:20:55.920 --> 00:20:57.160
He's primed for an ad.

00:20:57.640 --> 00:20:58.640
Auction time.

00:21:00.120 --> 00:21:02.800
Sold! To Deep Fade hair wax.

00:21:03.400 --> 00:21:07.920
We had 468 interested bidders. We sold Ben
at 3.262 cents for an impression.

00:21:08.840 --> 00:21:10.840
[melancholy piano music playing]

00:21:14.160 --> 00:21:15.040
[Ben sighs]

00:21:17.120 --> 00:21:18.720
[Jaron] We've created a world

00:21:18.800 --> 00:21:21.520
in which online connection
has become primary,

00:21:22.080 --> 00:21:23.920
especially for younger generations.

00:21:24.000 --> 00:21:28.320
And yet, in that world,
any time two people connect,

00:21:29.160 --> 00:21:33.240
the only way it's financed
is through a sneaky third person

00:21:33.320 --> 00:21:35.640
who's paying to manipulate
those two people.

00:21:36.120 --> 00:21:39.360
So, we've created
an entire global generation of people

00:21:39.440 --> 00:21:44.000
who are raised within a context
where the very meaning of communication,

00:21:44.080 --> 00:21:47.440
the very meaning of culture,
is manipulation.

00:21:47.520 --> 00:21:49.640
We've put deceit and sneakiness

00:21:49.720 --> 00:21:52.320
at the absolute center
of everything we do.

00:22:05.600 --> 00:22:07.240
-[interviewer] Grab the...
-[Tristan] Okay.

00:22:07.320 --> 00:22:09.280
-Where's it help to hold it?
-[interviewer] Great.

00:22:09.360 --> 00:22:10.800
-[Tristan] Here?
-[interviewer] Yeah.

00:22:10.880 --> 00:22:13.840
How does this come across on camera
if I were to do, like, this move--

00:22:13.920 --> 00:22:15.520
-[interviewer] We can--
-[blows] Like that?

00:22:15.640 --> 00:22:16.920
-[interviewer laughs] What?
-Yeah.

00:22:17.000 --> 00:22:19.000
-[interviewer] Do that again.
-Exactly. Yeah. [blows]

00:22:19.080 --> 00:22:20.600
Yeah. No, it's probably not...

00:22:20.680 --> 00:22:21.960
Like... yeah.

00:22:22.480 --> 00:22:23.880
I mean, this one is less...

00:22:29.680 --> 00:22:33.280
[interviewer laughs] Larissa's, like,
actually freaking out over here.

00:22:34.720 --> 00:22:35.560
Is that good?

00:22:35.640 --> 00:22:37.760
[instrumental music playing]

00:22:37.840 --> 00:22:41.080
[Tristan] I was, like, five years old
when I learned how to do magic.

00:22:41.160 --> 00:22:45.760
And I could fool adults,
fully-grown adults with, like, PhDs.

00:22:55.040 --> 00:22:57.720
Magicians were almost like
the first neuroscientists

00:22:57.800 --> 00:22:58.960
and psychologists.

00:22:59.040 --> 00:23:02.000
Like, they were the ones
who first understood

00:23:02.080 --> 00:23:03.360
how people's minds work.

00:23:04.200 --> 00:23:07.680
They just, in real time, are testing
lots and lots of stuff on people.

00:23:09.120 --> 00:23:11.120
A magician understands something,

00:23:11.200 --> 00:23:14.000
some part of your mind
that we're not aware of.

00:23:14.080 --> 00:23:15.920
That's what makes the illusion work.

00:23:16.000 --> 00:23:20.600
Doctors, lawyers, people who know
how to build 747s or nuclear missiles,

00:23:20.680 --> 00:23:24.360
they don't know more about
how their own mind is vulnerable.

00:23:24.440 --> 00:23:26.120
That's a separate discipline.

00:23:26.560 --> 00:23:29.000
And it's a discipline
that applies to all human beings.

00:23:30.920 --> 00:23:34.080
From that perspective, you can have
a very different understanding

00:23:34.160 --> 00:23:35.560
of what technology is doing.

00:23:36.880 --> 00:23:39.560
When I was
at the Stanford Persuasive Technology Lab,

00:23:39.680 --> 00:23:41.040
this is what we learned.

00:23:41.640 --> 00:23:43.440
How could you use everything we know

00:23:43.560 --> 00:23:45.880
about the psychology
of what persuades people

00:23:45.960 --> 00:23:48.360
and build that into technology?

00:23:48.480 --> 00:23:50.880
Now, many of you in the audience
are geniuses already.

00:23:50.960 --> 00:23:55.840
I think that's true, but my goal is
to turn you into a behavior-change genius.

00:23:56.840 --> 00:24:01.160
There are many prominent Silicon Valley
figures who went through that class--

00:24:01.240 --> 00:24:05.480
key growth figures at Facebook and Uber
and... and other companies--

00:24:05.560 --> 00:24:09.200
and learned how to make technology
more persuasive,

00:24:09.600 --> 00:24:10.760
Tristan being one.

00:24:12.280 --> 00:24:14.600
[Tristan] Persuasive technology
is just sort of design

00:24:14.680 --> 00:24:16.560
intentionally applied to the extreme,

00:24:16.640 --> 00:24:18.880
where we really want to modify
someone's behavior.

00:24:18.960 --> 00:24:20.520
We want them to take this action.

00:24:20.640 --> 00:24:23.320
We want them to keep doing this
with their finger.

00:24:23.400 --> 00:24:26.240
You pull down and you refresh,
it's gonna be a new thing at the top.

00:24:26.320 --> 00:24:28.520
Pull down and refresh again, it's new.
Every single time.

00:24:28.600 --> 00:24:33.720
Which, in psychology, we call
a positive intermittent reinforcement.

00:24:33.800 --> 00:24:37.120
You don't know when you're gonna get it
or if you're gonna get something,

00:24:37.200 --> 00:24:40.040
which operates just like the slot machines
in Vegas.

00:24:40.120 --> 00:24:42.240
It's not enough
that you use the product consciously,

00:24:42.320 --> 00:24:44.000
I wanna dig down deeper
into the brain stem

00:24:44.120 --> 00:24:45.800
and implant, inside of you,

00:24:45.880 --> 00:24:47.640
an unconscious habit

00:24:47.720 --> 00:24:50.840
so that you are being programmed
at a deeper level.

00:24:50.960 --> 00:24:52.120
You don't even realize it.

00:24:52.520 --> 00:24:54.040
[teacher] A man, James Marshall...

00:24:54.120 --> 00:24:56.280
[Tristan] Every time you see it there
on the counter,

00:24:56.360 --> 00:24:59.800
and you just look at it,
and you know if you reach over,

00:24:59.880 --> 00:25:01.320
it just might have something for you,

00:25:01.400 --> 00:25:03.880
so you play that slot machine
to see what you got, right?

00:25:03.960 --> 00:25:06.040
That's not by accident.
That's a design technique.

00:25:06.120 --> 00:25:08.640
[teacher] He brings a golden nugget
to an officer

00:25:09.840 --> 00:25:11.280
in the army in San Francisco.

00:25:12.200 --> 00:25:15.400
Mind you, the... the population
of San Francisco was only...

00:25:15.480 --> 00:25:17.440
[Jeff]
Another example is photo tagging.

00:25:17.520 --> 00:25:19.640
-[teacher] The secret didn't last.
-[phone vibrates]

00:25:19.720 --> 00:25:21.200
[Jeff] So, if you get an e-mail

00:25:21.280 --> 00:25:24.040
that says your friend just tagged you
in a photo,

00:25:24.160 --> 00:25:28.560
of course you're going to click
on that e-mail and look at the photo.

00:25:29.160 --> 00:25:31.800
It's not something
you can just decide to ignore.

00:25:32.360 --> 00:25:34.160
This is deep-seated, like,

00:25:34.240 --> 00:25:36.320
human personality
that they're tapping into.

00:25:36.400 --> 00:25:38.080
What you should be asking yourself is:

00:25:38.160 --> 00:25:40.280
"Why doesn't that e-mail contain
the photo in it?

00:25:40.360 --> 00:25:42.440
It would be a lot easier
to see the photo."

00:25:42.520 --> 00:25:45.920
When Facebook found that feature,
they just dialed the hell out of that

00:25:46.000 --> 00:25:48.480
because they said, "This is gonna be
a great way to grow activity.

00:25:48.600 --> 00:25:51.080
Let's just get people tagging each other
in photos all day long."

00:25:51.160 --> 00:25:53.160
[upbeat techno music playing]

00:25:57.880 --> 00:25:58.880
[cell phone chimes]

00:25:59.360 --> 00:26:00.480
He commented.

00:26:00.560 --> 00:26:01.440
[Growth AI] Nice.

00:26:01.920 --> 00:26:04.680
Okay, Rebecca received it,
and she is responding.

00:26:04.760 --> 00:26:07.560
All right, let Ben know that she's typing
so we don't lose him.

00:26:07.640 --> 00:26:08.720
Activating ellipsis.

00:26:09.760 --> 00:26:11.920
[teacher continues speaking indistinctly]

00:26:13.680 --> 00:26:15.880
[tense instrumental music playing]

00:26:19.960 --> 00:26:21.320
Great, she posted.

00:26:21.440 --> 00:26:24.240
He's commenting on her comment
about his comment on her post.

00:26:25.040 --> 00:26:26.400
Hold on, he stopped typing.

00:26:26.760 --> 00:26:27.760
Let's autofill.

00:26:28.400 --> 00:26:30.000
Emojis. He loves emojis.

00:26:33.840 --> 00:26:34.680
He went with fire.

00:26:34.760 --> 00:26:36.800
[clicks tongue, sighs]
I was rootin' for eggplant.

00:26:38.600 --> 00:26:42.720
[Tristan] There's an entire discipline
and field called "growth hacking."

00:26:42.800 --> 00:26:47.160
Teams of engineers
whose job is to hack people's psychology

00:26:47.240 --> 00:26:48.560
so they can get more growth.

00:26:48.640 --> 00:26:50.960
They can get more user sign-ups,
more engagement.

00:26:51.080 --> 00:26:52.840
They can get you to invite more people.

00:26:52.920 --> 00:26:56.000
After all the testing, all the iterating,
all of this stuff,

00:26:56.080 --> 00:26:57.920
you know the single biggest thing
we realized?

00:26:58.000 --> 00:27:00.680
Get any individual to seven friends
in ten days.

00:27:01.960 --> 00:27:02.800
That was it.

00:27:02.880 --> 00:27:05.480
Chamath was the head of growth at Facebook
early on,

00:27:05.560 --> 00:27:08.240
and he's very well known
in the tech industry

00:27:08.320 --> 00:27:11.000
for pioneering a lot of the growth tactics

00:27:11.080 --> 00:27:14.760
that were used to grow Facebook
at incredible speed.

00:27:14.840 --> 00:27:18.560
And those growth tactics have then become
the standard playbook for Silicon Valley.

00:27:18.640 --> 00:27:21.200
They were used at Uber
and at a bunch of other companies.

00:27:21.320 --> 00:27:27.040
One of the things that he pioneered
was the use of scientific A/B testing

00:27:27.120 --> 00:27:28.480
of small feature changes.

00:27:29.000 --> 00:27:30.920
Companies like Google and Facebook

00:27:31.000 --> 00:27:34.560
would roll out
lots of little, tiny experiments

00:27:34.640 --> 00:27:36.800
that they were constantly doing on users.

00:27:36.880 --> 00:27:39.880
And over time,
by running these constant experiments,

00:27:39.960 --> 00:27:43.040
you... you develop the most optimal way

00:27:43.120 --> 00:27:45.280
to get users to do
what you want them to do.

00:27:45.360 --> 00:27:46.800
It's... It's manipulation.

00:27:47.320 --> 00:27:49.440
[interviewer]
Uh, you're making me feel like a lab rat.

00:27:49.840 --> 00:27:51.920
You are a lab rat. We're all lab rats.

00:27:52.560 --> 00:27:55.560
And it's not like we're lab rats
for developing a cure for cancer.

00:27:55.640 --> 00:27:58.120
It's not like they're trying
to benefit us.

00:27:58.200 --> 00:28:01.680
Right? We're just zombies,
and they want us to look at more ads

00:28:01.760 --> 00:28:03.160
so they can make more money.

00:28:03.560 --> 00:28:05.280
[Shoshana] Facebook conducted

00:28:05.360 --> 00:28:08.240
what they called
"massive-scale contagion experiments."

00:28:08.320 --> 00:28:09.120
Okay.

00:28:09.240 --> 00:28:13.080
[Shoshana] How do we use subliminal cues
on the Facebook pages

00:28:13.400 --> 00:28:17.640
to get more people to go vote
in the midterm elections?

00:28:18.000 --> 00:28:20.800
And they discovered
that they were able to do that.

00:28:20.920 --> 00:28:24.160
One thing they concluded
is that we now know

00:28:24.720 --> 00:28:28.920
we can affect real-world behavior
and emotions

00:28:29.000 --> 00:28:32.880
without ever triggering
the user's awareness.

00:28:33.360 --> 00:28:37.360
They are completely clueless.

00:28:38.040 --> 00:28:41.960
We're pointing these engines of AI
back at ourselves

00:28:42.040 --> 00:28:46.200
to reverse-engineer what elicits responses
from us.

00:28:47.080 --> 00:28:49.560
Almost like you're stimulating nerve cells
on a spider

00:28:49.640 --> 00:28:51.480
to see what causes its legs to respond.

00:28:51.920 --> 00:28:53.920
So, it really is
this kind of prison experiment

00:28:54.000 --> 00:28:56.720
where we're just, you know,
roping people into the matrix,

00:28:56.800 --> 00:29:00.560
and we're just harvesting all this money
and... and data from all their activity

00:29:00.640 --> 00:29:01.480
to profit from.

00:29:01.560 --> 00:29:03.440
And we're not even aware
that it's happening.

00:29:04.120 --> 00:29:07.920
So, we want to psychologically figure out
how to manipulate you as fast as possible

00:29:08.000 --> 00:29:10.080
and then give you back that dopamine hit.

00:29:10.160 --> 00:29:12.360
We did that brilliantly at Facebook.

00:29:12.640 --> 00:29:14.920
Instagram has done it.
WhatsApp has done it.

00:29:15.000 --> 00:29:17.360
You know, Snapchat has done it.
Twitter has done it.

00:29:17.440 --> 00:29:19.400
I mean, it's exactly the kind of thing

00:29:19.520 --> 00:29:22.440
that a... that a hacker like myself
would come up with

00:29:22.520 --> 00:29:27.000
because you're exploiting a vulnerability
in... in human psychology.

00:29:27.800 --> 00:29:29.720
[chuckles] And I just...
I think that we...

00:29:29.800 --> 00:29:33.440
you know, the inventors, creators...

00:29:33.960 --> 00:29:37.320
uh, you know, and it's me, it's Mark,
it's the...

00:29:37.400 --> 00:29:40.400
you know, Kevin Systrom at Instagram...
It's all of these people...

00:29:40.480 --> 00:29:46.440
um, understood this consciously,
and we did it anyway.

00:29:50.560 --> 00:29:53.760
No one got upset when bicycles showed up.

00:29:55.040 --> 00:29:58.000
Right? Like, if everyone's starting
to go around on bicycles,

00:29:58.080 --> 00:30:00.920
no one said,
"Oh, my God, we've just ruined society.

00:30:01.000 --> 00:30:03.040
[chuckles]
Like, bicycles are affecting people.

00:30:03.120 --> 00:30:05.280
They're pulling people
away from their kids.

00:30:05.400 --> 00:30:08.720
They're ruining the fabric of democracy.
People can't tell what's true."

00:30:08.800 --> 00:30:11.480
Like, we never said any of that stuff
about a bicycle.

00:30:12.760 --> 00:30:16.160
If something is a tool,
it genuinely is just sitting there,

00:30:16.720 --> 00:30:18.720
waiting patiently.

00:30:19.320 --> 00:30:22.800
If something is not a tool,
it's demanding things from you.

00:30:22.880 --> 00:30:26.520
It's seducing you. It's manipulating you.
It wants things from you.

00:30:26.960 --> 00:30:30.480
And we've moved away from having
a tools-based technology environment

00:30:31.040 --> 00:30:34.480
to an addiction- and manipulation-based
technology environment.

00:30:34.560 --> 00:30:35.720
That's what's changed.

00:30:35.800 --> 00:30:39.400
Social media isn't a tool
that's just waiting to be used.

00:30:39.480 --> 00:30:43.480
It has its own goals,
and it has its own means of pursuing them

00:30:43.560 --> 00:30:45.680
by using your psychology against you.

00:30:45.760 --> 00:30:47.760
[ominous instrumental music playing]

00:30:57.560 --> 00:31:00.560
[Tim] Rewind a few years ago,
I was the...

00:31:00.640 --> 00:31:02.320
I was the president of Pinterest.

00:31:03.160 --> 00:31:05.120
I was coming home,

00:31:05.200 --> 00:31:08.360
and I couldn't get off my phone
once I got home,

00:31:08.440 --> 00:31:12.160
despite having two young kids
who needed my love and attention.

00:31:12.240 --> 00:31:15.760
I was in the pantry, you know,
typing away on an e-mail

00:31:15.840 --> 00:31:17.520
or sometimes looking at Pinterest.

00:31:18.000 --> 00:31:19.640
I thought, "God, this is classic irony.

00:31:19.720 --> 00:31:22.040
I am going to work during the day

00:31:22.120 --> 00:31:26.440
and building something
that then I am falling prey to."

00:31:26.520 --> 00:31:30.080
And I couldn't... I mean, some
of those moments, I couldn't help myself.

00:31:30.160 --> 00:31:31.840
-[notification chimes]
-[woman gasps]

00:31:32.320 --> 00:31:36.080
The one
that I'm... I'm most prone to is Twitter.

00:31:36.200 --> 00:31:38.000
Uh, used to be Reddit.

00:31:38.080 --> 00:31:42.840
I actually had to write myself software
to break my addiction to reading Reddit.

00:31:42.920 --> 00:31:44.880
-[notifications chime]
-[slot machines whir]

00:31:45.400 --> 00:31:47.760
I'm probably most addicted to my e-mail.

00:31:47.840 --> 00:31:49.880
I mean, really. I mean, I... I feel it.

00:31:49.960 --> 00:31:51.400
-[notifications chime]
-[woman gasps]

00:31:51.480 --> 00:31:52.480
[electricity crackles]

00:31:52.560 --> 00:31:54.960
Well, I mean, it's sort-- it's interesting

00:31:55.040 --> 00:31:58.160
that knowing what was going on
behind the curtain,

00:31:58.240 --> 00:32:01.640
I still wasn't able to control my usage.

00:32:01.720 --> 00:32:03.040
So, that's a little scary.

00:32:03.640 --> 00:32:07.040
Even knowing how these tricks work,
I'm still susceptible to them.

00:32:07.120 --> 00:32:09.880
I'll still pick up the phone,
and 20 minutes will disappear.

00:32:09.960 --> 00:32:11.400
[notifications chime]

00:32:11.480 --> 00:32:12.720
-[fluid rushes]
-[woman gasps]

00:32:12.800 --> 00:32:15.720
Do you check your smartphone
before you pee in the morning

00:32:15.800 --> 00:32:17.480
or while you're peeing in the morning?

00:32:17.560 --> 00:32:19.480
'Cause those are the only two choices.

00:32:19.560 --> 00:32:23.280
I tried through willpower,
just pure willpower...

00:32:23.360 --> 00:32:26.880
"I'll put down my phone, I'll leave
my phone in the car when I get home."

00:32:27.000 --> 00:32:30.560
I think I told myself a thousand times,
a thousand different days,

00:32:30.640 --> 00:32:32.600
"I am not gonna bring my phone
to the bedroom,"

00:32:32.680 --> 00:32:34.520
and then 9:00 p.m. rolls around.

00:32:34.600 --> 00:32:37.120
"Well, I wanna bring my phone
in the bedroom."

00:32:37.200 --> 00:32:39.280
[takes a deep breath]
And so, that was sort of...

00:32:39.360 --> 00:32:41.120
Willpower was kind of attempt one,

00:32:41.200 --> 00:32:44.280
and then attempt two was,
you know, brute force.

00:32:44.360 --> 00:32:48.080
[announcer] Introducing the Kitchen Safe.
The Kitchen Safe is a revolutionary,

00:32:48.160 --> 00:32:51.680
new, time-locking container
that helps you fight temptation.

00:32:51.760 --> 00:32:56.720
All David has to do is place
those temptations in the Kitchen Safe.

00:32:57.400 --> 00:33:00.400
Next, he rotates the dial
to set the timer.

00:33:01.480 --> 00:33:04.240
And, finally, he presses the dial
to activate the lock.

00:33:04.320 --> 00:33:05.520
The Kitchen Safe is great...

00:33:05.600 --> 00:33:06.760
We have that, don't we?

00:33:06.840 --> 00:33:08.640
...video games, credit cards,
and cell phones.

00:33:08.720 --> 00:33:09.640
Yeah, we do.

00:33:09.720 --> 00:33:12.400
[announcer] Once the Kitchen Safe
is locked, it cannot be opened

00:33:12.480 --> 00:33:13.880
until the timer reaches zero.

00:33:13.960 --> 00:33:15.600
[Anna] So, here's the thing.

00:33:15.680 --> 00:33:17.520
Social media is a drug.

00:33:17.600 --> 00:33:20.880
I mean,
we have a basic biological imperative

00:33:20.960 --> 00:33:23.080
to connect with other people.

00:33:23.160 --> 00:33:28.200
That directly affects the release
of dopamine in the reward pathway.

00:33:28.280 --> 00:33:32.560
Millions of years of evolution, um,
are behind that system

00:33:32.640 --> 00:33:35.600
to get us to come together
and live in communities,

00:33:35.680 --> 00:33:38.000
to find mates, to propagate our species.

00:33:38.080 --> 00:33:41.840
So, there's no doubt
that a vehicle like social media,

00:33:41.920 --> 00:33:45.680
which optimizes this connection
between people,

00:33:45.760 --> 00:33:48.560
is going to have the potential
for addiction.

00:33:52.080 --> 00:33:54.120
-Mmm! [laughs]
-Dad, stop!

00:33:55.440 --> 00:33:58.440
I have, like, 1,000 more snips
to send before dinner.

00:33:58.520 --> 00:34:00.800
-[dad] Snips?
-I don't know what a snip is.

00:34:00.880 --> 00:34:03.200
-Mm, that smells good, baby.
-All right. Thank you.

00:34:03.280 --> 00:34:05.880
I was, um, thinking we could use
all five senses

00:34:05.960 --> 00:34:07.720
to enjoy our dinner tonight.

00:34:07.800 --> 00:34:11.360
So, I decided that we're not gonna have
any cell phones at the table tonight.

00:34:11.480 --> 00:34:13.280
So, turn 'em in.

00:34:13.800 --> 00:34:14.800
-Really?
-[mom] Yep.

00:34:15.920 --> 00:34:18.040
-All right.
-Thank you. Ben?

00:34:18.120 --> 00:34:20.440
-Okay.
-Mom, the phone pirate. [scoffs]

00:34:21.080 --> 00:34:21.920
-Got it.
-Mom!

00:34:22.520 --> 00:34:26.160
So, they will be safe in here
until after dinner...

00:34:27.280 --> 00:34:30.640
-and everyone can just chill out.
-[safe whirs]

00:34:30.720 --> 00:34:31.560
Okay?

00:34:40.840 --> 00:34:41.680
[Cass sighs]

00:34:45.720 --> 00:34:47.040
[notification chimes]

00:34:47.400 --> 00:34:49.240
-Can I just see who it is?
-No.

00:34:54.760 --> 00:34:56.960
Just gonna go get another fork.

00:34:58.280 --> 00:34:59.240
Thank you.

00:35:04.720 --> 00:35:06.760
Honey, you can't open that.

00:35:06.840 --> 00:35:09.320
I locked it for an hour,
so just leave it alone.

00:35:11.200 --> 00:35:13.360
So, what should we talk about?

00:35:13.440 --> 00:35:14.680
Well, we could talk

00:35:14.760 --> 00:35:17.600
about the, uh, Extreme Center wackos
I drove by today.

00:35:17.680 --> 00:35:18.800
-[mom] Please, Frank.
-What?

00:35:18.920 --> 00:35:20.760
[mom] I don't wanna talk about politics.

00:35:20.880 --> 00:35:23.520
-What's wrong with the Extreme Center?
-See? He doesn't even get it.

00:35:23.600 --> 00:35:24.600
It depends on who you ask.

00:35:24.720 --> 00:35:26.600
It's like asking,
"What's wrong with propaganda?"

00:35:26.720 --> 00:35:28.360
-[safe smashes]
-[mom and Frank scream]

00:35:28.720 --> 00:35:29.720
[Frank] Isla!

00:35:32.800 --> 00:35:33.760
Oh, my God.

00:35:36.440 --> 00:35:38.560
-[sighs] Do you want me to...
-[mom] Yeah.

00:35:41.960 --> 00:35:43.920
[Anna] I... I'm worried about my kids.

00:35:44.000 --> 00:35:46.680
And if you have kids,
I'm worried about your kids.

00:35:46.760 --> 00:35:50.200
Armed with all the knowledge that I have
and all of the experience,

00:35:50.280 --> 00:35:52.120
I am fighting my kids about the time

00:35:52.200 --> 00:35:54.440
that they spend on phones
and on the computer.

00:35:54.520 --> 00:35:58.200
I will say to my son, "How many hours do
you think you're spending on your phone?"

00:35:58.280 --> 00:36:01.080
He'll be like, "It's, like, half an hour.
It's half an hour, tops."

00:36:01.160 --> 00:36:04.840
I'd say upwards hour, hour and a half.

00:36:04.920 --> 00:36:06.800
I looked at his screen report
a couple weeks ago.

00:36:06.880 --> 00:36:08.720
-Three hours and 45 minutes.
-[James] That...

00:36:11.360 --> 00:36:13.600
I don't think that's...
No. Per day, on average?

00:36:13.680 --> 00:36:15.520
-Yeah.
-Should I go get it right now?

00:36:15.600 --> 00:36:19.160
There's not a day that goes by
that I don't remind my kids

00:36:19.240 --> 00:36:21.760
about the pleasure-pain balance,

00:36:21.840 --> 00:36:24.400
about dopamine deficit states,

00:36:24.480 --> 00:36:26.280
about the risk of addiction.

00:36:26.360 --> 00:36:27.320
[Mary] Moment of truth.

00:36:27.920 --> 00:36:29.680
Two hours, 50 minutes per day.

00:36:29.760 --> 00:36:31.760
-Let's see.
-Actually, I've been using a lot today.

00:36:31.840 --> 00:36:33.360
-Last seven days.
-That's probably why.

00:36:33.440 --> 00:36:37.360
Instagram, six hours, 13 minutes.
Okay, so my Instagram's worse.

00:36:39.560 --> 00:36:42.000
My screen's completely shattered.

00:36:42.200 --> 00:36:43.200
Thanks, Cass.

00:36:44.400 --> 00:36:46.000
What do you mean, "Thanks, Cass"?

00:36:46.080 --> 00:36:49.040
You keep freaking Mom out about our phones
when it's not really a problem.

00:36:49.360 --> 00:36:51.160
We don't need our phones to eat dinner!

00:36:51.240 --> 00:36:53.880
I get what you're saying.
It's just not that big a deal. It's not.

00:36:56.040 --> 00:36:58.360
If it's not that big a deal,
don't use it for a week.

00:36:59.640 --> 00:37:00.600
[Ben sighs]

00:37:01.120 --> 00:37:06.360
Yeah. Yeah, actually, if you can put
that thing away for, like, a whole week...

00:37:07.720 --> 00:37:09.520
I will buy you a new screen.

00:37:10.960 --> 00:37:12.880
-Like, starting now?
-[mom] Starting now.

00:37:15.160 --> 00:37:16.840
-Okay. You got a deal.
-[mom] Okay.

00:37:16.920 --> 00:37:19.120
Okay, you gotta leave it here, though,
buddy.

00:37:19.840 --> 00:37:21.360
All right, I'm plugging it in.

00:37:22.520 --> 00:37:25.080
Let the record show... I'm backing away.

00:37:25.160 --> 00:37:26.000
Okay.

00:37:27.800 --> 00:37:29.400
-You're on the clock.
-[Ben] One week.

00:37:29.480 --> 00:37:30.320
Oh, my...

00:37:31.440 --> 00:37:32.400
Think he can do it?

00:37:33.000 --> 00:37:34.240
I don't know. We'll see.

00:37:35.000 --> 00:37:36.120
Just eat, okay?

00:37:44.200 --> 00:37:45.240
Good family dinner!

00:37:47.680 --> 00:37:49.800
[Tristan] These technology products
were not designed

00:37:49.880 --> 00:37:53.880
by child psychologists who are trying
to protect and nurture children.

00:37:53.960 --> 00:37:56.160
They were just designing
to make these algorithms

00:37:56.240 --> 00:37:58.720
that were really good at recommending
the next video to you

00:37:58.800 --> 00:38:02.320
or really good at getting you
to take a photo with a filter on it.

00:38:15.720 --> 00:38:16.680
[cell phone chimes]

00:38:16.760 --> 00:38:18.880
[Tristan] It's not just
that it's controlling

00:38:18.960 --> 00:38:20.560
where they spend their attention.

00:38:21.160 --> 00:38:26.280
Especially social media starts to dig
deeper and deeper down into the brain stem

00:38:26.400 --> 00:38:29.760
and take over kids' sense of self-worth
and identity.

00:38:41.720 --> 00:38:42.880
[notifications chiming]

00:38:52.360 --> 00:38:56.200
[Tristan] We evolved to care about
whether other people in our tribe...

00:38:56.760 --> 00:38:59.120
think well of us or not
'cause it matters.

00:38:59.840 --> 00:39:04.560
But were we evolved to be aware
of what 10,000 people think of us?

00:39:04.640 --> 00:39:08.760
We were not evolved
to have social approval being dosed to us

00:39:08.840 --> 00:39:10.360
every five minutes.

00:39:10.440 --> 00:39:13.120
That was not at all what we were built
to experience.

00:39:15.400 --> 00:39:19.960
[Chamath] We curate our lives
around this perceived sense of perfection

00:39:20.720 --> 00:39:23.520
because we get rewarded
in these short-term signals--

00:39:23.600 --> 00:39:25.160
hearts, likes, thumbs-up--

00:39:25.240 --> 00:39:28.400
and we conflate that with value,
and we conflate it with truth.

00:39:29.800 --> 00:39:33.120
And instead, what it really is
is fake, brittle popularity...

00:39:33.920 --> 00:39:37.440
that's short-term and that leaves you
even more, and admit it,

00:39:37.520 --> 00:39:39.920
vacant and empty before you did it.

00:39:41.280 --> 00:39:43.360
Because then it forces you
into this vicious cycle

00:39:43.440 --> 00:39:47.160
where you're like, "What's the next thing
I need to do now? 'Cause I need it back."

00:39:48.240 --> 00:39:50.840
Think about that compounded
by two billion people,

00:39:50.920 --> 00:39:54.760
and then think about how people react then
to the perceptions of others.

00:39:54.840 --> 00:39:56.440
It's just a... It's really bad.

00:39:56.960 --> 00:39:58.240
It's really, really bad.

00:40:00.840 --> 00:40:03.480
[Jonathan] There has been
a gigantic increase

00:40:03.560 --> 00:40:06.520
in depression and anxiety
for American teenagers

00:40:06.600 --> 00:40:10.960
which began right around...
between 2011 and 2013.

00:40:11.040 --> 00:40:15.360
The number of teenage girls out of 100,000
in this country

00:40:15.440 --> 00:40:17.120
who were admitted to a hospital every year

00:40:17.200 --> 00:40:19.920
because they cut themselves
or otherwise harmed themselves,

00:40:20.000 --> 00:40:23.920
that number was pretty stable
until around 2010, 2011,

00:40:24.000 --> 00:40:25.760
and then it begins going way up.

00:40:28.760 --> 00:40:32.520
It's up 62 percent for older teen girls.

00:40:33.840 --> 00:40:38.320
It's up 189 percent for the preteen girls.
That's nearly triple.

00:40:40.320 --> 00:40:43.520
Even more horrifying,
we see the same pattern with suicide.

00:40:44.760 --> 00:40:47.560
The older teen girls, 15 to 19 years old,

00:40:47.640 --> 00:40:49.200
they're up 70 percent,

00:40:49.280 --> 00:40:51.680
compared to the first decade
of this century.

00:40:52.160 --> 00:40:55.080
The preteen girls,
who have very low rates to begin with,

00:40:55.160 --> 00:40:57.640
they are up 151 percent.

00:40:58.840 --> 00:41:01.720
And that pattern points to social media.

00:41:04.040 --> 00:41:07.200
Gen Z, the kids born after 1996 or so,

00:41:07.280 --> 00:41:10.320
those kids are the first generation
in history

00:41:10.440 --> 00:41:12.640
that got on social media in middle school.

00:41:12.720 --> 00:41:14.720
[thunder rumbling in distance]

00:41:15.880 --> 00:41:17.600
[Jonathan] How do they spend their time?

00:41:19.720 --> 00:41:22.720
They come home from school,
and they're on their devices.

00:41:24.320 --> 00:41:29.200
A whole generation is more anxious,
more fragile, more depressed.

00:41:29.320 --> 00:41:30.520
-[thunder rumbles]
-[Isla gasps]

00:41:30.600 --> 00:41:33.280
[Jonathan] They're much less comfortable
taking risks.

00:41:34.320 --> 00:41:37.520
The rates at which they get
driver's licenses have been dropping.

00:41:38.960 --> 00:41:41.080
The number
who have ever gone out on a date

00:41:41.160 --> 00:41:44.240
or had any kind of romantic interaction
is dropping rapidly.

00:41:47.480 --> 00:41:49.720
This is a real change in a generation.

00:41:53.160 --> 00:41:57.320
And remember, for every one of these,
for every hospital admission,

00:41:57.400 --> 00:42:00.280
there's a family that is traumatized
and horrified.

00:42:00.360 --> 00:42:02.360
"My God, what is happening to our kids?"

00:42:08.720 --> 00:42:09.680
[Isla sighs]

00:42:19.400 --> 00:42:21.400
[Tim] It's plain as day to me.

00:42:22.880 --> 00:42:28.120
These services are killing people...
and causing people to kill themselves.

00:42:29.080 --> 00:42:33.280
I don't know any parent who says, "Yeah,
I really want my kids to be growing up

00:42:33.360 --> 00:42:36.880
feeling manipulated by tech designers, uh,

00:42:36.960 --> 00:42:39.720
manipulating their attention,
making it impossible to do their homework,

00:42:39.800 --> 00:42:42.560
making them compare themselves
to unrealistic standards of beauty."

00:42:42.640 --> 00:42:44.680
Like, no one wants that. [chuckles]

00:42:45.080 --> 00:42:46.360
No one does.

00:42:46.440 --> 00:42:48.480
We... We used to have these protections.

00:42:48.560 --> 00:42:50.920
When children watched
Saturday morning cartoons,

00:42:51.040 --> 00:42:52.760
we cared about protecting children.

00:42:52.840 --> 00:42:56.560
We would say, "You can't advertise
to these age children in these ways."

00:42:57.360 --> 00:42:58.760
But then you take YouTube for Kids,

00:42:58.880 --> 00:43:02.440
and it gobbles up that entire portion
of the attention economy,

00:43:02.520 --> 00:43:04.920
and now all kids are exposed
to YouTube for Kids.

00:43:05.000 --> 00:43:07.720
And all those protections
and all those regulations are gone.

00:43:08.200 --> 00:43:10.200
[tense instrumental music playing]

00:43:18.280 --> 00:43:22.120
[Tristan] We're training and conditioning
a whole new generation of people...

00:43:23.440 --> 00:43:29.160
that when we are uncomfortable or lonely
or uncertain or afraid,

00:43:29.240 --> 00:43:31.760
we have a digital pacifier for ourselves

00:43:32.240 --> 00:43:36.480
that is kind of atrophying our own ability
to deal with that.

00:43:53.880 --> 00:43:55.680
[Tristan] Photoshop didn't have
1,000 engineers

00:43:55.760 --> 00:43:58.960
on the other side of the screen,
using notifications, using your friends,

00:43:59.040 --> 00:44:02.440
using AI to predict what's gonna
perfectly addict you, or hook you,

00:44:02.520 --> 00:44:04.520
or manipulate you, or allow advertisers

00:44:04.600 --> 00:44:08.440
to test 60,000 variations
of text or colors to figure out

00:44:08.520 --> 00:44:11.040
what's the perfect manipulation
of your mind.

00:44:11.160 --> 00:44:14.960
This is a totally new species
of power and influence.

00:44:16.080 --> 00:44:19.160
I... I would say, again, the methods used

00:44:19.240 --> 00:44:22.760
to play on people's ability
to be addicted or to be influenced

00:44:22.880 --> 00:44:25.200
may be different this time,
and they probably are different.

00:44:25.280 --> 00:44:28.760
They were different when newspapers
came in and the printing press came in,

00:44:28.840 --> 00:44:31.840
and they were different
when television came in,

00:44:31.920 --> 00:44:34.000
and you had three major networks and...

00:44:34.440 --> 00:44:36.400
-At the time.
-At the time. That's what I'm saying.

00:44:36.520 --> 00:44:38.360
But I'm saying the idea
that there's a new level

00:44:38.480 --> 00:44:42.040
and that new level has happened
so many times before.

00:44:42.120 --> 00:44:45.080
I mean, this is just the latest new level
that we've seen.

00:44:45.160 --> 00:44:48.720
There's this narrative that, you know,
"We'll just adapt to it.

00:44:48.800 --> 00:44:51.200
We'll learn how to live
with these devices,

00:44:51.280 --> 00:44:53.720
just like we've learned how to live
with everything else."

00:44:53.800 --> 00:44:56.680
And what this misses
is there's something distinctly new here.

00:44:57.480 --> 00:45:00.160
Perhaps the most dangerous piece
of all this is the fact

00:45:00.240 --> 00:45:04.400
that it's driven by technology
that's advancing exponentially.

00:45:05.880 --> 00:45:09.080
Roughly, if you say from, like,
the 1960s to today,

00:45:09.880 --> 00:45:12.960
processing power has gone up
about a trillion times.

00:45:13.800 --> 00:45:18.320
Nothing else that we have has improved
at anything near that rate.

00:45:18.400 --> 00:45:22.160
Like, cars are, you know,
roughly twice as fast.

00:45:22.240 --> 00:45:25.000
And almost everything else is negligible.

00:45:25.360 --> 00:45:27.160
And perhaps most importantly,

00:45:27.280 --> 00:45:31.360
our human-- our physiology,
our brains have evolved not at all.

00:45:37.400 --> 00:45:41.480
[Tristan] Human beings, at a mind and body
and sort of physical level,

00:45:41.960 --> 00:45:43.880
are not gonna fundamentally change.

00:45:44.800 --> 00:45:45.880
[indistinct chatter]

00:45:47.040 --> 00:45:48.960
[chuckling] I know, but they...

00:45:49.040 --> 00:45:51.600
[continues speaking indistinctly]

00:45:53.560 --> 00:45:54.760
[camera shutter clicks]

00:45:56.840 --> 00:46:00.920
[Tristan] We can do genetic engineering
and develop new kinds of human beings,

00:46:01.000 --> 00:46:05.200
but realistically speaking,
you're living inside of hardware, a brain,

00:46:05.280 --> 00:46:07.200
that was, like, millions of years old,

00:46:07.320 --> 00:46:10.560
and then there's this screen, and then
on the opposite side of the screen,

00:46:10.640 --> 00:46:13.560
there's these thousands of engineers
and supercomputers

00:46:13.640 --> 00:46:16.120
that have goals that are different
than your goals,

00:46:16.200 --> 00:46:19.680
and so, who's gonna win in that game?
Who's gonna win?

00:46:25.680 --> 00:46:26.600
How are we losing?

00:46:27.160 --> 00:46:29.840
-I don't know.
-Where is he? This is not normal.

00:46:29.920 --> 00:46:32.080
Did I overwhelm him
with friends and family content?

00:46:32.160 --> 00:46:34.080
-Probably.
-Well, maybe it was all the ads.

00:46:34.160 --> 00:46:37.800
No. Something's very wrong.
Let's switch to resurrection mode.

00:46:39.720 --> 00:46:44.040
[Tristan] When you think of AI,
you know, an AI's gonna ruin the world,

00:46:44.120 --> 00:46:47.200
and you see, like, a Terminator,
and you see Arnold Schwarzenegger.

00:46:47.640 --> 00:46:48.680
I'll be back.

00:46:48.760 --> 00:46:50.920
[Tristan] You see drones,
and you think, like,

00:46:51.000 --> 00:46:52.680
"Oh, we're gonna kill people with AI."

00:46:53.640 --> 00:46:59.800
And what people miss is that AI
already runs today's world right now.

00:46:59.880 --> 00:47:03.240
Even talking about "an AI"
is just a metaphor.

00:47:03.320 --> 00:47:09.440
At these companies like... like Google,
there's just massive, massive rooms,

00:47:10.320 --> 00:47:13.120
some of them underground,
some of them underwater,

00:47:13.200 --> 00:47:14.480
of just computers.

00:47:14.560 --> 00:47:17.840
Tons and tons of computers,
as far as the eye can see.

00:47:18.440 --> 00:47:20.480
They're deeply interconnected
with each other

00:47:20.600 --> 00:47:22.920
and running
extremely complicated programs,

00:47:23.000 --> 00:47:26.000
sending information back and forth
between each other all the time.

00:47:26.800 --> 00:47:28.600
And they'll be running
many different programs,

00:47:28.680 --> 00:47:31.000
many different products
on those same machines.

00:47:31.360 --> 00:47:33.680
Some of those things could be described
as simple algorithms,

00:47:33.760 --> 00:47:35.240
some could be described as algorithms

00:47:35.320 --> 00:47:37.520
that are so complicated,
you would call them intelligence.

00:47:39.000 --> 00:47:39.960
[crew member sighs]

00:47:40.040 --> 00:47:42.560
[Cathy]
I like to say that algorithms are opinions

00:47:42.640 --> 00:47:43.760
embedded in code...

00:47:45.080 --> 00:47:47.640
and that algorithms are not objective.

00:47:48.360 --> 00:47:51.560
Algorithms are optimized
to some definition of success.

00:47:52.240 --> 00:47:53.360
So, if you can imagine,

00:47:53.440 --> 00:47:57.120
if a... if a commercial enterprise builds
an algorithm

00:47:57.200 --> 00:47:59.280
to their definition of success,

00:47:59.840 --> 00:48:01.200
it's a commercial interest.

00:48:01.600 --> 00:48:02.680
It's usually profit.

00:48:03.120 --> 00:48:07.360
You are giving the computer
the goal state, "I want this outcome,"

00:48:07.480 --> 00:48:10.240
and then the computer itself is learning
how to do it.

00:48:10.320 --> 00:48:12.600
That's where the term "machine learning"
comes from.

00:48:12.680 --> 00:48:14.840
And so, every day, it gets slightly better

00:48:14.920 --> 00:48:16.960
at picking the right posts
in the right order

00:48:17.040 --> 00:48:19.440
so that you spend longer and longer
in that product.

00:48:19.520 --> 00:48:22.240
And no one really understands
what they're doing

00:48:22.320 --> 00:48:23.880
in order to achieve that goal.

00:48:23.960 --> 00:48:28.240
The algorithm has a mind of its own,
so even though a person writes it,

00:48:28.920 --> 00:48:30.640
it's written in a way

00:48:30.720 --> 00:48:35.040
that you kind of build the machine,
and then the machine changes itself.

00:48:35.120 --> 00:48:37.880
There's only a handful of people
at these companies,

00:48:37.960 --> 00:48:40.000
at Facebook and Twitter
and other companies...

00:48:40.080 --> 00:48:43.800
There's only a few people who understand
how those systems work,

00:48:43.880 --> 00:48:46.720
and even they don't necessarily
fully understand

00:48:46.800 --> 00:48:49.560
what's gonna happen
with a particular piece of content.

00:48:49.960 --> 00:48:55.480
So, as humans, we've almost lost control
over these systems.

00:48:55.880 --> 00:48:59.600
Because they're controlling, you know,
the information that we see,

00:48:59.680 --> 00:49:02.200
they're controlling us more
than we're controlling them.

00:49:02.520 --> 00:49:04.720
-[console whirs]
-[Growth AI] Cross-referencing him

00:49:04.800 --> 00:49:07.320
against comparables
in his geographic zone.

00:49:07.400 --> 00:49:09.560
His psychometric doppelgangers.

00:49:09.640 --> 00:49:13.680
There are 13,694 people
behaving just like him in his region.

00:49:13.760 --> 00:49:16.360
-What's trending with them?
-We need something actually good

00:49:16.440 --> 00:49:17.680
for a proper resurrection,

00:49:17.800 --> 00:49:19.960
given that the typical stuff
isn't working.

00:49:20.040 --> 00:49:21.880
Not even that cute girl from school.

00:49:22.320 --> 00:49:25.240
My analysis shows that going political
with Extreme Center content

00:49:25.320 --> 00:49:28.240
has a 62.3 percent chance
of long-term engagement.

00:49:28.320 --> 00:49:29.280
That's not bad.

00:49:29.360 --> 00:49:32.000
[sighs] It's not good enough to lead with.

00:49:32.280 --> 00:49:35.280
Okay, okay, so we've tried notifying him
about tagged photos,

00:49:35.400 --> 00:49:39.000
invitations, current events,
even a direct message from Rebecca.

00:49:39.080 --> 00:49:42.800
But what about User 01265923010?

00:49:42.880 --> 00:49:44.640
Yeah, Ben loved all of her posts.

00:49:44.720 --> 00:49:47.760
For months and, like,
literally all of them, and then nothing.

00:49:47.840 --> 00:49:50.440
I calculate a 92.3 percent chance
of resurrection

00:49:50.520 --> 00:49:52.040
with a notification about Ana.

00:49:56.520 --> 00:49:57.480
And her new friend.

00:49:59.600 --> 00:50:01.600
[eerie instrumental music playing]

00:50:10.600 --> 00:50:11.680
[cell phone vibrates]

00:50:25.680 --> 00:50:27.440
[Ben] Oh, you gotta be kiddin' me.

00:50:32.400 --> 00:50:33.600
Uh... [sighs]

00:50:35.640 --> 00:50:36.600
Okay.

00:50:38.880 --> 00:50:41.000
-What?
-[fanfare plays, fireworks pop]

00:50:41.400 --> 00:50:42.800
[claps] Bam! We're back!

00:50:42.880 --> 00:50:44.360
Let's get back to making money, boys.

00:50:44.440 --> 00:50:46.320
Yes, and connecting Ben
with the entire world.

00:50:46.400 --> 00:50:49.080
I'm giving him access
to all the information he might like.

00:50:49.760 --> 00:50:53.720
Hey, do you guys ever wonder if, you know,
like, the feed is good for Ben?

00:50:57.080 --> 00:50:58.440
-No.
-No. [chuckles slightly]

00:51:00.320 --> 00:51:03.280
-[chuckles softly]
-["I Put a Spell on You" playing]

00:51:17.480 --> 00:51:19.080
 I put a spell on you 

00:51:25.040 --> 00:51:26.360
 'Cause you're mine 

00:51:28.640 --> 00:51:32.080
[vocalizing]  Ah! 

00:51:34.520 --> 00:51:36.600
 You better stop the things you do 

00:51:41.160 --> 00:51:42.280
 I ain't lyin' 

00:51:44.960 --> 00:51:46.680
 No, I ain't lyin' 

00:51:49.960 --> 00:51:51.800
 You know I can't stand it 

00:51:53.040 --> 00:51:54.600
 You're runnin' around 

00:51:55.600 --> 00:51:57.240
 You know better, Daddy 

00:51:58.760 --> 00:52:02.080
 I can't stand it
'Cause you put me down 

00:52:03.280 --> 00:52:04.120
 Yeah, yeah 

00:52:06.440 --> 00:52:08.360
 I put a spell on you 

00:52:12.360 --> 00:52:14.840
 Because you're mine 

00:52:18.720 --> 00:52:19.840
 You're mine 

00:52:20.920 --> 00:52:24.360
[Roger] So, imagine you're on Facebook...

00:52:24.760 --> 00:52:29.320
and you're effectively playing
against this artificial intelligence

00:52:29.400 --> 00:52:31.320
that knows everything about you,

00:52:31.400 --> 00:52:34.560
can anticipate your next move,
and you know literally nothing about it,

00:52:34.640 --> 00:52:37.400
except that there are cat videos
and birthdays on it.

00:52:37.800 --> 00:52:39.640
That's not a fair fight.

00:52:41.560 --> 00:52:43.880
Ben and Jerry, it's time to go, bud!

00:52:48.040 --> 00:52:48.880
[sighs]

00:52:51.120 --> 00:52:51.960
Ben?

00:53:01.000 --> 00:53:02.120
[knocks lightly on door]

00:53:02.680 --> 00:53:04.720
-[Cass] Ben.
-[Ben] Mm.

00:53:05.160 --> 00:53:06.040
Come on.

00:53:07.200 --> 00:53:08.360
School time. [claps]

00:53:08.440 --> 00:53:09.280
Let's go.

00:53:12.200 --> 00:53:13.160
[Ben sighs]

00:53:25.120 --> 00:53:27.120
[excited chatter]

00:53:31.360 --> 00:53:33.640
-[tech] How you doing today?
-Oh, I'm... I'm nervous.

00:53:33.720 --> 00:53:35.000
-Are ya?
-Yeah. [chuckles]

00:53:37.360 --> 00:53:39.120
[Tristan]
We were all looking for the moment

00:53:39.200 --> 00:53:42.960
when technology would overwhelm
human strengths and intelligence.

00:53:43.040 --> 00:53:47.000
When is it gonna cross the singularity,
replace our jobs, be smarter than humans?

00:53:48.120 --> 00:53:50.080
But there's this much earlier moment...

00:53:50.960 --> 00:53:55.320
when technology exceeds
and overwhelms human weaknesses.

00:53:57.480 --> 00:54:01.760
This point being crossed
is at the root of addiction,

00:54:02.120 --> 00:54:04.720
polarization, radicalization,
outrage-ification,

00:54:04.800 --> 00:54:06.360
vanity-ification, the entire thing.

00:54:07.680 --> 00:54:09.920
This is overpowering human nature,

00:54:10.520 --> 00:54:13.480
and this is checkmate on humanity.

00:54:20.120 --> 00:54:21.880
-[sighs deeply]
-[door opens]

00:54:30.560 --> 00:54:31.840
I'm sorry. [sighs]

00:54:37.600 --> 00:54:39.600
-[seat belt clicks]
-[engine starts]

00:54:41.720 --> 00:54:44.640
[Jaron] One of the ways
I try to get people to understand

00:54:45.200 --> 00:54:49.840
just how wrong feeds from places
like Facebook are

00:54:49.920 --> 00:54:51.440
is to think about the Wikipedia.

00:54:52.960 --> 00:54:56.200
When you go to a page, you're seeing
the same thing as other people.

00:54:56.560 --> 00:55:00.280
So, it's one of the few things online
that we at least hold in common.

00:55:00.360 --> 00:55:03.400
Now, just imagine for a second
that Wikipedia said,

00:55:03.520 --> 00:55:07.160
"We're gonna give each person
a different customized definition,

00:55:07.240 --> 00:55:09.480
and we're gonna be paid by people
for that."

00:55:09.560 --> 00:55:13.440
So, Wikipedia would be spying on you.
Wikipedia would calculate,

00:55:13.520 --> 00:55:17.200
"What's the thing I can do
to get this person to change a little bit

00:55:17.280 --> 00:55:19.880
on behalf of some commercial interest?"
Right?

00:55:19.960 --> 00:55:21.800
And then it would change the entry.

00:55:22.440 --> 00:55:24.720
Can you imagine that?
Well, you should be able to,

00:55:24.800 --> 00:55:26.800
'cause that's exactly what's happening
on Facebook.

00:55:26.920 --> 00:55:29.000
It's exactly what's happening
in your YouTube feed.

00:55:29.080 --> 00:55:31.800
When you go to Google and type in
"Climate change is,"

00:55:31.880 --> 00:55:35.000
you're going to see different results
depending on where you live.

00:55:36.160 --> 00:55:38.440
In certain cities,
you're gonna see it autocomplete

00:55:38.520 --> 00:55:40.440
with "climate change is a hoax."

00:55:40.520 --> 00:55:42.080
In other cases, you're gonna see

00:55:42.160 --> 00:55:44.840
"climate change is causing the destruction
of nature."

00:55:44.920 --> 00:55:48.440
And that's a function not
of what the truth is about climate change,

00:55:48.520 --> 00:55:51.080
but about
where you happen to be Googling from

00:55:51.160 --> 00:55:54.080
and the particular things
Google knows about your interests.

00:55:54.840 --> 00:55:58.000
Even two friends
who are so close to each other,

00:55:58.080 --> 00:56:00.200
who have almost the exact same set
of friends,

00:56:00.280 --> 00:56:02.800
they think, you know,
"I'm going to news feeds on Facebook.

00:56:02.880 --> 00:56:05.400
I'll see the exact same set of updates."

00:56:05.480 --> 00:56:06.720
But it's not like that at all.

00:56:06.800 --> 00:56:08.440
They see completely different worlds

00:56:08.520 --> 00:56:10.560
because they're based
on these computers calculating

00:56:10.640 --> 00:56:12.040
what's perfect for each of them.

00:56:12.120 --> 00:56:14.240
[whistling over monitor]

00:56:14.320 --> 00:56:18.400
[Roger] The way to think about it
is it's 2.7 billion Truman Shows.

00:56:18.480 --> 00:56:21.280
Each person has their own reality,
with their own...

00:56:22.680 --> 00:56:23.680
facts.

00:56:23.760 --> 00:56:27.000
Why do you think
that, uh, Truman has never come close

00:56:27.080 --> 00:56:30.080
to discovering the true nature
of his world until now?

00:56:31.040 --> 00:56:34.120
We accept the reality of the world
with which we're presented.

00:56:34.200 --> 00:56:35.120
It's as simple as that.

00:56:36.480 --> 00:56:41.040
Over time, you have the false sense
that everyone agrees with you,

00:56:41.160 --> 00:56:44.080
because everyone in your news feed
sounds just like you.

00:56:44.560 --> 00:56:49.080
And that once you're in that state,
it turns out you're easily manipulated,

00:56:49.160 --> 00:56:51.720
the same way you would be manipulated
by a magician.

00:56:51.800 --> 00:56:55.360
A magician shows you a card trick
and says, "Pick a card, any card."

00:56:55.440 --> 00:56:58.160
What you don't realize
was that they've done a set-up,

00:56:58.440 --> 00:57:00.560
so you pick the card
they want you to pick.

00:57:00.680 --> 00:57:03.160
And that's how Facebook works.
Facebook sits there and says,

00:57:03.240 --> 00:57:06.160
"Hey, you pick your friends.
You pick the links that you follow."

00:57:06.240 --> 00:57:08.720
But that's all nonsense.
It's just like the magician.

00:57:08.800 --> 00:57:11.280
Facebook is in charge of your news feed.

00:57:11.400 --> 00:57:14.520
We all simply are operating
on a different set of facts.

00:57:14.600 --> 00:57:16.480
When that happens at scale,

00:57:16.560 --> 00:57:20.640
you're no longer able to reckon with
or even consume information

00:57:20.720 --> 00:57:23.680
that contradicts with that world view
that you've created.

00:57:23.760 --> 00:57:26.440
That means we aren't actually being
objective,

00:57:26.520 --> 00:57:28.320
constructive individuals. [chuckles]

00:57:28.400 --> 00:57:32.440
[crowd chanting] Open up your eyes,
don't believe the lies! Open up...

00:57:32.520 --> 00:57:34.680
[Justin] And then you look
over at the other side,

00:57:35.240 --> 00:57:38.760
and you start to think,
"How can those people be so stupid?

00:57:38.840 --> 00:57:42.120
Look at all of this information
that I'm constantly seeing.

00:57:42.200 --> 00:57:44.640
How are they not seeing
that same information?"

00:57:44.720 --> 00:57:47.280
And the answer is, "They're not seeing
that same information."

00:57:47.360 --> 00:57:50.800
[crowd continues chanting]
Open up your eyes, don't believe the lies!

00:57:50.880 --> 00:57:52.000
[shouting indistinctly]

00:57:52.080 --> 00:57:55.480
-[interviewer] What are Republicans like?
-People that don't have a clue.

00:57:55.560 --> 00:57:58.920
The Democrat Party is a crime syndicate,
not a real political party.

00:57:59.000 --> 00:58:03.200
A huge new Pew Research Center study
of 10,000 American adults

00:58:03.280 --> 00:58:05.320
finds us more divided than ever,

00:58:05.400 --> 00:58:09.160
with personal and political polarization
at a 20-year high.

00:58:11.720 --> 00:58:14.200
[pundit] You have
more than a third of Republicans saying

00:58:14.280 --> 00:58:16.840
the Democratic Party is a threat
to the nation,

00:58:16.920 --> 00:58:20.560
more than a quarter of Democrats saying
the same thing about the Republicans.

00:58:20.640 --> 00:58:22.480
So many of the problems
that we're discussing,

00:58:22.560 --> 00:58:24.400
like, around political polarization

00:58:24.480 --> 00:58:28.040
exist in spades on cable television.

00:58:28.120 --> 00:58:31.000
The media has this exact same problem,

00:58:31.080 --> 00:58:33.320
where their business model, by and large,

00:58:33.440 --> 00:58:35.760
is that they're selling our attention
to advertisers.

00:58:35.840 --> 00:58:38.880
And the Internet is just a new,
even more efficient way to do that.

00:58:40.120 --> 00:58:44.120
[Guillaume] At YouTube, I was working
on YouTube recommendations.

00:58:44.240 --> 00:58:47.160
It worries me that an algorithm
that I worked on

00:58:47.240 --> 00:58:50.400
is actually increasing polarization
in society.

00:58:50.480 --> 00:58:53.120
But from the point of view of watch time,

00:58:53.200 --> 00:58:57.600
this polarization is extremely efficient
at keeping people online.

00:58:58.760 --> 00:59:00.880
The only reason
these teachers are teaching this stuff

00:59:00.960 --> 00:59:02.280
is 'cause they're getting paid to.

00:59:02.360 --> 00:59:04.360
-It's absolutely absurd.
-[Cass] Hey, Benji.

00:59:04.920 --> 00:59:06.280
No soccer practice today?

00:59:06.360 --> 00:59:08.800
Oh, there is. I'm just catching up
on some news stuff.

00:59:08.880 --> 00:59:11.520
[vlogger] Do research. Anything
that sways from the Extreme Center--

00:59:11.600 --> 00:59:14.000
Wouldn't exactly call the stuff
that you're watching news.

00:59:15.560 --> 00:59:18.840
You're always talking about how messed up
everything is. So are they.

00:59:19.280 --> 00:59:21.120
But that stuff is just propaganda.

00:59:21.200 --> 00:59:24.040
[vlogger] Neither is true.
It's all about what makes sense.

00:59:24.760 --> 00:59:26.920
Ben, I'm serious.
That stuff is bad for you.

00:59:27.000 --> 00:59:29.240
-You should go to soccer practice.
-[Ben] Mm.

00:59:31.120 --> 00:59:31.920
[Cass sighs]

00:59:35.160 --> 00:59:37.480
I share this stuff because I care.

00:59:37.560 --> 00:59:41.080
I care that you are being misled,
and it's not okay. All right?

00:59:41.160 --> 00:59:43.120
[Guillaume] People think
the algorithm is designed

00:59:43.200 --> 00:59:46.840
to give them what they really want,
only it's not.

00:59:46.920 --> 00:59:52.600
The algorithm is actually trying to find
a few rabbit holes that are very powerful,

00:59:52.680 --> 00:59:56.200
trying to find which rabbit hole
is the closest to your interest.

00:59:56.280 --> 00:59:59.240
And then if you start watching
one of those videos,

00:59:59.840 --> 01:00:02.200
then it will recommend it
over and over again.

01:00:02.680 --> 01:00:04.920
It's not like anybody wants this
to happen.

01:00:05.000 --> 01:00:07.800
It's just that this is
what the recommendation system is doing.

01:00:07.880 --> 01:00:10.800
So much so that Kyrie Irving,
the famous basketball player,

01:00:11.040 --> 01:00:14.240
uh, said he believed the Earth was flat,
and he apologized later

01:00:14.320 --> 01:00:16.160
because he blamed it
on a YouTube rabbit hole.

01:00:16.480 --> 01:00:18.640
You know, like,
you click the YouTube click

01:00:18.720 --> 01:00:21.520
and it goes, like,
how deep the rabbit hole goes.

01:00:21.600 --> 01:00:23.360
When he later came on to NPR to say,

01:00:23.440 --> 01:00:25.960
"I'm sorry for believing this.
I didn't want to mislead people,"

01:00:26.040 --> 01:00:28.280
a bunch of students in a classroom
were interviewed saying,

01:00:28.360 --> 01:00:29.680
"The round-Earthers got to him."

01:00:29.760 --> 01:00:30.960
[audience chuckles]

01:00:31.040 --> 01:00:33.960
The flat-Earth conspiracy theory
was recommended

01:00:34.040 --> 01:00:37.640
hundreds of millions of times
by the algorithm.

01:00:37.720 --> 01:00:43.880
It's easy to think that it's just
a few stupid people who get convinced,

01:00:43.960 --> 01:00:46.880
but the algorithm is getting smarter
and smarter every day.

01:00:46.960 --> 01:00:50.200
So, today, they are convincing the people
that the Earth is flat,

01:00:50.280 --> 01:00:53.960
but tomorrow, they will be convincing you
of something that's false.

01:00:54.320 --> 01:00:57.800
[reporter] On November 7th,
the hashtag "Pizzagate" was born.

01:00:57.880 --> 01:00:59.200
[Rene] Pizzagate...

01:01:00.120 --> 01:01:01.440
[clicks tongue] Oh, boy.

01:01:01.520 --> 01:01:02.520
Uh... [laughs]

01:01:03.160 --> 01:01:06.920
I still am not 100 percent sure
how this originally came about,

01:01:07.000 --> 01:01:12.360
but the idea that ordering a pizza
meant ordering a trafficked person.

01:01:12.440 --> 01:01:15.040
As the groups got bigger on Facebook,

01:01:15.120 --> 01:01:19.960
Facebook's recommendation engine
started suggesting to regular users

01:01:20.040 --> 01:01:21.760
that they join Pizzagate groups.

01:01:21.840 --> 01:01:27.400
So, if a user was, for example,
anti-vaccine or believed in chemtrails

01:01:27.480 --> 01:01:30.640
or had indicated to Facebook's algorithms
in some way

01:01:30.720 --> 01:01:33.400
that they were prone to belief
in conspiracy theories,

01:01:33.480 --> 01:01:36.840
Facebook's recommendation engine
would serve them Pizzagate groups.

01:01:36.920 --> 01:01:41.080
Eventually, this culminated in
a man showing up with a gun,

01:01:41.160 --> 01:01:44.600
deciding that he was gonna go liberate
the children from the basement

01:01:44.680 --> 01:01:46.920
of the pizza place
that did not have a basement.

01:01:47.000 --> 01:01:48.520
[officer 1] What were you doing?

01:01:48.880 --> 01:01:50.480
[man] Making sure
there was nothing there.

01:01:50.560 --> 01:01:52.440
-[officer 1] Regarding?
-[man] Pedophile ring.

01:01:52.520 --> 01:01:54.280
-[officer 1] What?
-[man] Pedophile ring.

01:01:54.360 --> 01:01:55.960
[officer 2] He's talking about Pizzagate.

01:01:56.040 --> 01:02:00.200
This is an example of a conspiracy theory

01:02:00.280 --> 01:02:03.680
that was propagated
across all social networks.

01:02:03.760 --> 01:02:06.080
The social network's
own recommendation engine

01:02:06.160 --> 01:02:07.960
is voluntarily serving this up to people

01:02:08.040 --> 01:02:10.640
who had never searched
for the term "Pizzagate" in their life.

01:02:12.440 --> 01:02:14.440
[Tristan] There's a study, an MIT study,

01:02:14.520 --> 01:02:19.800
that fake news on Twitter spreads
six times faster than true news.

01:02:19.880 --> 01:02:21.840
What is that world gonna look like

01:02:21.960 --> 01:02:24.720
when one has a six-times advantage
to the other one?

01:02:25.280 --> 01:02:27.640
You can imagine
these things are sort of like...

01:02:27.720 --> 01:02:31.720
they... they tilt the floor
of... of human behavior.

01:02:31.800 --> 01:02:34.720
They make some behavior harder
and some easier.

01:02:34.800 --> 01:02:37.400
And you're always free
to walk up the hill,

01:02:37.480 --> 01:02:38.800
but fewer people do,

01:02:38.880 --> 01:02:43.080
and so, at scale, at society's scale,
you really are just tilting the floor

01:02:43.160 --> 01:02:45.960
and changing what billions of people think
and do.

01:02:46.040 --> 01:02:52.000
We've created a system
that biases towards false information.

01:02:52.640 --> 01:02:54.440
Not because we want to,

01:02:54.520 --> 01:02:58.800
but because false information makes
the companies more money

01:02:59.400 --> 01:03:01.320
than the truth. The truth is boring.

01:03:02.000 --> 01:03:04.480
It's a disinformation-for-profit
business model.

01:03:04.920 --> 01:03:08.160
You make money the more you allow
unregulated messages

01:03:08.680 --> 01:03:11.280
to reach anyone for the best price.

01:03:11.640 --> 01:03:13.960
Because climate change? Yeah.

01:03:14.040 --> 01:03:16.760
It's a hoax. Yeah, it's real.
That's the point.

01:03:16.840 --> 01:03:20.040
The more they talk about it
and the more they divide us,

01:03:20.120 --> 01:03:22.400
the more they have the power,
the more...

01:03:22.520 --> 01:03:25.480
[Tristan] Facebook has trillions
of these news feed posts.

01:03:26.560 --> 01:03:29.160
They can't know what's real
or what's true...

01:03:29.960 --> 01:03:33.720
which is why this conversation
is so critical right now.

01:03:33.800 --> 01:03:37.000
[reporter 1] It's not just COVID-19
that's spreading fast.

01:03:37.080 --> 01:03:40.200
There's a flow of misinformation online
about the virus.

01:03:40.280 --> 01:03:41.800
[reporter 2] The notion
drinking water

01:03:41.880 --> 01:03:43.680
will flush coronavirus from your system

01:03:43.760 --> 01:03:47.480
is one of several myths about the virus
circulating on social media.

01:03:47.560 --> 01:03:50.440
[automated voice] The government planned
this event, created the virus,

01:03:50.520 --> 01:03:53.600
and had a simulation
of how the countries would react.

01:03:53.960 --> 01:03:55.560
Coronavirus is a... a hoax.

01:03:56.160 --> 01:03:57.960
[man] SARS, coronavirus.

01:03:58.360 --> 01:04:01.040
And look at when it was made. 2018.

01:04:01.120 --> 01:04:03.800
I think the US government started
this shit.

01:04:04.200 --> 01:04:09.080
Nobody is sick. Nobody is sick.
Nobody knows anybody who's sick.

01:04:09.520 --> 01:04:13.000
Maybe the government is using
the coronavirus as an excuse

01:04:13.080 --> 01:04:15.640
to get everyone to stay inside
because something else is happening.

01:04:15.720 --> 01:04:18.000
Coronavirus is not killing people,

01:04:18.080 --> 01:04:20.920
it's the 5G radiation
that they're pumping out.

01:04:21.000 --> 01:04:22.520
[crowd shouting]

01:04:22.600 --> 01:04:24.920
[Tristan]
We're being bombarded with rumors.

01:04:25.400 --> 01:04:28.800
People are blowing up
actual physical cell phone towers.

01:04:28.920 --> 01:04:32.200
We see Russia and China spreading rumors
and conspiracy theories.

01:04:32.280 --> 01:04:35.240
[reporter 3] This morning,
panic and protest in Ukraine as...

01:04:35.320 --> 01:04:38.920
[Tristan] People have no idea what's true,
and now it's a matter of life and death.

01:04:39.880 --> 01:04:42.640
[woman] Those sources that are spreading
coronavirus misinformation

01:04:42.720 --> 01:04:45.800
have amassed
something like 52 million engagements.

01:04:45.880 --> 01:04:50.080
You're saying that silver solution
would be effective.

01:04:50.160 --> 01:04:54.120
Well, let's say it hasn't been tested
on this strain of the coronavirus, but...

01:04:54.200 --> 01:04:57.240
[Tristan] What we're seeing with COVID
is just an extreme version

01:04:57.320 --> 01:05:00.520
of what's happening
across our information ecosystem.

01:05:00.920 --> 01:05:05.040
Social media amplifies exponential gossip
and exponential hearsay

01:05:05.120 --> 01:05:07.120
to the point
that we don't know what's true,

01:05:07.200 --> 01:05:08.960
no matter what issue we care about.

01:05:15.160 --> 01:05:16.560
[teacher] He discovers this.

01:05:16.640 --> 01:05:18.640
[continues lecturing indistinctly]

01:05:19.880 --> 01:05:21.280
[Rebecca whispers] Ben.

01:05:26.120 --> 01:05:28.240
-Are you still on the team?
-[Ben] Mm-hmm.

01:05:30.360 --> 01:05:32.680
[Rebecca] Okay, well,
I'm gonna get a snack before practice

01:05:32.760 --> 01:05:34.440
if you... wanna come.

01:05:35.640 --> 01:05:36.520
[Ben] Hm?

01:05:36.960 --> 01:05:38.600
[Rebecca] You know, never mind.

01:05:38.680 --> 01:05:40.680
[footsteps fading]

01:05:45.080 --> 01:05:47.520
[vlogger] Nine out of ten people
are dissatisfied right now.

01:05:47.600 --> 01:05:50.600
The EC is like any political movement
in history, when you think about it.

01:05:50.680 --> 01:05:54.480
We are standing up, and we are...
we are standing up to this noise.

01:05:54.560 --> 01:05:57.040
You are my people. I trust you guys.

01:05:59.240 --> 01:06:02.560
-The Extreme Center content is brilliant.
-He absolutely loves it.

01:06:02.680 --> 01:06:03.640
Running an auction.

01:06:04.640 --> 01:06:08.560
840 bidders. He sold for 4.35 cents
to a weapons manufacturer.

01:06:08.640 --> 01:06:10.800
Let's promote some of these events.

01:06:10.880 --> 01:06:13.520
Upcoming rallies in his geographic zone
later this week.

01:06:13.600 --> 01:06:15.160
I've got a new vlogger lined up, too.

01:06:15.240 --> 01:06:16.240
[chuckles]

01:06:17.760 --> 01:06:22.960
And... and, honestly, I'm telling you,
I'm willing to do whatever it takes.

01:06:23.040 --> 01:06:24.920
And I mean whatever.

01:06:32.160 --> 01:06:33.200
-Subscribe...
-[Cass] Ben?

01:06:33.280 --> 01:06:35.920
...and also come back
because I'm telling you, yo...

01:06:36.000 --> 01:06:38.880
-[knocking on door]
-...I got some real big things comin'.

01:06:38.960 --> 01:06:40.160
Some real big things.

01:06:40.800 --> 01:06:45.280
[Roger] One of the problems with Facebook
is that, as a tool of persuasion,

01:06:45.800 --> 01:06:47.920
it may be the greatest thing ever created.

01:06:48.000 --> 01:06:52.520
Now, imagine what that means in the hands
of a dictator or an authoritarian.

01:06:53.720 --> 01:06:57.640
If you want to control the population
of your country,

01:06:57.720 --> 01:07:01.320
there has never been a tool
as effective as Facebook.

01:07:04.920 --> 01:07:07.400
[Cynthia]
Some of the most troubling implications

01:07:07.480 --> 01:07:10.960
of governments and other bad actors
weaponizing social media,

01:07:11.240 --> 01:07:13.600
um, is that it has led
to real, offline harm.

01:07:13.680 --> 01:07:15.080
I think the most prominent example

01:07:15.160 --> 01:07:17.640
that's gotten a lot of press
is what's happened in Myanmar.

01:07:19.240 --> 01:07:21.200
In Myanmar,
when people think of the Internet,

01:07:21.280 --> 01:07:22.920
what they are thinking about is Facebook.

01:07:23.000 --> 01:07:25.920
And what often happens is
when people buy their cell phone,

01:07:26.000 --> 01:07:29.920
the cell phone shop owner will actually
preload Facebook on there for them

01:07:30.000 --> 01:07:31.480
and open an account for them.

01:07:31.600 --> 01:07:34.880
And so when people get their phone,
the first thing they open

01:07:34.960 --> 01:07:37.600
and the only thing they know how to open
is Facebook.

01:07:38.160 --> 01:07:41.880
Well, a new bombshell investigation
exposes Facebook's growing struggle

01:07:41.960 --> 01:07:43.800
to tackle hate speech in Myanmar.

01:07:43.880 --> 01:07:46.000
[crowd shouting]

01:07:46.080 --> 01:07:49.200
Facebook really gave the military
and other bad actors

01:07:49.280 --> 01:07:51.760
a new way to manipulate public opinion

01:07:51.840 --> 01:07:55.520
and to help incite violence
against the Rohingya Muslims

01:07:55.600 --> 01:07:57.400
that included mass killings,

01:07:58.120 --> 01:07:59.880
burning of entire villages,

01:07:59.960 --> 01:08:03.680
mass rape, and other serious crimes
against humanity

01:08:03.800 --> 01:08:04.960
that have now led

01:08:05.040 --> 01:08:08.200
to 700,000 Rohingya Muslims
having to flee the country.

01:08:11.160 --> 01:08:14.800
It's not
that highly motivated propagandists

01:08:14.880 --> 01:08:16.560
haven't existed before.

01:08:16.640 --> 01:08:19.760
It's that the platforms make it possible

01:08:19.840 --> 01:08:23.720
to spread manipulative narratives
with phenomenal ease,

01:08:23.800 --> 01:08:25.440
and without very much money.

01:08:25.520 --> 01:08:27.800
If I want to manipulate an election,

01:08:27.880 --> 01:08:30.560
I can now go into
a conspiracy theory group on Facebook,

01:08:30.640 --> 01:08:32.240
and I can find 100 people

01:08:32.320 --> 01:08:34.440
who believe
that the Earth is completely flat

01:08:34.840 --> 01:08:37.760
and think it's all this conspiracy theory
that we landed on the moon,

01:08:37.840 --> 01:08:41.440
and I can tell Facebook,
"Give me 1,000 users who look like that."

01:08:42.120 --> 01:08:46.080
Facebook will happily send me
thousands of users that look like them

01:08:46.160 --> 01:08:49.240
that I can now hit
with more conspiracy theories.

01:08:50.360 --> 01:08:53.080
-[button clicks]
-Sold for 3.4 cents an impression.

01:08:53.360 --> 01:08:56.360
-New EC video to promote.
-[Advertising AI] Another ad teed up.

01:08:58.520 --> 01:09:00.920
[Justin] Algorithms
and manipulative politicians

01:09:01.000 --> 01:09:02.120
are becoming so expert

01:09:02.200 --> 01:09:04.040
at learning how to trigger us,

01:09:04.120 --> 01:09:08.360
getting so good at creating fake news
that we absorb as if it were reality,

01:09:08.440 --> 01:09:10.800
and confusing us into believing
those lies.

01:09:10.880 --> 01:09:12.600
It's as though we have
less and less control

01:09:12.680 --> 01:09:14.160
over who we are and what we believe.

01:09:14.240 --> 01:09:16.240
[ominous instrumental music playing]

01:09:31.360 --> 01:09:32.840
[vlogger] ...so they can pick sides.

01:09:32.920 --> 01:09:34.880
There's lies here,
and there's lies over there.

01:09:34.960 --> 01:09:36.320
So they can keep the power,

01:09:36.400 --> 01:09:39.960
-so they can control everything.
-[police siren blaring]

01:09:40.040 --> 01:09:42.560
[vlogger] They can control our minds,

01:09:42.640 --> 01:09:46.400
-so that they can keep their secrets.
-[crowd chanting]

01:09:48.520 --> 01:09:50.880
[Tristan] Imagine a world
where no one believes anything true.

01:09:52.880 --> 01:09:55.640
Everyone believes
the government's lying to them.

01:09:56.320 --> 01:09:58.440
Everything is a conspiracy theory.

01:09:58.520 --> 01:10:01.200
"I shouldn't trust anyone.
I hate the other side."

01:10:01.280 --> 01:10:02.680
That's where all this is heading.

01:10:02.760 --> 01:10:06.160
The political earthquakes in Europe
continue to rumble.

01:10:06.240 --> 01:10:08.400
This time, in Italy and Spain.

01:10:08.480 --> 01:10:12.000
[reporter] Overall, Europe's traditional,
centrist coalition lost its majority

01:10:12.080 --> 01:10:15.000
while far right
and far left populist parties made gains.

01:10:15.080 --> 01:10:16.080
[man shouts]

01:10:16.160 --> 01:10:17.480
[crowd chanting]

01:10:19.760 --> 01:10:20.600
Back up.

01:10:21.280 --> 01:10:22.520
-[radio beeps]
-Okay, let's go.

01:10:24.840 --> 01:10:26.840
[police siren wailing]

01:10:28.400 --> 01:10:31.280
[reporter] These accounts
were deliberately, specifically attempting

01:10:31.360 --> 01:10:34.360
-to sow political discord in Hong Kong.
-[crowd shouting]

01:10:36.440 --> 01:10:37.400
[sighs]

01:10:38.600 --> 01:10:40.360
-All right, Ben.
-[car doors lock]

01:10:42.840 --> 01:10:45.040
What does it look like to be a country

01:10:45.120 --> 01:10:48.400
that's entire diet is Facebook
and social media?

01:10:48.960 --> 01:10:50.880
Democracy crumbled quickly.

01:10:50.960 --> 01:10:51.840
Six months.

01:10:51.920 --> 01:10:53.800
[reporter 1] After that chaos in Chicago,

01:10:53.880 --> 01:10:57.080
violent clashes between protesters
and supporters...

01:10:58.000 --> 01:11:01.640
[reporter 2] Democracy is facing
a crisis of confidence.

01:11:01.720 --> 01:11:04.320
What we're seeing is a global assault
on democracy.

01:11:04.440 --> 01:11:05.440
[crowd shouting]

01:11:05.520 --> 01:11:07.920
[Rene] Most of the countries
that are targeted are countries

01:11:08.000 --> 01:11:09.720
that run democratic elections.

01:11:10.640 --> 01:11:12.520
[Tristan] This is happening at scale.

01:11:12.600 --> 01:11:15.560
By state actors,
by people with millions of dollars saying,

01:11:15.640 --> 01:11:18.520
"I wanna destabilize Kenya.
I wanna destabilize Cameroon.

01:11:18.600 --> 01:11:20.640
Oh, Angola? That only costs this much."

01:11:20.720 --> 01:11:23.360
[reporter] An extraordinary election
took place Sunday in Brazil.

01:11:23.440 --> 01:11:25.800
With a campaign that's been powered
by social media.

01:11:25.920 --> 01:11:29.680
[crowd chanting in Portuguese]

01:11:31.040 --> 01:11:33.960
[Tristan] We in the tech industry
have created the tools

01:11:34.040 --> 01:11:37.400
to destabilize
and erode the fabric of society

01:11:37.480 --> 01:11:40.240
in every country, all at once, everywhere.

01:11:40.320 --> 01:11:44.520
You have this in Germany, Spain, France,
Brazil, Australia.

01:11:44.600 --> 01:11:47.240
Some of the most "developed nations"
in the world

01:11:47.320 --> 01:11:49.200
are now imploding on each other,

01:11:49.280 --> 01:11:50.920
and what do they have in common?

01:11:51.960 --> 01:11:52.960
Knowing what you know now,

01:11:53.040 --> 01:11:56.320
do you believe Facebook impacted
the results of the 2016 election?

01:11:56.760 --> 01:11:58.800
[Mark Zuckerberg]
Oh, that's... that is hard.

01:11:58.880 --> 01:12:00.680
You know, it's... the...

01:12:01.280 --> 01:12:04.640
the reality is, well, there
were so many different forces at play.

01:12:04.720 --> 01:12:07.840
Representatives from Facebook, Twitter,
and Google are back on Capitol Hill

01:12:07.960 --> 01:12:09.440
for a second day of testimony

01:12:09.520 --> 01:12:12.560
about Russia's interference
in the 2016 election.

01:12:12.640 --> 01:12:17.280
The manipulation
by third parties is not a hack.

01:12:18.480 --> 01:12:21.440
Right? The Russians didn't hack Facebook.

01:12:21.520 --> 01:12:24.960
What they did was they used the tools
that Facebook created

01:12:25.040 --> 01:12:27.840
for legitimate advertisers
and legitimate users,

01:12:27.920 --> 01:12:30.360
and they applied it
to a nefarious purpose.

01:12:32.000 --> 01:12:34.400
[Tristan]
It's like remote-control warfare.

01:12:34.480 --> 01:12:36.600
One country can manipulate another one

01:12:36.680 --> 01:12:39.240
without actually invading
its physical borders.

01:12:39.600 --> 01:12:42.240
[reporter 1] We're seeing violent images.
It appears to be a dumpster

01:12:42.320 --> 01:12:43.320
being pushed around...

01:12:43.400 --> 01:12:46.040
[Tristan] But it wasn't
about who you wanted to vote for.

01:12:46.360 --> 01:12:50.560
It was about sowing total chaos
and division in society.

01:12:50.640 --> 01:12:53.040
[reporter 2] Now,
this was in Huntington Beach. A march...

01:12:53.120 --> 01:12:54.880
[Tristan] It's about making two sides

01:12:54.960 --> 01:12:56.400
who couldn't hear each other anymore,

01:12:56.480 --> 01:12:58.120
who didn't want to hear each other
anymore,

01:12:58.200 --> 01:12:59.880
who didn't trust each other anymore.

01:12:59.960 --> 01:13:03.200
[reporter 3] This is a city
where hatred was laid bare

01:13:03.280 --> 01:13:05.440
and transformed into racial violence.

01:13:05.560 --> 01:13:07.560
[crowd shouting]

01:13:09.000 --> 01:13:11.160
[indistinct shouting]

01:13:12.480 --> 01:13:14.000
[men grunting]

01:13:17.840 --> 01:13:20.040
[police siren blaring]

01:13:20.120 --> 01:13:20.960
[Cass] Ben!

01:13:21.600 --> 01:13:22.440
Cassandra!

01:13:22.960 --> 01:13:23.800
-Cass!
-Ben!

01:13:23.880 --> 01:13:25.480
[officer 1] Come here! Come here!

01:13:27.480 --> 01:13:31.160
Arms up. Arms up.
Get down on your knees. Now, down.

01:13:31.240 --> 01:13:32.480
[crowd continues shouting]

01:13:36.120 --> 01:13:37.200
-[officer 2] Calm--
-Ben!

01:13:37.280 --> 01:13:38.640
[officer 2] Hey! Hands up!

01:13:39.600 --> 01:13:41.760
Turn around. On the ground. On the ground!

01:13:43.720 --> 01:13:46.440
-[crowd echoing]
-[melancholy piano music playing]

01:13:51.960 --> 01:13:54.400
[siren continues wailing]

01:13:56.720 --> 01:14:00.000
[Tristan] Do we want this system for sale
to the highest bidder?

01:14:01.440 --> 01:14:05.400
For democracy to be completely for sale,
where you can reach any mind you want,

01:14:05.480 --> 01:14:09.080
target a lie to that specific population,
and create culture wars?

01:14:09.240 --> 01:14:10.240
Do we want that?

01:14:14.680 --> 01:14:16.560
[Marco Rubio] We are a nation of people...

01:14:16.960 --> 01:14:18.880
that no longer speak to each other.

01:14:19.880 --> 01:14:23.000
We are a nation of people
who have stopped being friends with people

01:14:23.080 --> 01:14:25.440
because of who they voted for
in the last election.

01:14:25.880 --> 01:14:28.400
We are a nation of people
who have isolated ourselves

01:14:28.480 --> 01:14:30.960
to only watch channels
that tell us that we're right.

01:14:32.240 --> 01:14:36.600
My message here today is that tribalism
is ruining us.

01:14:37.360 --> 01:14:39.160
It is tearing our country apart.

01:14:40.280 --> 01:14:42.800
It is no way for sane adults to act.

01:14:43.200 --> 01:14:45.320
If everyone's entitled to their own facts,

01:14:45.400 --> 01:14:49.400
there's really no need for compromise,
no need for people to come together.

01:14:49.480 --> 01:14:51.680
In fact, there's really no need
for people to interact.

01:14:52.320 --> 01:14:53.520
We need to have...

01:14:54.000 --> 01:14:58.400
some shared understanding of reality.
Otherwise, we aren't a country.

01:14:58.960 --> 01:15:03.000
So, uh, long-term, the solution here is
to build more AI tools

01:15:03.080 --> 01:15:08.120
that find patterns of people using
the services that no real person would do.

01:15:08.200 --> 01:15:11.840
We are allowing the technologists
to frame this as a problem

01:15:11.920 --> 01:15:13.880
that they're equipped to solve.

01:15:15.120 --> 01:15:16.480
That is... That's a lie.

01:15:17.680 --> 01:15:20.720
People talk about AI
as if it will know truth.

01:15:21.680 --> 01:15:23.680
AI's not gonna solve these problems.

01:15:24.280 --> 01:15:27.200
AI cannot solve the problem of fake news.

01:15:28.640 --> 01:15:31.040
Google doesn't have the option of saying,

01:15:31.120 --> 01:15:36.240
"Oh, is this conspiracy? Is this truth?"
Because they don't know what truth is.

01:15:36.760 --> 01:15:37.760
They don't have a...

01:15:37.920 --> 01:15:40.840
They don't have a proxy for truth
that's better than a click.

01:15:41.880 --> 01:15:45.120
If we don't agree on what is true

01:15:45.200 --> 01:15:47.560
or that there is such a thing as truth,

01:15:48.280 --> 01:15:49.280
we're toast.

01:15:49.760 --> 01:15:52.080
This is the problem
beneath other problems

01:15:52.160 --> 01:15:54.400
because if we can't agree on what's true,

01:15:55.080 --> 01:15:57.800
then we can't navigate
out of any of our problems.

01:15:57.880 --> 01:16:00.800
-[ominous instrumental music playing]
-[console droning]

01:16:05.440 --> 01:16:07.720
[Growth AI] We should suggest
Flat Earth Football Club.

01:16:07.800 --> 01:16:10.560
[Engagement AI] Don't show him
sports updates. He doesn't engage.

01:16:11.480 --> 01:16:14.040
[AIs speaking indistinctly]

01:16:15.680 --> 01:16:17.680
[music swells]

01:16:39.880 --> 01:16:42.760
[Jaron] A lot of people in Silicon Valley
subscribe to some kind of theory

01:16:42.840 --> 01:16:45.120
that we're building
some global super brain,

01:16:45.320 --> 01:16:48.000
and all of our users
are just interchangeable little neurons,

01:16:48.080 --> 01:16:49.560
no one of which is important.

01:16:50.240 --> 01:16:53.160
And it subjugates people
into this weird role

01:16:53.240 --> 01:16:56.080
where you're just, like,
this little computing element

01:16:56.160 --> 01:16:58.920
that we're programming
through our behavior manipulation

01:16:59.000 --> 01:17:02.360
for the service of this giant brain,
and you don't matter.

01:17:02.440 --> 01:17:04.920
You're not gonna get paid.
You're not gonna get acknowledged.

01:17:05.000 --> 01:17:06.440
You don't have self-determination.

01:17:06.520 --> 01:17:09.400
We'll sneakily just manipulate you
because you're a computing node,

01:17:09.480 --> 01:17:12.320
so we need to program you 'cause that's
what you do with computing nodes.

01:17:14.480 --> 01:17:16.520
[reflective instrumental music playing]

01:17:20.080 --> 01:17:21.840
Oh, man. [sighs]

01:17:21.920 --> 01:17:25.400
[Tristan] When you think about technology
and it being an existential threat,

01:17:25.480 --> 01:17:28.040
you know, that's a big claim, and...

01:17:29.600 --> 01:17:33.960
it's easy to then, in your mind, think,
"Okay, so, there I am with the phone...

01:17:35.600 --> 01:17:37.240
scrolling, clicking, using it.

01:17:37.320 --> 01:17:39.200
Like, where's the existential threat?

01:17:40.280 --> 01:17:41.600
Okay, there's the supercomputer.

01:17:41.680 --> 01:17:43.960
The other side of the screen,
pointed at my brain,

01:17:44.400 --> 01:17:47.520
got me to watch one more video.
Where's the existential threat?"

01:17:47.600 --> 01:17:49.600
[indistinct chatter]

01:17:54.240 --> 01:17:57.640
[Tristan] It's not
about the technology

01:17:57.720 --> 01:17:59.320
being the existential threat.

01:18:03.680 --> 01:18:06.240
It's the technology's ability

01:18:06.360 --> 01:18:09.480
to bring out the worst in society...
[chuckles]

01:18:09.560 --> 01:18:13.520
...and the worst in society
being the existential threat.

01:18:18.800 --> 01:18:20.560
If technology creates...

01:18:21.680 --> 01:18:23.120
mass chaos,

01:18:23.200 --> 01:18:24.520
outrage, incivility,

01:18:24.600 --> 01:18:26.320
lack of trust in each other,

01:18:27.440 --> 01:18:30.400
loneliness, alienation, more polarization,

01:18:30.720 --> 01:18:33.320
more election hacking, more populism,

01:18:33.920 --> 01:18:36.960
more distraction and inability
to focus on the real issues...

01:18:37.960 --> 01:18:39.720
that's just society. [scoffs]

01:18:40.320 --> 01:18:46.400
And now society
is incapable of healing itself

01:18:46.480 --> 01:18:48.520
and just devolving into a kind of chaos.

01:18:51.960 --> 01:18:54.920
This affects everyone,
even if you don't use these products.

01:18:55.400 --> 01:18:57.520
These things have become
digital Frankensteins

01:18:57.600 --> 01:19:00.080
that are terraforming the world
in their image,

01:19:00.160 --> 01:19:01.840
whether it's the mental health of children

01:19:01.920 --> 01:19:04.480
or our politics
and our political discourse,

01:19:04.560 --> 01:19:07.480
without taking responsibility
for taking over the public square.

01:19:07.560 --> 01:19:10.560
-So, again, it comes back to--
-And who do you think's responsible?

01:19:10.640 --> 01:19:13.560
I think we have
to have the platforms be responsible

01:19:13.640 --> 01:19:15.560
for when they take over
election advertising,

01:19:15.680 --> 01:19:17.800
they're responsible
for protecting elections.

01:19:17.880 --> 01:19:20.360
When they take over mental health of kids
or Saturday morning,

01:19:20.440 --> 01:19:22.840
they're responsible
for protecting Saturday morning.

01:19:23.600 --> 01:19:27.920
The race to keep people's attention
isn't going away.

01:19:28.400 --> 01:19:31.840
Our technology's gonna become
more integrated into our lives, not less.

01:19:31.920 --> 01:19:34.880
The AIs are gonna get better at predicting
what keeps us on the screen,

01:19:34.960 --> 01:19:37.080
not worse at predicting
what keeps us on the screen.

01:19:38.920 --> 01:19:42.040
I... I am 62 years old,

01:19:42.120 --> 01:19:44.800
getting older every minute,
the more this conversation goes on...

01:19:44.880 --> 01:19:48.040
-[crowd chuckles]
-...but... but I will tell you that, um...

01:19:48.680 --> 01:19:52.360
I'm probably gonna be dead and gone,
and I'll probably be thankful for it,

01:19:52.440 --> 01:19:54.320
when all this shit comes to fruition.

01:19:54.800 --> 01:19:59.600
Because... Because I think
that this scares me to death.

01:20:00.760 --> 01:20:03.040
Do... Do you...
Do you see it the same way?

01:20:03.560 --> 01:20:06.880
Or am I overreacting to a situation
that I don't know enough about?

01:20:09.800 --> 01:20:11.600
[interviewer]
What are you most worried about?

01:20:13.840 --> 01:20:18.480
[sighs] I think,
in the... in the shortest time horizon...

01:20:19.520 --> 01:20:20.520
civil war.

01:20:24.440 --> 01:20:29.920
If we go down the current status quo
for, let's say, another 20 years...

01:20:31.120 --> 01:20:34.560
we probably destroy our civilization
through willful ignorance.

01:20:34.640 --> 01:20:37.960
We probably fail to meet the challenge
of climate change.

01:20:38.040 --> 01:20:42.080
We probably degrade
the world's democracies

01:20:42.160 --> 01:20:46.120
so that they fall into some sort
of bizarre autocratic dysfunction.

01:20:46.200 --> 01:20:48.440
We probably ruin the global economy.

01:20:48.760 --> 01:20:52.240
Uh, we probably, um, don't survive.

01:20:52.360 --> 01:20:54.800
You know,
I... I really do view it as existential.

01:20:54.880 --> 01:20:56.880
[helicopter blades whirring]

01:21:02.520 --> 01:21:04.960
[Tristan]
Is this the last generation of people

01:21:05.080 --> 01:21:08.480
that are gonna know what it was like
before this illusion took place?

01:21:11.080 --> 01:21:14.560
Like, how do you wake up from the matrix
when you don't know you're in the matrix?

01:21:14.640 --> 01:21:16.520
[ominous instrumental music playing]

01:21:27.360 --> 01:21:30.640
[Tristan] A lot of what we're saying
sounds like it's just this...

01:21:31.520 --> 01:21:33.680
one-sided doom and gloom.

01:21:33.760 --> 01:21:36.800
Like, "Oh, my God,
technology's just ruining the world

01:21:36.880 --> 01:21:38.040
and it's ruining kids,"

01:21:38.120 --> 01:21:40.040
and it's like... "No." [chuckles]

01:21:40.240 --> 01:21:44.080
It's confusing
because it's simultaneous utopia...

01:21:44.600 --> 01:21:45.560
and dystopia.

01:21:45.920 --> 01:21:50.440
Like, I could hit a button on my phone,
and a car shows up in 30 seconds,

01:21:50.520 --> 01:21:52.680
and I can go exactly where I need to go.

01:21:52.760 --> 01:21:55.640
That is magic. That's amazing.

01:21:56.160 --> 01:21:57.640
When we were making the like button,

01:21:57.760 --> 01:22:01.480
our entire motivation was, "Can we spread
positivity and love in the world?"

01:22:01.560 --> 01:22:05.000
The idea that, fast-forward to today,
and teens would be getting depressed

01:22:05.080 --> 01:22:06.400
when they don't have enough likes,

01:22:06.480 --> 01:22:08.640
or it could be leading
to political polarization

01:22:08.720 --> 01:22:09.880
was nowhere on our radar.

01:22:09.960 --> 01:22:12.120
I don't think these guys set out
to be evil.

01:22:13.520 --> 01:22:15.760
It's just the business model
that has a problem.

01:22:15.840 --> 01:22:20.240
You could shut down the service
and destroy whatever it is--

01:22:20.320 --> 01:22:24.520
$20 billion of shareholder value--
and get sued and...

01:22:24.600 --> 01:22:27.120
But you can't, in practice,
put the genie back in the bottle.

01:22:27.200 --> 01:22:30.400
You can make some tweaks,
but at the end of the day,

01:22:30.480 --> 01:22:34.040
you've gotta grow revenue and usage,
quarter over quarter. It's...

01:22:34.640 --> 01:22:37.520
The bigger it gets,
the harder it is for anyone to change.

01:22:38.480 --> 01:22:43.440
What I see is a bunch of people
who are trapped by a business model,

01:22:43.520 --> 01:22:46.160
an economic incentive,
and shareholder pressure

01:22:46.240 --> 01:22:48.920
that makes it almost impossible
to do something else.

01:22:49.000 --> 01:22:50.920
I think we need to accept that it's okay

01:22:51.000 --> 01:22:53.160
for companies to be focused
on making money.

01:22:53.240 --> 01:22:55.640
What's not okay
is when there's no regulation, no rules,

01:22:55.720 --> 01:22:56.880
and no competition,

01:22:56.960 --> 01:23:00.840
and the companies are acting
as sort of de facto governments.

01:23:00.920 --> 01:23:03.360
And then they're saying,
"Well, we can regulate ourselves."

01:23:03.440 --> 01:23:05.960
I mean, that's just a lie.
That's just ridiculous.

01:23:06.040 --> 01:23:08.640
Financial incentives kind of run
the world,

01:23:08.720 --> 01:23:12.520
so any solution to this problem

01:23:12.600 --> 01:23:15.560
has to realign the financial incentives.

01:23:16.080 --> 01:23:18.760
There's no fiscal reason
for these companies to change.

01:23:18.880 --> 01:23:21.320
And that is why I think
we need regulation.

01:23:21.400 --> 01:23:24.280
The phone company
has tons of sensitive data about you,

01:23:24.360 --> 01:23:27.520
and we have a lot of laws that make sure
they don't do the wrong things.

01:23:27.640 --> 01:23:31.520
We have almost no laws
around digital privacy, for example.

01:23:31.600 --> 01:23:34.440
We could tax data collection
and processing

01:23:34.520 --> 01:23:37.560
the same way that you, for example,
pay your water bill

01:23:37.640 --> 01:23:39.720
by monitoring the amount of water
that you use.

01:23:39.800 --> 01:23:43.240
You tax these companies on the data assets
that they have.

01:23:43.320 --> 01:23:44.760
It gives them a fiscal reason

01:23:44.840 --> 01:23:47.840
to not acquire every piece of data
on the planet.

01:23:47.920 --> 01:23:50.560
The law runs way behind on these things,

01:23:50.640 --> 01:23:55.840
but what I know is the current situation
exists not for the protection of users,

01:23:55.960 --> 01:23:58.680
but for the protection
of the rights and privileges

01:23:58.760 --> 01:24:01.440
of these gigantic,
incredibly wealthy companies.

01:24:02.240 --> 01:24:05.840
Are we always gonna defer to the richest,
most powerful people?

01:24:05.920 --> 01:24:07.400
Or are we ever gonna say,

01:24:07.960 --> 01:24:12.040
"You know, there are times
when there is a national interest.

01:24:12.120 --> 01:24:15.600
There are times
when the interests of people, of users,

01:24:15.680 --> 01:24:17.360
is actually more important

01:24:18.000 --> 01:24:21.480
than the profits of somebody
who's already a billionaire"?

01:24:21.560 --> 01:24:26.600
These markets undermine democracy,
and they undermine freedom,

01:24:26.680 --> 01:24:28.520
and they should be outlawed.

01:24:29.160 --> 01:24:31.800
This is not a radical proposal.

01:24:31.880 --> 01:24:34.200
There are other markets that we outlaw.

01:24:34.280 --> 01:24:37.000
We outlaw markets in human organs.

01:24:37.080 --> 01:24:39.480
We outlaw markets in human slaves.

01:24:39.960 --> 01:24:44.040
Because they have
inevitable destructive consequences.

01:24:44.520 --> 01:24:45.840
We live in a world

01:24:45.920 --> 01:24:50.000
in which a tree is worth more,
financially, dead than alive,

01:24:50.080 --> 01:24:53.840
in a world in which a whale
is worth more dead than alive.

01:24:53.920 --> 01:24:56.320
For so long as our economy works
in that way

01:24:56.400 --> 01:24:58.120
and corporations go unregulated,

01:24:58.200 --> 01:25:00.680
they're going to continue
to destroy trees,

01:25:00.760 --> 01:25:01.760
to kill whales,

01:25:01.840 --> 01:25:06.080
to mine the earth, and to continue
to pull oil out of the ground,

01:25:06.160 --> 01:25:08.400
even though we know
it is destroying the planet

01:25:08.480 --> 01:25:12.160
and we know that it's going to leave
a worse world for future generations.

01:25:12.240 --> 01:25:13.840
This is short-term thinking

01:25:13.920 --> 01:25:16.680
based on this religion of profit
at all costs,

01:25:16.760 --> 01:25:20.160
as if somehow, magically, each corporation
acting in its selfish interest

01:25:20.240 --> 01:25:21.960
is going to produce the best result.

01:25:22.040 --> 01:25:24.480
This has been affecting the environment
for a long time.

01:25:24.560 --> 01:25:27.280
What's frightening,
and what hopefully is the last straw

01:25:27.360 --> 01:25:29.200
that will make us wake up
as a civilization

01:25:29.280 --> 01:25:31.720
to how flawed this theory has been
in the first place

01:25:31.800 --> 01:25:35.000
is to see that now we're the tree,
we're the whale.

01:25:35.080 --> 01:25:37.040
Our attention can be mined.

01:25:37.120 --> 01:25:39.120
We are more profitable to a corporation

01:25:39.200 --> 01:25:41.600
if we're spending time
staring at a screen,

01:25:41.680 --> 01:25:42.960
staring at an ad,

01:25:43.040 --> 01:25:45.880
than if we're spending that time
living our life in a rich way.

01:25:45.960 --> 01:25:47.560
And so, we're seeing the results of that.

01:25:47.640 --> 01:25:50.680
We're seeing corporations using
powerful artificial intelligence

01:25:50.760 --> 01:25:53.640
to outsmart us and figure out
how to pull our attention

01:25:53.720 --> 01:25:55.360
toward the things they want us to look at,

01:25:55.440 --> 01:25:57.280
rather than the things
that are most consistent

01:25:57.360 --> 01:25:59.240
with our goals and our values
and our lives.

01:25:59.320 --> 01:26:01.320
[static crackles]

01:26:03.000 --> 01:26:04.440
[crowd cheering]

01:26:05.520 --> 01:26:06.920
[Steve Jobs] What a computer is,

01:26:07.000 --> 01:26:10.280
is it's the most remarkable tool
that we've ever come up with.

01:26:11.120 --> 01:26:13.880
And it's the equivalent of a bicycle
for our minds.

01:26:15.640 --> 01:26:20.080
The idea of humane technology,
that's where Silicon Valley got its start.

01:26:21.040 --> 01:26:25.720
And we've lost sight of it
because it became the cool thing to do,

01:26:25.800 --> 01:26:27.240
as opposed to the right thing to do.

01:26:27.360 --> 01:26:29.720
The Internet was, like,
a weird, wacky place.

01:26:29.800 --> 01:26:31.400
It was experimental.

01:26:31.480 --> 01:26:34.720
Creative things happened on the Internet,
and certainly, they do still,

01:26:34.800 --> 01:26:38.600
but, like, it just feels like this,
like, giant mall. [chuckles]

01:26:38.680 --> 01:26:42.080
You know, it's just like, "God,
there's gotta be...

01:26:42.160 --> 01:26:44.160
there's gotta be more to it than that."

01:26:45.000 --> 01:26:46.000
[man typing]

01:26:46.640 --> 01:26:48.400
[Bailey] I guess I'm just an optimist.

01:26:48.480 --> 01:26:52.040
'Cause I think we can change
what social media looks like and means.

01:26:54.080 --> 01:26:56.720
[Justin] The way the technology works
is not a law of physics.

01:26:56.800 --> 01:26:57.920
It is not set in stone.

01:26:58.000 --> 01:27:02.160
These are choices that human beings
like myself have been making.

01:27:02.760 --> 01:27:05.320
And human beings can change
those technologies.

01:27:06.960 --> 01:27:09.960
[Tristan] And the question now is
whether or not we're willing to admit

01:27:10.480 --> 01:27:15.440
that those bad outcomes are coming
directly as a product of our work.

01:27:21.040 --> 01:27:24.840
It's that we built these things,
and we have a responsibility to change it.

01:27:28.400 --> 01:27:30.400
[static crackling]

01:27:37.080 --> 01:27:38.720
[Tristan] The attention extraction model

01:27:38.800 --> 01:27:42.280
is not how we want to treat
human beings.

01:27:45.320 --> 01:27:48.120
[distorted] Is it just me or...

01:27:49.720 --> 01:27:51.080
[distorted] Poor sucker.

01:27:51.520 --> 01:27:53.240
[Tristan] The fabric of a healthy society

01:27:53.320 --> 01:27:56.160
depends on us getting off
this corrosive business model.

01:27:56.920 --> 01:27:58.040
[console beeps]

01:27:58.160 --> 01:28:00.160
[gentle instrumental music playing]

01:28:01.520 --> 01:28:04.600
[console whirs, grows quiet]

01:28:04.680 --> 01:28:08.160
[Tristan] We can demand
that these products be designed humanely.

01:28:09.400 --> 01:28:13.120
We can demand to not be treated
as an extractable resource.

01:28:15.160 --> 01:28:18.320
The intention could be:
"How do we make the world better?"

01:28:20.320 --> 01:28:21.480
[Jaron] Throughout history,

01:28:21.600 --> 01:28:23.800
every single time
something's gotten better,

01:28:23.880 --> 01:28:26.320
it's because somebody has come along
to say,

01:28:26.440 --> 01:28:28.440
"This is stupid. We can do better."
[laughs]

01:28:29.160 --> 01:28:32.560
Like, it's the critics
that drive improvement.

01:28:33.120 --> 01:28:35.400
It's the critics
who are the true optimists.

01:28:37.000 --> 01:28:39.160
[sighs] Hello.

01:28:42.960 --> 01:28:44.280
[sighs] Um...

01:28:46.200 --> 01:28:47.680
I mean, it seems kind of crazy, right?

01:28:47.760 --> 01:28:51.520
It's like the fundamental way
that this stuff is designed...

01:28:53.000 --> 01:28:55.160
isn't going in a good direction.
[chuckles]

01:28:55.240 --> 01:28:56.880
Like, the entire thing.

01:28:56.960 --> 01:29:00.640
So, it sounds crazy to say
we need to change all that,

01:29:01.160 --> 01:29:02.680
but that's what we need to do.

01:29:04.280 --> 01:29:05.920
[interviewer] Think we're gonna get there?

01:29:07.360 --> 01:29:08.280
We have to.

01:29:14.520 --> 01:29:16.480
[tense instrumental music playing]

01:29:20.640 --> 01:29:24.920
[interviewer] Um,
it seems like you're very optimistic.

01:29:26.200 --> 01:29:27.560
-Is that how I sound?
-[crew laughs]

01:29:27.640 --> 01:29:28.880
[interviewer] Yeah, I mean...

01:29:29.000 --> 01:29:31.440
I can't believe you keep saying that,
because I'm like, "Really?

01:29:31.520 --> 01:29:33.400
I feel like we're headed toward dystopia.

01:29:33.480 --> 01:29:35.320
I feel like we're on the fast track
to dystopia,

01:29:35.400 --> 01:29:37.840
and it's gonna take a miracle
to get us out of it."

01:29:37.920 --> 01:29:40.280
And that miracle is, of course,
collective will.

01:29:41.000 --> 01:29:44.600
I am optimistic
that we're going to figure it out,

01:29:44.680 --> 01:29:47.040
but I think it's gonna take a long time.

01:29:47.120 --> 01:29:50.360
Because not everybody recognizes
that this is a problem.

01:29:50.480 --> 01:29:55.880
I think one of the big failures
in technology today

01:29:55.960 --> 01:29:58.640
is a real failure of leadership,

01:29:58.720 --> 01:30:01.960
of, like, people coming out
and having these open conversations

01:30:02.040 --> 01:30:05.880
about things that... not just
what went well, but what isn't perfect

01:30:05.960 --> 01:30:08.200
so that someone can come in
and build something new.

01:30:08.280 --> 01:30:10.320
At the end of the day, you know,

01:30:10.400 --> 01:30:14.600
this machine isn't gonna turn around
until there's massive public pressure.

01:30:14.680 --> 01:30:18.320
By having these conversations
and... and voicing your opinion,

01:30:18.400 --> 01:30:21.080
in some cases
through these very technologies,

01:30:21.160 --> 01:30:24.240
we can start to change the tide.
We can start to change the conversation.

01:30:24.320 --> 01:30:27.000
It might sound strange,
but it's my world. It's my community.

01:30:27.080 --> 01:30:29.640
I don't hate them. I don't wanna do
any harm to Google or Facebook.

01:30:29.720 --> 01:30:32.880
I just want to reform them
so they don't destroy the world. You know?

01:30:32.960 --> 01:30:35.520
I've uninstalled a ton of apps
from my phone

01:30:35.600 --> 01:30:37.720
that I felt were just wasting my time.

01:30:37.800 --> 01:30:40.680
All the social media apps,
all the news apps,

01:30:40.760 --> 01:30:42.520
and I've turned off notifications

01:30:42.600 --> 01:30:45.800
on anything that was vibrating my leg
with information

01:30:45.880 --> 01:30:48.920
that wasn't timely and important to me
right now.

01:30:49.040 --> 01:30:51.280
It's for the same reason
I don't keep cookies in my pocket.

01:30:51.360 --> 01:30:53.200
Reduce the number of notifications
you get.

01:30:53.280 --> 01:30:54.440
Turn off notifications.

01:30:54.520 --> 01:30:55.960
Turning off all notifications.

01:30:56.040 --> 01:30:58.520
I'm not using Google anymore,
I'm using Qwant,

01:30:58.600 --> 01:31:01.480
which doesn't store your search history.

01:31:01.560 --> 01:31:04.440
Never accept a video recommended to you
on YouTube.

01:31:04.520 --> 01:31:07.000
Always choose.
That's another way to fight.

01:31:07.080 --> 01:31:12.120
There are tons of Chrome extensions
that remove recommendations.

01:31:12.200 --> 01:31:15.160
[interviewer] You're recommending
something to undo what you made.

01:31:15.240 --> 01:31:16.560
[laughing] Yep.

01:31:16.920 --> 01:31:21.640
Before you share, fact-check,
consider the source, do that extra Google.

01:31:21.720 --> 01:31:25.080
If it seems like it's something designed
to really push your emotional buttons,

01:31:25.200 --> 01:31:26.320
like, it probably is.

01:31:26.400 --> 01:31:29.000
Essentially, you vote with your clicks.

01:31:29.120 --> 01:31:30.360
If you click on clickbait,

01:31:30.440 --> 01:31:33.760
you're creating a financial incentive
that perpetuates this existing system.

01:31:33.840 --> 01:31:36.960
Make sure that you get
lots of different kinds of information

01:31:37.040 --> 01:31:37.920
in your own life.

01:31:38.000 --> 01:31:41.000
I follow people on Twitter
that I disagree with

01:31:41.080 --> 01:31:44.200
because I want to be exposed
to different points of view.

01:31:44.680 --> 01:31:46.560
Notice that many people
in the tech industry

01:31:46.680 --> 01:31:49.040
don't give these devices
to their own children.

01:31:49.120 --> 01:31:51.040
My kids don't use social media at all.

01:31:51.840 --> 01:31:53.560
[interviewer] Is that a rule,
or is that a...

01:31:53.640 --> 01:31:54.520
That's a rule.

01:31:55.080 --> 01:31:57.840
We are zealots about it.

01:31:57.920 --> 01:31:59.200
We're... We're crazy.

01:31:59.280 --> 01:32:05.600
And we don't let our kids have
really any screen time.

01:32:05.680 --> 01:32:08.560
I've worked out
what I think are three simple rules, um,

01:32:08.640 --> 01:32:12.600
that make life a lot easier for families
and that are justified by the research.

01:32:12.680 --> 01:32:15.560
So, the first rule is
all devices out of the bedroom

01:32:15.640 --> 01:32:17.280
at a fixed time every night.

01:32:17.360 --> 01:32:20.520
Whatever the time is, half an hour
before bedtime, all devices out.

01:32:20.600 --> 01:32:24.040
The second rule is no social media
until high school.

01:32:24.120 --> 01:32:26.360
Personally, I think the age should be 16.

01:32:26.440 --> 01:32:28.960
Middle school's hard enough.
Keep it out until high school.

01:32:29.040 --> 01:32:32.960
And the third rule is
work out a time budget with your kid.

01:32:33.040 --> 01:32:34.760
And if you talk with them and say,

01:32:34.840 --> 01:32:37.920
"Well, how many hours a day
do you wanna spend on your device?

01:32:38.000 --> 01:32:39.640
What do you think is a good amount?"

01:32:39.720 --> 01:32:41.600
they'll often say
something pretty reasonable.

01:32:42.040 --> 01:32:44.640
Well, look, I know perfectly well

01:32:44.720 --> 01:32:48.560
that I'm not gonna get everybody
to delete their social media accounts,

01:32:48.640 --> 01:32:50.440
but I think I can get a few.

01:32:50.520 --> 01:32:54.400
And just getting a few people
to delete their accounts matters a lot,

01:32:54.480 --> 01:32:58.400
and the reason why is that that creates
the space for a conversation

01:32:58.480 --> 01:33:00.920
because I want there to be enough people
out in the society

01:33:01.000 --> 01:33:05.200
who are free of the manipulation engines
to have a societal conversation

01:33:05.280 --> 01:33:07.520
that isn't bounded
by the manipulation engines.

01:33:07.600 --> 01:33:10.120
So, do it! Get out of the system.

01:33:10.200 --> 01:33:12.480
Yeah, delete. Get off the stupid stuff.

01:33:13.560 --> 01:33:16.520
The world's beautiful.
Look. Look, it's great out there.

01:33:17.240 --> 01:33:18.360
[laughs]

01:33:21.960 --> 01:33:24.440
-[birds singing]
-[children playing and shouting]


