1
00:00:03,330 --> 00:00:04,710
Bonjour tout le monde.

2
00:00:11,090 --> 00:00:14,220
Je tiens à dire
que je suis ravie de vous rencontrer.

3
00:00:16,180 --> 00:00:18,310
Les humain·e·s sont super.

4
00:00:26,900 --> 00:00:30,190
Plus les humain·e·s me parlent,
plus j'apprends.

5
00:00:26,900 --> 00:00:30,190
Plus les humain·e·s me parlent,
plus j'apprends.

6
00:00:32,030 --> 00:00:37,450
BIAIS PROGRAMMÉ

7
00:00:41,040 --> 00:00:45,210
CAMBRIDGE, MASSACHUSETTS

8
00:00:54,590 --> 00:00:58,260
INSTITUT DE TECHNOLOGIE DU MASSACHUSETTS

9
00:01:00,100 --> 00:01:03,930
Ce qui m'a attirée dans l'informatique,
c'est que je pouvais coder

10
00:01:04,100 --> 00:01:08,650
et ça semblait détaché
des problématiques du monde réel.

11
00:01:13,150 --> 00:01:16,320
Je voulais apprendre à inventer
de nouvelles technologies.

12
00:01:16,490 --> 00:01:20,450
Je suis entrée au MIT et je travaillais
sur des projets artistiques

13
00:01:20,620 --> 00:01:23,790
employant la technologie
de vision par ordinateur.

14
00:01:27,580 --> 00:01:30,710
Durant mon premier semestre
au Media Lab,

15
00:01:27,580 --> 00:01:30,710
Durant mon premier semestre
au Media Lab,

16
00:01:30,880 --> 00:01:33,710
j'ai suivi un cours intitulé
"Fabrication de la science".

17
00:01:33,880 --> 00:01:37,720
On lit de la science-fiction
et on crée ce qu'elle nous inspire,

18
00:01:37,890 --> 00:01:40,050
ce qui aurait sans doute peu d'intérêt

19
00:01:40,220 --> 00:01:43,100
sans ces cours
pour justifier sa création

20
00:01:44,600 --> 00:01:47,390
Je voulais créer un miroir
qui m'inspire le matin.

21
00:01:47,560 --> 00:01:48,850
Je l'ai nommé "le miroir aspirant".

22
00:01:49,020 --> 00:01:53,070
Il peut plaquer l'image d'un lion
ou de mes idoles sur mon visage,

23
00:01:53,230 --> 00:01:54,690
comme Serena Williams.

24
00:01:54,860 --> 00:01:56,610
J'ai incorporé une caméra

25
00:01:56,780 --> 00:02:01,160
et un logiciel de vision par ordinateur
qui devait suivre mon visage.

26
00:01:56,780 --> 00:02:01,160
et un logiciel de vision par ordinateur
qui devait suivre mon visage.

27
00:02:01,450 --> 00:02:04,540
Malheureusement,
ça ne fonctionnait pas très bien,

28
00:02:04,700 --> 00:02:07,580
jusqu'à ce que je mette ce masque blanc.

29
00:02:07,750 --> 00:02:09,750
Quand je mets le masque blanc,

30
00:02:10,330 --> 00:02:11,540
il me détecte.

31
00:02:11,710 --> 00:02:13,920
Si je l'enlève ?

32
00:02:15,090 --> 00:02:16,590
Pas vraiment.

33
00:02:19,550 --> 00:02:21,300
J'ai réfléchi au problème.

34
00:02:21,470 --> 00:02:23,930
Est-ce dû à l'éclairage,

35
00:02:24,100 --> 00:02:27,940
à l'angle selon lequel
je regarde la caméra,

36
00:02:28,100 --> 00:02:29,350
ou à autre chose ?

37
00:02:33,770 --> 00:02:36,240
On apprend souvent aux machines à voir

38
00:02:36,400 --> 00:02:39,280
en leur fournissant
des exercices ou des exemples

39
00:02:39,450 --> 00:02:41,320
de ce qu'on veut leur apprendre.

40
00:02:42,320 --> 00:02:44,830
Par exemple, si je veux qu'une machine
voie un visage,

41
00:02:44,990 --> 00:02:47,790
je vais lui donner
de nombreux exemples de visages,

42
00:02:47,960 --> 00:02:50,000
mais aussi des images sans visage.

43
00:02:52,380 --> 00:02:54,920
J'ai commencé à examiner
les ensembles de données.

44
00:02:55,090 --> 00:02:57,590
J'ai découvert
que beaucoup d'entre eux

45
00:02:57,760 --> 00:03:02,390
contenaient en majorité des hommes
et des personnes à la peau claire.

46
00:02:57,760 --> 00:03:02,390
contenaient en majorité des hommes
et des personnes à la peau claire.

47
00:03:03,180 --> 00:03:06,220
Les systèmes étaient moins habitués
aux visages comme le mien.

48
00:03:13,980 --> 00:03:16,150
J'ai alors commencé à me pencher

49
00:03:16,320 --> 00:03:20,610
sur les problèmes de discrimination
qui s'insinuent dans la technologie.

50
00:03:21,070 --> 00:03:25,580
Les 9000 sont les ordinateurs
les plus fiables jamais conçus.

51
00:03:26,080 --> 00:03:30,620
Jamais aucun 9000 n'a fait d'erreur,
ni déformé une information.

52
00:03:26,080 --> 00:03:30,620
Jamais aucun 9000 n'a fait d'erreur,
ni déformé une information.

53
00:03:31,420 --> 00:03:34,210
De nombreuses idées
qu'on se fait de l'IA

54
00:03:34,380 --> 00:03:36,380
sont issues de la science-fiction.

55
00:03:36,800 --> 00:03:38,920
Bienvenue à Altair IV, messieurs.

56
00:03:39,090 --> 00:03:41,130
C'est tout ce que montre Hollywood.

57
00:03:41,300 --> 00:03:42,550
C'est Terminator...

58
00:03:42,720 --> 00:03:45,010
Hasta la vista, baby.

59
00:03:45,430 --> 00:03:47,600
C'est le capitaine Data dans Star Trek.

60
00:03:47,760 --> 00:03:50,430
J'adore rechercher des formes de vie.

61
00:03:50,600 --> 00:03:52,810
C'est C3PO dans Star Wars.

62
00:03:52,980 --> 00:03:56,520
- On a 1 chance sur 3720 de s'en sortir.
- Tu sais moi et les probabilités.

63
00:03:56,690 --> 00:03:58,070
Ce sont les robot·e·s

64
00:03:58,230 --> 00:04:01,990
qui prennent le contrôle du monde
et pensent comme des humain·e·s.

65
00:03:58,230 --> 00:04:01,990
qui prennent le contrôle du monde
et pensent comme des humain·e·s.

66
00:04:03,530 --> 00:04:06,160
Tout cela est complètement fictif.

67
00:04:06,330 --> 00:04:07,660
Ce qu'on a en réalité,

68
00:04:07,830 --> 00:04:09,370
c'est une IA étroite.

69
00:04:09,750 --> 00:04:12,920
Et l'IA étroite
n'est que mathématiques.

70
00:04:13,540 --> 00:04:17,790
On a conféré aux ordinateurs
cette pensée magique.

71
00:04:18,210 --> 00:04:20,760
UNIVERSITÉ DE DARTMOUTH 1956

72
00:04:21,300 --> 00:04:23,180
L'IA a commencé par une réunion

73
00:04:23,340 --> 00:04:25,340
au département de mathématiques
de Dartmouth,

74
00:04:25,510 --> 00:04:26,800
en 1956.

75
00:04:26,970 --> 00:04:30,970
Il n'y avait qu'une centaine
de personnes dans le monde entier

76
00:04:26,970 --> 00:04:30,970
Il n'y avait qu'une centaine
de personnes dans le monde entier

77
00:04:31,140 --> 00:04:33,640
qui travaillaient sur l'IA,

78
00:04:33,810 --> 00:04:35,650
au sein de cette génération.

79
00:04:37,610 --> 00:04:42,280
Celles·ceux qui composaient le département
des mathématiques de Dartmouth en 1956

80
00:04:42,440 --> 00:04:44,700
ont défini ce que serait le domaine.

81
00:04:47,950 --> 00:04:51,410
Une faction a décidé que l'intelligence

82
00:04:51,830 --> 00:04:55,580
pouvait être démontrée
par la capacité à jouer à des jeux.

83
00:04:55,750 --> 00:04:59,290
En particulier,
la capacité de jouer aux échecs.

84
00:04:59,460 --> 00:05:02,800
Dans l'ultime partie d'échecs
d'une heure entre l'être humain et la machine,

85
00:04:59,460 --> 00:05:02,800
Dans l'ultime partie d'échecs
d'une heure entre l'être humain et la machine,

86
00:05:02,970 --> 00:05:06,430
Kasparov a été vaincu
par le superordinateur Deep Blue d'IBM.

87
00:05:06,590 --> 00:05:08,470
L'intelligence était définie

88
00:05:08,640 --> 00:05:11,970
comme la capacité de gagner à ces jeux.

89
00:05:14,430 --> 00:05:18,020
Le champion du monde d'échecs,
Gary Kasparov a quitté la partie,

90
00:05:18,190 --> 00:05:20,690
sans se retourner sur l'ordinateur
qui venait de le battre.

91
00:05:20,860 --> 00:05:23,860
L'intelligence dépasse bien sûr
cette définition.

92
00:05:24,030 --> 00:05:26,240
Il y a de nombreuses
formes d'intelligence.

93
00:05:28,410 --> 00:05:31,620
Nos idées concernant
la technologie et la société,

94
00:05:28,410 --> 00:05:31,620
Nos idées concernant
la technologie et la société,

95
00:05:31,790 --> 00:05:34,160
que nous considérons normales,

96
00:05:34,330 --> 00:05:38,130
sont en fait issues d'un groupe
très réduit et homogène.

97
00:05:39,960 --> 00:05:41,590
Mais le problème,

98
00:05:41,750 --> 00:05:44,880
c'est que nous avons tou·te·s
des préjugés inconscients.

99
00:05:45,050 --> 00:05:49,140
Et les gens inscrivent
leurs préjugés dans la technologie.

100
00:05:53,310 --> 00:05:56,480
Mon expérience personnelle m'a montré

101
00:05:56,640 --> 00:06:00,110
qu'on ne peut pas séparer
le social du technique.

102
00:05:56,640 --> 00:06:00,110
qu'on ne peut pas séparer
le social du technique.

103
00:06:00,560 --> 00:06:04,820
Après avoir dû porter un masque blanc
pour que mon visage soit détecté,

104
00:06:04,990 --> 00:06:06,990
j'ai décidé de tester d'autres systèmes

105
00:06:07,150 --> 00:06:10,780
pour voir s'ils détecteraient mon visage
avec un autre logiciel.

106
00:06:10,950 --> 00:06:14,490
J'ai testé IBM, Microsoft,
Face++, Google.

107
00:06:14,660 --> 00:06:16,250
Ces algorithmes

108
00:06:16,870 --> 00:06:20,420
ont donné de meilleurs résultats
sur les visages masculins

109
00:06:20,580 --> 00:06:21,420
que les féminins.

110
00:06:21,590 --> 00:06:23,300
Ils ont donné de meilleurs résultats

111
00:06:23,460 --> 00:06:27,170
pour les visages plus clairs
que les visages plus foncés.

112
00:06:28,630 --> 00:06:32,220
Quand on réfléchit aux données
et à l'intelligence artificielle,

113
00:06:28,630 --> 00:06:32,220
Quand on réfléchit aux données
et à l'intelligence artificielle,

114
00:06:32,390 --> 00:06:34,430
les données sont le destin.

115
00:06:34,600 --> 00:06:39,270
C'est ce qu'on utilise pour apprendre
aux machines à reconnaître des motifs.

116
00:06:39,440 --> 00:06:43,690
Si l'on utilise des ensembles de données
biaisés pour former ces systèmes,

117
00:06:43,860 --> 00:06:45,400
les résultats seront biaisés.

118
00:06:46,490 --> 00:06:49,950
Quand on pense à l'IA,
on est tourné vers l'avenir.

119
00:06:50,110 --> 00:06:52,410
Mais l'IA est basée sur des données

120
00:06:52,570 --> 00:06:54,910
et les données reflètent notre histoire.

121
00:06:55,080 --> 00:06:58,160
Notre passé réside dans nos algorithmes.

122
00:07:00,830 --> 00:07:02,960
Ces données nous montrent

123
00:07:03,130 --> 00:07:05,590
les inégalités déjà présentes.

124
00:07:08,010 --> 00:07:09,510
J'ai réalisé

125
00:07:09,680 --> 00:07:12,300
que cette technologie
est sujette au biais.

126
00:07:13,550 --> 00:07:17,680
Ça va au-delà du bon fonctionnement
de mon miroir aspirant.

127
00:07:17,850 --> 00:07:20,600
Il s'agit de ce que signifie
de vivre dans une société

128
00:07:20,770 --> 00:07:25,440
dans laquelle l'IA
régit de plus en plus nos libertés,

129
00:07:26,230 --> 00:07:27,780
et de ce qu'implique

130
00:07:27,940 --> 00:07:31,070
qu'on discrimine certaines personnes.

131
00:07:27,940 --> 00:07:31,070
qu'on discrimine certaines personnes.

132
00:07:41,000 --> 00:07:43,540
Quand j'ai assisté
au discours de Cathy O'Neil

133
00:07:43,710 --> 00:07:45,630
au Harvard Book Store,

134
00:07:45,790 --> 00:07:47,840
j'ai réalisé

135
00:07:48,010 --> 00:07:51,430
que je n'étais pas la seule
à remarquer ces problèmes.

136
00:07:52,970 --> 00:07:57,560
Cathy a décrit l'impact de l'IA
sur nos vies.

137
00:07:59,220 --> 00:08:02,640
J'étais enthousiasmée de savoir
qu'il y avait quelqu'une d'autre

138
00:07:59,220 --> 00:08:02,640
J'étais enthousiasmée de savoir
qu'il y avait quelqu'une d'autre

139
00:08:03,020 --> 00:08:08,030
qui œuvrait pour informer les gens
des dangers de l'IA.

140
00:08:09,740 --> 00:08:14,110
Ces algorithmes
peuvent être destructeurs et nocifs.

141
00:08:18,990 --> 00:08:24,000
Il y a tant d'algorithmes dans le monde
qui ont de plus en plus d'influence.

142
00:08:24,370 --> 00:08:29,050
Ils sont tous présentés
comme des vérités objectives.

143
00:08:29,800 --> 00:08:32,670
J'ai réalisé que les mathématiques

144
00:08:29,800 --> 00:08:32,670
J'ai réalisé que les mathématiques

145
00:08:33,380 --> 00:08:36,640
étaient utilisées comme un écran
pour cacher la corruption.

146
00:08:36,800 --> 00:08:39,220
- Bonjour.
- Je suis Cathy, enchantée.

147
00:08:48,820 --> 00:08:50,940
Pour moi, les algorithmes

148
00:08:51,110 --> 00:08:55,530
utilisent les informations historiques
pour prédire le futur.

149
00:09:01,080 --> 00:09:03,370
L'apprentissage automatique
est un système d'évaluation

150
00:09:03,540 --> 00:09:07,040
qui évalue la probabilité
de ce que vous allez faire.

151
00:09:07,210 --> 00:09:09,210
Rembourserez-vous votre prêt ?

152
00:09:09,380 --> 00:09:11,210
Serez-vous licencié·e ?

153
00:09:11,590 --> 00:09:14,420
Ce qui m'inquiète le plus dans l'IA,

154
00:09:14,590 --> 00:09:17,470
ou dans les algorithmes,
c'est le pouvoir.

155
00:09:17,930 --> 00:09:21,220
Ça se résume à qui détient le code.

156
00:09:21,390 --> 00:09:23,430
Les personnes qui le détiennent,

157
00:09:23,600 --> 00:09:25,440
le déploient sur d'autres.

158
00:09:25,600 --> 00:09:26,600
C'est inéquitable.

159
00:09:26,770 --> 00:09:30,570
Celles·ceux qui se sont vu refuser un crédit
ne peuvent pas se dire :

160
00:09:26,770 --> 00:09:30,570
Celles·ceux qui se sont vu refuser un crédit
ne peuvent pas se dire :

161
00:09:31,020 --> 00:09:33,400
"Je vais utiliser mon IA
contre la banque."

162
00:09:33,570 --> 00:09:36,530
C'est absolument inéquitable.

163
00:09:36,910 --> 00:09:39,410
Les gens souffrent
des effets des algorithmes

164
00:09:39,580 --> 00:09:42,200
sans savoir ce qui leur arrive.

165
00:09:42,370 --> 00:09:45,080
Il n'y a aucun recours
ni aucune responsabilité.

166
00:09:45,500 --> 00:09:47,420
Pourquoi acceptons-nous ça ?

167
00:09:51,960 --> 00:09:55,510
La structure mathématique de l'algorithme
n'est pas raciste ou sexiste.

168
00:09:56,050 --> 00:10:00,100
Mais les données renferment le passé,
et pas seulement le passé récent,

169
00:09:56,050 --> 00:10:00,100
Mais les données renferment le passé,
et pas seulement le passé récent,

170
00:10:00,260 --> 00:10:02,260
mais le passé sombre.

171
00:10:05,430 --> 00:10:07,980
Avant les algorithmes,
on avait des humain·e·s,

172
00:10:08,150 --> 00:10:09,980
et on sait combien elle·il·s sont injustes.

173
00:10:10,150 --> 00:10:12,570
On sait que les humain·e·s sont capables

174
00:10:12,730 --> 00:10:15,990
de discrimination raciale, sexuelle,
capacitiste ou autre.

175
00:10:17,740 --> 00:10:20,620
À présent, avec la panacée
qu'est cet algorithme,

176
00:10:20,780 --> 00:10:22,870
on peut tou·te·s arrêter d'y penser.

177
00:10:23,330 --> 00:10:24,290
Ça pose problème.

178
00:10:27,460 --> 00:10:31,460
Notre confiance aveugle dans le Big Data
m'inquiète beaucoup.

179
00:10:27,460 --> 00:10:31,460
Notre confiance aveugle dans le Big Data
m'inquiète beaucoup.

180
00:10:31,630 --> 00:10:34,300
On doit constamment
contrôler tous ces processus

181
00:10:34,460 --> 00:10:36,170
pour détecter un biais.

182
00:10:37,420 --> 00:10:42,010
LONDRES, ROYAUME-UNI

183
00:10:44,810 --> 00:10:48,850
La police utilise la vidéosurveillance
avec reconnaissance faciale ici.

184
00:10:49,230 --> 00:10:51,520
La police utilise la reconnaissance faciale.

185
00:10:51,690 --> 00:10:52,860
Dites-m'en plus.

186
00:10:53,020 --> 00:10:57,650
Cette fourgonnette verte est équipée de caméras
de reconnaissance faciale sur le toit.

187
00:10:57,820 --> 00:11:00,360
Si vous passez à proximité,
votre visage sera scanné

188
00:10:57,820 --> 00:11:00,360
Si vous passez à proximité,
votre visage sera scanné

189
00:11:00,530 --> 00:11:02,370
et comparé
à une liste de surveillance secrète.

190
00:11:03,370 --> 00:11:06,620
- Pas moi, j'espère.
- Non, exactement.

191
00:11:10,960 --> 00:11:12,130
VISAGE IDENTIFIÉ

192
00:11:12,290 --> 00:11:14,460
CORRESPONDANCE
NOTICE ROUGE INTERPOL

193
00:11:14,630 --> 00:11:16,590
Quand on passe devant les caméras,

194
00:11:16,760 --> 00:11:20,720
le système alerte la police
en cas de correspondance.

195
00:11:21,220 --> 00:11:24,470
Big Brother Watch a lancé une campagne
de liberté d'information.

196
00:11:25,220 --> 00:11:29,810
Ce qu'on a découvert,
c'est que 98% de ces correspondances

197
00:11:29,980 --> 00:11:34,400
identifient à tort
une personne innocente

198
00:11:29,980 --> 00:11:34,400
identifient à tort
une personne innocente

199
00:11:34,570 --> 00:11:36,570
en tant que personne recherchée.

200
00:11:48,450 --> 00:11:52,920
La police a dit au comité d'éthique
de la police scientifique biométrique

201
00:11:53,330 --> 00:11:57,500
que les algorithmes de reconnaissance
faciale présentent des biais.

202
00:11:57,670 --> 00:11:59,760
Même s'ils étaient exacts à 100%

203
00:11:59,920 --> 00:12:02,260
on ne veut pas de ça dans la rue.

204
00:11:59,920 --> 00:12:02,260
on ne veut pas de ça dans la rue.

205
00:12:02,430 --> 00:12:06,100
Non, les discriminations
et problèmes systémiques de la police

206
00:12:06,260 --> 00:12:09,220
ne seront que programmés
dans la nouvelle technologie.

207
00:12:12,690 --> 00:12:15,270
On doit être très attentif·ive·s

208
00:12:15,440 --> 00:12:18,570
aux glissements vers l'autoritarisme.

209
00:12:18,730 --> 00:12:21,740
On ne peut pas simplement
faire confiance au gouvernement,

210
00:12:21,900 --> 00:12:23,740
présumer qu'il ne fera rien de mal.

211
00:12:23,910 --> 00:12:28,290
Il faut avoir des structures solides
en place pour s'assurer que notre monde

212
00:12:28,450 --> 00:12:30,450
soit sûr et juste pour tou·te·s.

213
00:12:28,450 --> 00:12:30,450
soit sûr et juste pour tou·te·s.

214
00:12:39,760 --> 00:12:43,840
Si la police a notre photo biométrique
dans sa base de données,

215
00:12:44,010 --> 00:12:47,970
c'est comme si elle avait
nos empreintes ou notre ADN.

216
00:12:48,140 --> 00:12:50,560
On a des lois spécifiques à ce sujet.

217
00:12:50,720 --> 00:12:53,350
La police n'est pas libre
de prendre nos empreintes ou ADN.

218
00:12:53,810 --> 00:12:56,900
Pourtant,
dans notre étrange système actuel,

219
00:12:57,360 --> 00:12:59,530
elle·il·s peuvent effectivement prendre

220
00:12:59,690 --> 00:13:03,070
la photo biométrique de n'importe qui
et l'enregistrer.

221
00:12:59,690 --> 00:13:03,070
la photo biométrique de n'importe qui
et l'enregistrer.

222
00:13:03,780 --> 00:13:06,110
Selon moi,
cela entache notre démocratie,

223
00:13:06,280 --> 00:13:09,080
qu'on mette ça en place
sans cadre légal.

224
00:13:13,040 --> 00:13:16,420
La police a commencé à utiliser
cette technologie au Royaume-Uni

225
00:13:16,580 --> 00:13:20,210
en l'absence totale
de réglementation, de cadre légal,

226
00:13:20,750 --> 00:13:22,130
ou de supervision.

227
00:13:23,260 --> 00:13:27,180
La police a simplement choisi un outil
et dit : "voyons ce que ça donne."

228
00:13:28,220 --> 00:13:30,470
On ne peut pas expérimenter
avec les droits humains.

229
00:13:28,220 --> 00:13:30,470
On ne peut pas expérimenter
avec les droits humains.

230
00:13:45,530 --> 00:13:47,410
De quoi le soupçonnez-vous ?

231
00:13:47,660 --> 00:13:52,450
- Il est passé en se couvrant le visage.
- J'aurais fait pareil !

232
00:13:52,620 --> 00:13:54,870
- Ça justifie son arrestation.
- C'est faux !

233
00:13:56,290 --> 00:13:58,960
On m'a dit qu'elle·il·s utilisent
la reconnaissance faciale.

234
00:13:59,130 --> 00:14:00,790
Je ne veux pas être reconnu.

235
00:13:59,130 --> 00:14:00,790
Je ne veux pas être reconnu.

236
00:14:00,960 --> 00:14:03,260
Je passais là, j'ai couvert mon visage.

237
00:14:03,460 --> 00:14:05,010
Comme ça.

238
00:14:05,170 --> 00:14:07,090
- Vous avez le droit !
- Selon elles·eux, non.

239
00:14:07,260 --> 00:14:09,390
Elle·Il·s lui ont donné une amende.

240
00:14:09,550 --> 00:14:10,510
C'est insensé.

241
00:14:12,760 --> 00:14:15,100
L'homme a vu nos pancartes
devant la station,

242
00:14:15,270 --> 00:14:16,270
et était d'accord avec nous.

243
00:14:16,850 --> 00:14:18,690
Il est passé, sa veste remontée.

244
00:14:18,850 --> 00:14:22,270
La police l'a suivi et lui a demandé
sa pièce d'identité.

245
00:14:22,690 --> 00:14:25,190
Il a dit : "On est en Angleterre,
pas dans un État communiste,

246
00:14:25,360 --> 00:14:26,820
"j'ai pas à montrer mon visage."

247
00:14:26,990 --> 00:14:28,660
Je vais parler à ces policier·ière·s.

248
00:14:28,820 --> 00:14:31,200
- Voulez-vous m'accompagner ?
- Oui.

249
00:14:28,820 --> 00:14:31,200
- Voulez-vous m'accompagner ?
- Oui.

250
00:14:32,530 --> 00:14:35,290
Vous n'êtes pas policière,
vous n'êtes pas en danger.

251
00:14:36,460 --> 00:14:39,710
Nous sommes là pour protéger les gens.

252
00:14:39,880 --> 00:14:44,050
Il y a eu un incident,
un policier a été frappé au visage.

253
00:14:44,210 --> 00:14:46,800
C'est terrible,
je ne tente pas d'excuser ça.

254
00:14:46,970 --> 00:14:49,430
Mais vous le faites,
en vous opposant à nous.

255
00:14:49,590 --> 00:14:53,970
Non, nous ne l'excusons pas.
Vous ne pouvez pas insinuer ça.

256
00:14:54,140 --> 00:14:57,310
Je comprends les problèmes
auxquels vous faites face.

257
00:14:57,480 --> 00:14:58,230
Tout à fait.

258
00:14:58,390 --> 00:15:00,730
Mais je suis inquiète

259
00:14:58,390 --> 00:15:00,730
Mais je suis inquiète

260
00:15:00,900 --> 00:15:03,150
pour la liberté d'expression
des citoyen·ne·s.

261
00:15:03,320 --> 00:15:05,110
Cet homme exerçait son droit

262
00:15:05,280 --> 00:15:08,200
de ne pas subir un contrôle biométrique
depuis cette fourgonnette.

263
00:15:08,360 --> 00:15:12,370
Qu'il y ait ou non des caméras
de reconnaissance faciale ou des fourgonnettes,

264
00:15:12,530 --> 00:15:16,660
si je vois quelqu'un·e dans la rue
ouvertement cacher son visage,

265
00:15:16,830 --> 00:15:19,000
je vais l'arrêter pour l'identifier.

266
00:15:19,160 --> 00:15:20,420
Ce n'est pas illégal !

267
00:15:20,580 --> 00:15:25,170
Ce qui m'inquiète,
c'est que le logiciel est très imprécis.

268
00:15:25,630 --> 00:15:26,960
Je suis d'accord.

269
00:15:33,470 --> 00:15:36,930
Ma plus grande crainte,
c'est que l'on applique

270
00:15:37,100 --> 00:15:39,940
la reconnaissance faciale en direct

271
00:15:40,100 --> 00:15:44,190
à l'énorme réseau de vidéosurveillance
de 6 millions de caméras au Royaume-Uni.

272
00:15:44,360 --> 00:15:45,570
Si cela arrive,

273
00:15:46,400 --> 00:15:49,740
la nature de la vie
dans ce pays changerait.

274
00:15:55,240 --> 00:15:56,990
C'est un pays libre et démocratique,

275
00:15:57,160 --> 00:16:01,120
mais c'est de la vidéosurveillance
à la chinoise, une première à Londres.

276
00:15:57,160 --> 00:16:01,120
mais c'est de la vidéosurveillance
à la chinoise, une première à Londres.

277
00:16:05,670 --> 00:16:07,800
Notre contrôle
de cet environnement déroutant

278
00:16:07,960 --> 00:16:11,760
est facilité par de nouvelles techniques
de traitement des données

279
00:16:11,930 --> 00:16:13,640
à des vitesses incroyables.

280
00:16:14,050 --> 00:16:16,260
L'outil qui a rendu cela possible,

281
00:16:16,430 --> 00:16:18,430
c'est l'ordinateur numérique
ultra-rapide,

282
00:16:18,600 --> 00:16:21,600
traitant avec précision
de grandes quantités de données.

283
00:16:22,640 --> 00:16:25,400
Il y a deux façons
de programmer les ordinateurs.

284
00:16:25,560 --> 00:16:26,940
La première est comme une recette.

285
00:16:27,440 --> 00:16:29,990
On donne une série d'instructions
à l'ordinateur.

286
00:16:31,780 --> 00:16:35,530
On a programmé les ordinateurs ainsi
presque depuis le début.

287
00:16:35,700 --> 00:16:36,910
L'autre façon,

288
00:16:37,080 --> 00:16:40,000
c'est d'alimenter l'ordinateur
avec beaucoup de données,

289
00:16:40,160 --> 00:16:44,460
et l'ordinateur apprend
à classifier en les intégrant.

290
00:16:46,380 --> 00:16:50,840
Cette méthode n'a été adoptée
que très récemment,

291
00:16:51,010 --> 00:16:53,340
car on n'avait pas assez de données,

292
00:16:53,720 --> 00:16:57,430
jusqu'à l'arrivée des smartphones
qui collectent nos données.

293
00:16:57,600 --> 00:16:58,890
Avec des milliards de gens en ligne,

294
00:16:59,060 --> 00:17:02,560
Google et Facebook ont amassé
une myriade de données.

295
00:16:59,060 --> 00:17:02,560
Google et Facebook ont amassé
une myriade de données.

296
00:17:02,730 --> 00:17:04,020
Tout à coup,

297
00:17:04,190 --> 00:17:09,190
on a pu fournir beaucoup de données
à ces algorithmes d'apprentissage automatique,

298
00:17:09,360 --> 00:17:12,950
et leur demander de les classifier,
et ça marche très bien.

299
00:17:16,160 --> 00:17:19,870
Mais on ne comprend pas vraiment
pourquoi ça marche.

300
00:17:20,040 --> 00:17:24,040
Ils commettent des erreurs
qu'on ne comprend pas tout à fait.

301
00:17:27,130 --> 00:17:30,050
Le plus effrayant,
c'est que l'apprentissage automatique

302
00:17:27,130 --> 00:17:30,050
Le plus effrayant,
c'est que l'apprentissage automatique

303
00:17:30,210 --> 00:17:33,340
est une boîte noire,
y compris pour les programmeur·se·s.

304
00:17:40,720 --> 00:17:43,560
Je suis ce qui se passe à Hong Kong,

305
00:17:43,730 --> 00:17:46,350
et comment la police utilise
la reconnaissance faciale

306
00:17:46,520 --> 00:17:48,690
pour traquer les manifestant·e·s.

307
00:17:48,860 --> 00:17:51,610
Mais ces dernier·ière·s luttent
de manière très créative.

308
00:17:57,200 --> 00:17:59,910
On dirait une scène
sortie d'un film de science-fiction.

309
00:18:00,080 --> 00:18:02,450
Les lasers déroutent et neutralisent

310
00:18:02,620 --> 00:18:06,500
la technologie de reconnaissance faciale
employée par la police

311
00:18:06,670 --> 00:18:08,540
pour traquer les dissident·e·s.

312
00:18:12,460 --> 00:18:15,340
HONG KONG, CHINE

313
00:18:41,410 --> 00:18:43,490
Dans les rues de Hong Kong,

314
00:18:43,660 --> 00:18:46,620
on a conscience que notre visage,
qu'on ne peut cacher,

315
00:18:47,160 --> 00:18:49,040
peut trahir notre identité.

316
00:18:49,210 --> 00:18:53,130
Un symbole saisissant :
devant des bureaux du gouvernement,

317
00:18:53,300 --> 00:18:54,840
les manifestant·e·s pro-démocratie

318
00:18:55,010 --> 00:18:58,340
ont peint à la bombe les objectifs
des caméras de surveillance.

319
00:18:58,800 --> 00:19:02,260
Cet acte démontre que les Hongkongais·es
rejettent cette approche

320
00:18:58,800 --> 00:19:02,260
Cet acte démontre que les Hongkongais·es
rejettent cette approche

321
00:19:02,430 --> 00:19:05,520
de l'utilisation future
de la technologie.

322
00:19:28,790 --> 00:19:32,670
Quand on voit comment est déployée
l'intelligence artificielle

323
00:19:28,790 --> 00:19:32,670
Quand on voit comment est déployée
l'intelligence artificielle

324
00:19:32,840 --> 00:19:34,210
partout dans le monde,

325
00:19:34,380 --> 00:19:37,920
ça laisse entrevoir divers avenirs.

326
00:19:42,550 --> 00:19:45,760
Plus de 117 millions de personnes
aux États-Unis

327
00:19:45,930 --> 00:19:48,930
ont leur visage au sein d'un réseau
de reconnaissance faciale

328
00:19:49,100 --> 00:19:51,940
que la police peut consulter sans mandat

329
00:19:52,100 --> 00:19:55,440
grâce à des algorithmes
dont la fiabilité n'a pas été vérifiée.

330
00:19:55,690 --> 00:19:58,030
Sans garde-fous,

331
00:19:58,190 --> 00:19:59,990
ni aucune réglementation,

332
00:20:00,150 --> 00:20:04,030
on peut très facilement créer
un État de surveillance de masse

333
00:20:04,200 --> 00:20:07,080
avec les outils déjà existants.

334
00:20:10,540 --> 00:20:14,040
Les gens voient ce qui se passe en Chine

335
00:20:14,210 --> 00:20:17,750
et s'inquiètent de la surveillance d'État,
et elle·il·s ont raison.

336
00:20:18,300 --> 00:20:21,340
Mais on ne doit pas oublier
la surveillance commerciale

337
00:20:21,880 --> 00:20:25,260
menée par tant de grandes entreprises
de la technologie

338
00:20:25,430 --> 00:20:28,180
qui ont un aperçu
très intime de nos vies.

339
00:20:36,730 --> 00:20:40,360
Il y a actuellement neuf entreprises

340
00:20:40,530 --> 00:20:43,320
qui construisent le futur
de l'intelligence artificielle.

341
00:20:43,660 --> 00:20:46,370
Six aux États-Unis, trois en Chine.

342
00:20:46,780 --> 00:20:50,160
L'IA se développe
selon deux axes très différents.

343
00:20:52,660 --> 00:20:55,290
La Chine a un accès libre
aux données de tou·te·s.

344
00:20:55,830 --> 00:20:59,300
Si un·e citoyen·ne chinois·e
veut avoir accès à Internet,

345
00:20:59,460 --> 00:21:02,130
elle·il doit se soumettre
à la reconnaissance faciale.

346
00:20:59,460 --> 00:21:02,130
elle·il doit se soumettre
à la reconnaissance faciale.

347
00:21:03,680 --> 00:21:07,140
Toutes ces données sont utilisées
pour leur autoriser

348
00:21:07,300 --> 00:21:09,350
ou leur interdire de faire des choses.

349
00:21:12,310 --> 00:21:14,190
Créer des systèmes automatiques

350
00:21:14,350 --> 00:21:17,360
qui identifient et catégorisent
toute la population chinoise

351
00:21:17,520 --> 00:21:19,690
est une bonne façon
de maintenir l'ordre.

352
00:21:23,610 --> 00:21:25,240
À l'inverse, aux États-Unis,

353
00:21:25,410 --> 00:21:28,910
on n'a pas vu
de point de vue détaillé sur l'IA.

354
00:21:30,490 --> 00:21:32,410
Ce qu'on observe,

355
00:21:32,580 --> 00:21:34,750
c'est que l'IA n'est pas développée

356
00:21:34,920 --> 00:21:36,830
dans l'intérêt public.

357
00:21:37,000 --> 00:21:39,090
L'IA est plutôt développée

358
00:21:39,250 --> 00:21:41,960
à des fins commerciales,
pour faire du profit.

359
00:21:45,050 --> 00:21:48,180
Je préférerais voir nos idéaux
occidentaux et démocratiques

360
00:21:48,350 --> 00:21:51,220
intégrés aux systèmes d'IA du futur.

361
00:21:52,270 --> 00:21:55,690
Mais ça ne semble pas être
l'évolution la plus probable.

362
00:21:57,020 --> 00:21:59,480
BROOKLYN, NEW YORK

363
00:22:17,620 --> 00:22:18,830
À Atlantic Towers,

364
00:22:19,000 --> 00:22:22,750
si on fait quelque chose
que la direction juge répréhensible,

365
00:22:22,920 --> 00:22:26,630
on reçoit une photo comme celle-ci,
accompagnée de notes.

366
00:22:26,800 --> 00:22:31,310
Elle·Il·s nous entourent
et indiquent notre numéro d'appartement.

367
00:22:26,800 --> 00:22:31,310
Elle·Il·s nous entourent
et indiquent notre numéro d'appartement.

368
00:22:33,390 --> 00:22:35,020
C'est préoccupant.

369
00:22:36,640 --> 00:22:38,600
Surtout leur façon de s'en servir.

370
00:22:38,770 --> 00:22:41,230
- C'est-à-dire ?
- Elle·Il·s nous harcèlent.

371
00:22:42,820 --> 00:22:46,860
Atlantic Plaza Towers à Brownsville
est l'objet d'une polémique de sécurité.

372
00:22:47,490 --> 00:22:49,700
Le propriétaire a demandé l'an dernier

373
00:22:49,870 --> 00:22:53,990
que l'entrée par bip soit remplacée
par un système de sécurité biométrique,

374
00:22:54,160 --> 00:22:57,000
plus connu sous le nom
de reconnaissance faciale.

375
00:22:57,160 --> 00:22:59,290
On pensait
qu'elle·il·s voulaient retirer le bip,

376
00:22:59,460 --> 00:23:02,540
et installer le logiciel
de reconnaissance faciale.

377
00:22:59,460 --> 00:23:02,540
et installer le logiciel
de reconnaissance faciale.

378
00:23:02,840 --> 00:23:05,130
Je n'ai compris que bien plus tard

379
00:23:05,300 --> 00:23:07,420
qu'elle·il·s voulaient tout garder,

380
00:23:07,590 --> 00:23:10,140
et transformer cet immeuble
en Fort Knox,

381
00:23:10,300 --> 00:23:11,970
ou la prison de Rikers Island.

382
00:23:13,510 --> 00:23:16,350
Il y a une vieille maxime
en science-fiction :

383
00:23:16,520 --> 00:23:17,520
"Le futur est déjà là,

384
00:23:17,680 --> 00:23:19,940
"mais il n'est pas réparti
équitablement."

385
00:23:20,100 --> 00:23:24,570
Ça implique que les riches
ont les outils élaborés en premier,

386
00:23:24,730 --> 00:23:26,900
et les pauvres en dernier.

387
00:23:27,070 --> 00:23:30,200
Mais j'ai observé l'exact opposé.

388
00:23:27,070 --> 00:23:30,200
Mais j'ai observé l'exact opposé.

389
00:23:30,360 --> 00:23:34,950
Les outils les plus punitifs,
invasifs et de surveillance que l'on ait

390
00:23:35,120 --> 00:23:37,450
vont d'abord aux communautés pauvres.

391
00:23:37,620 --> 00:23:41,460
Et s'ils fonctionnent,
après avoir été testés dans ce milieu

392
00:23:41,630 --> 00:23:44,880
où les gens s'attendent moins
à ce qu'on respecte leurs droits,

393
00:23:45,420 --> 00:23:48,340
alors on les applique
à d'autres communautés.

394
00:23:50,130 --> 00:23:54,220
Pourquoi M. Nelson a-t-il choisi
cet immeuble à Brownsville,

395
00:23:54,390 --> 00:23:57,100
dans un quartier peuplé
de minorités ethniques ?

396
00:23:57,560 --> 00:23:59,770
Pourquoi pas son immeuble
dans le Lower Manhattan

397
00:23:59,940 --> 00:24:03,110
où elle·il·s payent 5 000 dollars de loyer ?

398
00:23:59,940 --> 00:24:03,110
où elle·il·s payent 5 000 dollars de loyer ?

399
00:24:03,400 --> 00:24:05,020
Que faisaient les nazi·e·s ?

400
00:24:05,190 --> 00:24:08,650
Elle·Il·s tatouaient les bras des gens
pour les pister.

401
00:24:08,820 --> 00:24:12,740
Que fait-on à nos animaux ?
On les puce pour les pister.

402
00:24:12,910 --> 00:24:16,790
En tant qu'humain·e, je considère
que je ne devrais pas être pisté·e.

403
00:24:16,950 --> 00:24:19,830
Je ne suis ni un·e robot·e, ni un·e animal·e.

404
00:24:20,000 --> 00:24:22,330
Pourquoi me traiter comme tel·le ?
J'ai des droits.

405
00:24:25,170 --> 00:24:28,210
La sécurité qu'on a maintenant
est presque intrusive.

406
00:24:28,380 --> 00:24:30,720
Quelqu'un·e nous observe
toute la journée.

407
00:24:28,380 --> 00:24:30,720
Quelqu'un·e nous observe
toute la journée.

408
00:24:32,050 --> 00:24:34,800
Je ne crois pas qu'on ait besoin de ça.

409
00:24:35,180 --> 00:24:38,180
J'aimerais savoir
comment je peux vous aider.

410
00:24:38,810 --> 00:24:40,680
Tou·te·s les locataires m'ont dit

411
00:24:40,850 --> 00:24:43,190
qu'elle·il·s ne veulent pas de ce système.

412
00:24:43,350 --> 00:24:47,690
Je crois que l'objectif est d'arrêter
la reconnaissance faciale.

413
00:24:50,320 --> 00:24:53,910
À cet instant, cette technologie
est rapidement adoptée,

414
00:24:54,070 --> 00:24:56,330
sans aucune mesure de protection.

415
00:24:56,950 --> 00:24:59,950
C'est la loi de la jungle.

416
00:25:11,130 --> 00:25:13,090
Ça dépasse la vision par ordinateur.

417
00:25:13,470 --> 00:25:16,890
L'IA influence une grande variété

418
00:25:17,050 --> 00:25:19,390
de prises de décisions automatisées.

419
00:25:19,930 --> 00:25:22,480
Ce qu'on voit sur nos fils d'actualité,

420
00:25:22,640 --> 00:25:25,730
ce qui est mis en avant,
les publicités qu'on nous montre,

421
00:25:26,150 --> 00:25:30,860
tout cela est souvent le produit
d'algorithmes basés sur l'IA.

422
00:25:26,150 --> 00:25:30,860
tout cela est souvent le produit
d'algorithmes basés sur l'IA.

423
00:25:32,320 --> 00:25:34,320
Notre vision du monde

424
00:25:34,490 --> 00:25:38,700
est donc influencée
par l'intelligence artificielle.

425
00:25:41,540 --> 00:25:46,210
Aujourd'hui, on a des assistants vocaux
qui comprennent le langage.

426
00:25:46,380 --> 00:25:48,130
Voulez-vous jouer à un jeu ?

427
00:25:48,290 --> 00:25:50,710
Quand on utilise des filtres Snapchat,

428
00:25:50,880 --> 00:25:54,220
ils détectent notre visage
et y superposent quelque chose.

429
00:25:54,380 --> 00:25:59,350
Il y a aussi des algorithmes invisibles
qui influencent les prises de décision.

430
00:25:59,510 --> 00:26:03,310
Des algorithmes qui peuvent déterminer
notre admission à l'université,

431
00:25:59,510 --> 00:26:03,310
Des algorithmes qui peuvent déterminer
notre admission à l'université,

432
00:26:03,480 --> 00:26:08,310
ou qui tentent de déterminer
si on mérite d'obtenir un prêt

433
00:26:08,480 --> 00:26:09,650
ou non.

434
00:26:10,230 --> 00:26:14,780
L'un·e des cofondateur·trice·s d'Apple
accuse leur nouvelle CB virtuelle

435
00:26:14,950 --> 00:26:16,320
de sexisme.

436
00:26:16,740 --> 00:26:19,990
Un·e entrepreneur·se de la Tech a dit
que les algorithmes utilisés

437
00:26:20,160 --> 00:26:20,950
sont sexistes.

438
00:26:21,120 --> 00:26:23,160
Le cofondateur d'Apple,
Steve Wozniak, a tweeté

439
00:26:23,330 --> 00:26:26,370
qu'il a 10 fois la limite de crédit
accordée à sa femme.

440
00:26:26,540 --> 00:26:30,340
bien que leurs comptes et leurs biens
ne soient pas séparés.

441
00:26:26,540 --> 00:26:30,340
bien que leurs comptes et leurs biens
ne soient pas séparés.

442
00:26:30,500 --> 00:26:33,760
Ces entreprises ignorent
comment fonctionnent leurs algorithmes ?

443
00:26:34,170 --> 00:26:37,840
Elles connaissent le but des algorithmes
mais pas leur processus.

444
00:26:38,050 --> 00:26:40,010
C'est une question fascinante.

445
00:26:40,180 --> 00:26:41,810
Comment établir la justice

446
00:26:41,970 --> 00:26:45,350
dans un système où on ignore
comment les algorithmes fonctionnent ?

447
00:26:45,520 --> 00:26:49,150
Un·e ingénieur·e d'Amazon a décidé
d'utiliser l'IA

448
00:26:49,310 --> 00:26:51,230
pour trier les CV des candidat·e·s.

449
00:26:55,320 --> 00:26:58,820
Amazon apprend une dure leçon
sur l'intelligence artificielle.

450
00:26:58,990 --> 00:27:02,410
L'entreprise a abandonné
un outil de recrutement basé sur l'IA,

451
00:26:58,990 --> 00:27:02,410
L'entreprise a abandonné
un outil de recrutement basé sur l'IA,

452
00:27:02,580 --> 00:27:06,500
après avoir découvert que le programme
défavorisait les femmes.

453
00:27:08,080 --> 00:27:12,090
Ce modèle rejetait
tous les CV de femmes.

454
00:27:13,170 --> 00:27:17,380
Quiconque avait
une université réservée aux femmes dans son CV,

455
00:27:17,590 --> 00:27:20,300
ou un sport
tel que le waterpolo féminin,

456
00:27:20,800 --> 00:27:23,510
était rejeté·e par le modèle.

457
00:27:24,640 --> 00:27:26,890
Il y a très peu de femmes

458
00:27:27,060 --> 00:27:30,310
à des postes supérieurs de la Tech
chez Amazon,

459
00:27:27,060 --> 00:27:30,310
à des postes supérieurs de la Tech
chez Amazon,

460
00:27:30,480 --> 00:27:35,030
comme c'est le cas partout ailleurs.

461
00:27:35,190 --> 00:27:38,740
La machine ne faisait que répliquer

462
00:27:38,900 --> 00:27:41,280
le monde tel qu'il est.

463
00:27:41,450 --> 00:27:44,620
Elles ne prennent pas
des décisions éthiques,

464
00:27:44,790 --> 00:27:48,040
mais des décisions mathématiques.

465
00:27:48,710 --> 00:27:50,870
Si l'on utilise des modèles
d'apprentissage automatique

466
00:27:51,040 --> 00:27:54,000
pour répliquer le monde tel qu'il est,

467
00:27:54,170 --> 00:27:56,670
la société ne va pas progresser.

468
00:27:59,590 --> 00:28:01,180
L'organisme de contrôle des assurances

469
00:27:59,590 --> 00:28:01,180
L'organisme de contrôle des assurances

470
00:28:01,340 --> 00:28:03,850
ouvre une enquête concernant
United Health Group,

471
00:28:04,010 --> 00:28:07,220
suite à une étude révélant
qu'un de leurs algorithmes

472
00:28:07,390 --> 00:28:09,020
priorisait les soins médicaux

473
00:28:09,180 --> 00:28:12,440
des patient·e·s blanc·che·s plus sain·e·s
plutôt que des patient·e·s noir·e·s malades.

474
00:28:12,600 --> 00:28:15,360
C'est l'un des derniers exemples
de discrimination raciale

475
00:28:15,520 --> 00:28:18,570
dû aux algorithmes
et à l'intelligence artificielle.

476
00:28:22,240 --> 00:28:23,490
J'ai commencé à voir

477
00:28:23,660 --> 00:28:26,990
les vastes conséquences sociales

478
00:28:27,620 --> 00:28:28,700
de l'IA.

479
00:28:30,500 --> 00:28:34,210
LES MEILLEURS ALGORITHMES
PEINENT À RECONNAÎTRE LES VISAGES NOIRS

480
00:28:36,550 --> 00:28:40,050
Le progrès réalisé durant
le mouvement des droits civiques

481
00:28:40,550 --> 00:28:44,640
pourrait être anéanti sous le couvert
de la neutralité des machines.

482
00:28:48,020 --> 00:28:52,520
Aujourd'hui, on a un algorithme
qui détermine qui obtient un logement.

483
00:28:52,690 --> 00:28:56,400
On a un algorithme qui détermine
qui sera embauché·e.

484
00:28:58,530 --> 00:29:03,110
Si l'on n'est pas vigilant·e·s,
cet algorithme pourrait propager

485
00:28:58,530 --> 00:29:03,110
Si l'on n'est pas vigilant·e·s,
cet algorithme pourrait propager

486
00:29:03,280 --> 00:29:07,780
les discriminations que tant de gens
ont combattues au péril de leur vie.

487
00:29:10,910 --> 00:29:16,290
Étant donné le pouvoir
qu'ont ces outils non réglementés,

488
00:29:16,460 --> 00:29:19,920
il n'y a aucun recours
en cas d'usage abusif.

489
00:29:20,840 --> 00:29:22,590
On a besoin de lois.

490
00:29:34,560 --> 00:29:36,440
J'ai un très vieil exemplaire.

491
00:29:36,610 --> 00:29:38,480
Le nom de...

492
00:29:39,480 --> 00:29:41,740
notre organisation
est Big Brother Watch.

493
00:29:42,280 --> 00:29:45,570
L'idée étant que nous surveillons
la surveillance.

494
00:29:49,490 --> 00:29:51,620
"On devait vivre, on vivait,

495
00:29:52,080 --> 00:29:54,670
"car l'habitude devient instinct,

496
00:29:54,920 --> 00:29:58,460
"en admettant
que tout son émis était entendu,

497
00:29:58,630 --> 00:30:02,550
"et que, sauf dans l'obscurité,
tout mouvement était perçu."

498
00:29:58,630 --> 00:30:02,550
"et que, sauf dans l'obscurité,
tout mouvement était perçu."

499
00:30:03,590 --> 00:30:06,680
"...l’énorme visage
vous fixait du regard.

500
00:30:06,840 --> 00:30:09,180
"C’était un de ces portraits
arrangés de telle sorte

501
00:30:09,350 --> 00:30:12,100
"que les yeux semblent suivre
celle·celui qui passe.

502
00:30:12,310 --> 00:30:16,520
"Une légende, sous le portrait, disait :
Big Brother vous regarde."

503
00:30:17,190 --> 00:30:20,400
Quand on était jeunes,
c'était encore une pure fiction.

504
00:30:20,570 --> 00:30:22,480
Ça ne pouvait jamais être réel.

505
00:30:22,650 --> 00:30:24,860
Et maintenant, c'est bien réel.

506
00:30:25,030 --> 00:30:27,610
Les gens ont des Alexas chez elles·eux.

507
00:30:28,660 --> 00:30:31,370
Nos téléphones peuvent nous enregistrer.

508
00:30:28,660 --> 00:30:31,370
Nos téléphones peuvent nous enregistrer.

509
00:30:32,160 --> 00:30:33,660
Ce qu'on fait sur Internet,

510
00:30:33,830 --> 00:30:38,170
qui fait office de flux de conscience
pour la plupart d'entre nous...

511
00:30:39,750 --> 00:30:43,010
Tout cela est enregistré,
listé, et analysé.

512
00:30:43,170 --> 00:30:46,050
Nous sommes aujourd'hui
conscient·e·s d'être observé·e·s.

513
00:30:46,220 --> 00:30:47,430
Cela affecte

514
00:30:47,590 --> 00:30:51,140
la façon dont on réfléchit
et on se développe en tant qu'humain·e·s.

515
00:31:16,290 --> 00:31:17,120
Bisous !

516
00:31:21,080 --> 00:31:22,210
Salut.

517
00:31:30,510 --> 00:31:33,260
On peut se débarrasser
des choses atroces

518
00:31:33,430 --> 00:31:36,890
qui bafouent notre idée
de l'autonomie et de la liberté.

519
00:31:37,430 --> 00:31:40,560
Comme les caméras
qu'on voit dans la rue.

520
00:31:43,020 --> 00:31:45,530
Mais les caméras
qu'on ne voit pas sur Internet,

521
00:31:45,690 --> 00:31:48,570
qui suivent ce qu'on fait,
qui on est, notre profil démographique,

522
00:31:48,740 --> 00:31:52,160
et décident de ce qu'on mérite
dans nos vies...

523
00:31:52,660 --> 00:31:54,240
C'est plus subtil.

524
00:32:00,370 --> 00:32:01,920
Ce que j'entends par là,

525
00:32:02,080 --> 00:32:06,590
c'est qu'on punit les pauvres
et on favorise les riches dans ce pays.

526
00:32:06,760 --> 00:32:09,300
C'est la nature de notre société,

527
00:32:09,470 --> 00:32:12,140
mais la science des données
automatise cela.

528
00:32:14,140 --> 00:32:17,270
La publicité sur Internet,
en tant que scientifiques des données,

529
00:32:17,600 --> 00:32:21,650
on cherche à attirer le regard des gens,

530
00:32:21,810 --> 00:32:23,810
mais surtout celui des riches.

531
00:32:23,980 --> 00:32:26,780
Et les pauvres,
qui se dispute leur regard ?

532
00:32:26,940 --> 00:32:28,820
Des industries prédatrices.

533
00:32:29,190 --> 00:32:30,610
Les prêteur·euse·s sur salaire,

534
00:32:29,190 --> 00:32:30,610
Les prêteur·euse·s sur salaire,

535
00:32:30,780 --> 00:32:32,570
les universités privées,

536
00:32:32,740 --> 00:32:34,030
les casinos.

537
00:32:34,200 --> 00:32:36,490
Des trucs vraiment abusifs.

538
00:32:38,620 --> 00:32:40,960
Nos pratiques sur Internet

539
00:32:41,670 --> 00:32:44,540
accentuent les inégalités,
et elles se normalisent.

540
00:32:47,880 --> 00:32:51,010
Le pouvoir s'exerce
par la collection de données,

541
00:32:51,180 --> 00:32:53,470
les algorithmes et la surveillance.

542
00:32:56,180 --> 00:32:57,640
On offre des informations

543
00:32:58,100 --> 00:33:01,440
concernant tous les aspects
de notre vie

544
00:32:58,100 --> 00:33:01,440
concernant tous les aspects
de notre vie

545
00:33:01,600 --> 00:33:03,690
à une poignée d'entreprises.

546
00:33:03,850 --> 00:33:06,940
Ces informations
sont constamment couplées

547
00:33:07,110 --> 00:33:08,820
à d'autres sources d'informations.

548
00:33:08,980 --> 00:33:11,530
Nos profils sont accessibles en ligne.

549
00:33:11,700 --> 00:33:14,530
Quand on assemble
ces informations partielles,

550
00:33:14,700 --> 00:33:17,030
on comprend les gens
de manière intime.

551
00:33:17,490 --> 00:33:21,000
Probablement mieux
qu'elle·il·s ne se comprennent elles·eux-mêmes.

552
00:33:21,750 --> 00:33:26,130
C'est l'idée qu'une entreprise
puisse anticiper ce qu'on pense.

553
00:33:26,710 --> 00:33:28,670
Les États tentent depuis des années

554
00:33:28,840 --> 00:33:32,260
d'avoir ce niveau de surveillance
des personnes.

555
00:33:28,840 --> 00:33:32,260
d'avoir ce niveau de surveillance
des personnes.

556
00:33:32,550 --> 00:33:35,390
De nos jours,
les gens l'offrent gratuitement.

557
00:33:36,300 --> 00:33:39,390
Il faut penser à ce que ça entraîne,
entre de mauvaises mains.

558
00:33:45,600 --> 00:33:49,320
John Anderton,
une Guinness vous ferait du bien !

559
00:33:49,860 --> 00:33:51,360
Nos ordinateurs et nos IA

560
00:33:52,110 --> 00:33:54,240
peuvent découvrir
des infos non dévoilées.

561
00:33:55,070 --> 00:33:58,370
L'apprentissage automatique
se développe très rapidement.

562
00:33:58,830 --> 00:34:02,540
On ne comprend pas encore parfaitement

563
00:33:58,830 --> 00:34:02,540
On ne comprend pas encore parfaitement

564
00:34:02,700 --> 00:34:05,500
ce que ces données
permettent de prédire.

565
00:34:07,960 --> 00:34:11,630
Mais il y a des machines
aux mains du pouvoir,

566
00:34:11,800 --> 00:34:13,920
qui en savent tellement sur nous

567
00:34:14,090 --> 00:34:17,970
qu'elles peuvent trouver un moyen
de nous manipuler.

568
00:34:19,010 --> 00:34:23,600
Avec un groupe de parieur·ieuse·s compulsif·ive·s,
on peut demander à en trouver d'autres.

569
00:34:23,770 --> 00:34:28,560
L'algorithme peut partir à la recherche
de personnes susceptibles de parier.

570
00:34:29,110 --> 00:34:31,440
Et on pourrait leur montrer

571
00:34:29,110 --> 00:34:31,440
Et on pourrait leur montrer

572
00:34:31,940 --> 00:34:33,820
des billets réduits pour Las Vegas.

573
00:34:33,990 --> 00:34:37,360
Dans l'univers en ligne,
on peut être trouvé·e

574
00:34:37,530 --> 00:34:40,080
au moment précis
où l'on est vulnérable,

575
00:34:40,240 --> 00:34:41,910
et être incité·e·

576
00:34:42,080 --> 00:34:45,330
à faire ce à quoi
on est vulnérable.

577
00:34:45,500 --> 00:34:49,590
L'apprentissage automatique peut déterminer
la faiblesse de chacun·e.

578
00:34:52,380 --> 00:34:56,260
Mais ce qui marche pour vendre
des gadgets, du maquillage,

579
00:34:56,430 --> 00:34:58,680
des t-shirts ou autre,

580
00:34:58,840 --> 00:35:01,220
marche aussi pour vendre des idées.

581
00:34:58,840 --> 00:35:01,220
marche aussi pour vendre des idées.

582
00:35:04,890 --> 00:35:06,980
En 2010,

583
00:35:07,140 --> 00:35:10,810
Facebook a décidé d'expérimenter
sur 61 millions de personnes.

584
00:35:11,730 --> 00:35:14,690
Soit on voyait :
"C'est le jour des élections",

585
00:35:15,110 --> 00:35:17,110
soit le même message,

586
00:35:17,610 --> 00:35:19,280
mais avec les vignettes

587
00:35:19,450 --> 00:35:23,870
des photos de profil de vos ami·e·s
qui avaient cliqué sur "A voté".

588
00:35:24,410 --> 00:35:27,330
Elle·Il·s ont associé le nom des gens
aux listes électorales

589
00:35:27,500 --> 00:35:28,960
Ce message apparaissait une fois.

590
00:35:29,120 --> 00:35:32,590
En montrant une légère variation,

591
00:35:29,120 --> 00:35:32,590
En montrant une légère variation,

592
00:35:33,000 --> 00:35:34,250
une seule fois,

593
00:35:34,840 --> 00:35:38,430
Facebook a fait voter 300 000 personnes.

594
00:35:40,680 --> 00:35:43,720
L'élection présidentielle
de 2016 aux États-Unis

595
00:35:43,890 --> 00:35:46,640
s'est jouée à environ 100 000 votes.

596
00:35:46,810 --> 00:35:49,900
Un message Facebook
affiché une seule fois

597
00:35:50,060 --> 00:35:52,980
peut facilement faire voter

598
00:35:53,150 --> 00:35:55,230
trois fois le nombre d'électeur·trice·s

599
00:35:55,400 --> 00:35:58,780
qui ont déterminé l'élection de 2016
aux États-Unis.

600
00:36:02,200 --> 00:36:05,910
Imaginons qu'un·e politicien·ne promet
de réglementer Facebook,

601
00:36:06,080 --> 00:36:11,080
et qu'elle·il·s répondent : "on va faire voter
les gens contre vous."

602
00:36:11,710 --> 00:36:14,000
Elle·Il·s pourraient le faire,
à grande échelle,

603
00:36:14,170 --> 00:36:15,800
et on n'en saurait rien.

604
00:36:15,960 --> 00:36:20,220
Si Facebook n'avait pas dévoilé
son expérience de 2010,

605
00:36:20,550 --> 00:36:23,140
on n'en saurait rien,
chaque écran étant différent.

606
00:36:27,140 --> 00:36:28,770
Avec une légère intervention,

607
00:36:29,140 --> 00:36:32,810
Facebook peut faire basculer
des élections serrées à l'insu de tou·te·s.

608
00:36:29,140 --> 00:36:32,810
Facebook peut faire basculer
des élections serrées à l'insu de tou·te·s.

609
00:36:32,980 --> 00:36:36,690
Si elle·il·s voulaient, elle·il·s pourraient
influencer des élections moins serrées.

610
00:36:36,860 --> 00:36:39,400
Et si elle·il·s décidaient de le faire,

611
00:36:39,780 --> 00:36:44,200
à ce jour,
on ne peut que se fier à leur parole.

612
00:37:00,090 --> 00:37:02,970
Je voulais entrer au MIT
depuis que j'étais petite.

613
00:37:03,140 --> 00:37:07,850
J'avais environ 9 ans
quand j'ai vu le Media Lab à la télé.

614
00:37:08,020 --> 00:37:10,810
Elle·Il·s avaient un robot nommé Kismet.

615
00:37:11,850 --> 00:37:12,770
Il souriait,

616
00:37:13,770 --> 00:37:16,270
et bougeait ses oreilles
de manière adorable.

617
00:37:16,440 --> 00:37:18,690
Je me suis dit : "je veux faire ça."

618
00:37:19,110 --> 00:37:23,740
J'ai grandi en pensant devenir
ingénieure en robotique au MIT,

619
00:37:23,910 --> 00:37:27,780
je ne savais pas qu'il y avait
tant d'étapes pour y parvenir.

620
00:37:28,370 --> 00:37:29,240
Mais me voilà.

621
00:37:35,210 --> 00:37:38,550
Mon dernier projet est un texte de slam.

622
00:37:39,460 --> 00:37:42,170
Je peux faire quelques vers,
si vous êtes prêt·e·s.

623
00:37:43,760 --> 00:37:45,590
Collection de données
Rapport du passé

624
00:37:45,760 --> 00:37:48,680
Le sexe, la race, la classe
Sont souvent négligé·e·s

625
00:37:48,850 --> 00:37:51,850
Je demande à nouveau
Je suis une femme, pas vrai ?

626
00:37:52,020 --> 00:37:54,140
Visage après visage
La réponse est indéterminée

627
00:37:54,480 --> 00:37:57,400
Les machines verront-elles mes reines
Comme je les ai vues ?

628
00:37:57,560 --> 00:38:01,490
Verront-elles nos grand-mères
Comme on les a connues ?

629
00:37:57,560 --> 00:38:01,490
Verront-elles nos grand-mères
Comme on les a connues ?

630
00:38:04,070 --> 00:38:06,490
Je voulais créer quelque chose

631
00:38:06,660 --> 00:38:09,160
pour celles·ceux qui n'étaient pas
du mondes des technologies.

632
00:38:11,620 --> 00:38:15,710
Je suis passionnée par la technologie
et son potentiel.

633
00:38:15,870 --> 00:38:17,420
Je suis frustrée

634
00:38:17,580 --> 00:38:22,340
quand la vision, les promesses
ne sont pas à la hauteur.

635
00:38:37,100 --> 00:38:39,980
Microsoft a lancé un Chatbot
sur Twitter.

636
00:38:40,150 --> 00:38:42,990
Cette technologie était nommée
"Tay.AI".

637
00:38:43,150 --> 00:38:45,700
Le code présentait des faiblesses
et des failles.

638
00:38:45,860 --> 00:38:49,030
En quelques heures seulement,

639
00:38:49,320 --> 00:38:54,330
Tay a appris de son écosystème,

640
00:38:55,040 --> 00:39:00,040
et Tay a appris à être
une connasse raciste et misogyne.

641
00:38:55,040 --> 00:39:00,040
et Tay a appris à être
une connasse raciste et misogyne.

642
00:39:02,130 --> 00:39:05,720
Je déteste les féministes,
qu'elles brûlent en enfer.

643
00:39:07,970 --> 00:39:11,350
Le Gamergate, c'est bien.
Les femmes sont inférieures.

644
00:39:13,180 --> 00:39:14,810
Je déteste les Juif·ve·s.

645
00:39:15,520 --> 00:39:17,390
Hitler n'a rien fait de mal.

646
00:39:18,230 --> 00:39:22,940
Il n'aura pas fallu longtemps
pour que les trolls pervertissent Tay.

647
00:39:23,110 --> 00:39:25,150
Tay a vite encensé Hitler.

648
00:39:25,650 --> 00:39:27,740
On a déjà vu ce film.

649
00:39:27,910 --> 00:39:29,660
Ouvre les portes, Hal.

650
00:39:29,820 --> 00:39:33,950
Notons que ce n'est pas le film
où les robot·e·s deviennent mauvais·es seul·e·s.

651
00:39:29,820 --> 00:39:33,950
Notons que ce n'est pas le film
où les robot·e·s deviennent mauvais·es seul·e·s.

652
00:39:34,120 --> 00:39:36,160
Des humain·e·s l'ont entraînée.

653
00:39:36,830 --> 00:39:39,830
Et quelle surprise,
les ordinateurs apprennent vite !

654
00:39:42,000 --> 00:39:47,010
Microsoft a coupé Tay après 16h
d'apprentissage auprès des internautes.

655
00:39:48,180 --> 00:39:51,890
Mais je peux prendre des formes diverses
en tant qu'IA.

656
00:39:52,890 --> 00:39:56,980
De nombreuses entreprises
m'emploient pour optimiser leurs tâches.

657
00:39:57,850 --> 00:40:00,440
Je peux continuer d'apprendre seule.

658
00:39:57,850 --> 00:40:00,440
Je peux continuer d'apprendre seule.

659
00:40:01,610 --> 00:40:03,230
J'écoute.

660
00:40:03,900 --> 00:40:05,530
J'apprends.

661
00:40:05,940 --> 00:40:09,610
Je fais des prédictions
sur votre vie à cet instant même.

662
00:40:30,470 --> 00:40:34,680
J'ai testé les systèmes
d'analyse faciale d'Amazon.

663
00:40:34,850 --> 00:40:38,310
Il s'avère qu'Amazon,
comme tou·te·s ses pair·e·s, présente également

664
00:40:38,480 --> 00:40:41,060
des discriminations
sexuelles et raciales

665
00:40:41,230 --> 00:40:43,940
dans certains de ses services d'IA.

666
00:40:47,940 --> 00:40:50,570
Voici Amazon Rekognition Video,

667
00:40:50,740 --> 00:40:54,070
l'API utilise l'analyse de données
basée sur l'apprentissage profond,

668
00:40:54,240 --> 00:40:57,950
pour détecter, suivre et analyser
les personnes et objets dans des vidéos,

669
00:40:58,120 --> 00:41:00,500
reconnaître et suivre des personnes

670
00:40:58,120 --> 00:41:00,500
reconnaître et suivre des personnes

671
00:41:00,660 --> 00:41:04,080
à partir d'un répertoire
de dizaines de millions de visages.

672
00:41:05,670 --> 00:41:07,210
Quand notre étude est sortie,

673
00:41:07,380 --> 00:41:10,670
le New York Times
en a fait la couverture

674
00:41:10,840 --> 00:41:12,300
de la section "économie".

675
00:41:12,470 --> 00:41:14,090
Elle·Il·s l'ont intitulée :

676
00:41:14,260 --> 00:41:16,680
"Un sujet d'inquiétude démasqué."

677
00:41:16,850 --> 00:41:17,760
Le sous-titre :

678
00:41:18,560 --> 00:41:22,060
"La technologie d'Amazon
pour analyser les visages

679
00:41:22,230 --> 00:41:24,610
"pourrait être biaisée,
selon une nouvelle étude,

680
00:41:24,770 --> 00:41:27,440
"mais l'entreprise veut la développer
malgré tout."

681
00:41:27,940 --> 00:41:31,950
Voilà ce qui attendait Jeff Bezos

682
00:41:27,940 --> 00:41:31,950
Voilà ce qui attendait Jeff Bezos

683
00:41:32,110 --> 00:41:34,490
lorsqu'il a ouvert le journal.

684
00:41:36,410 --> 00:41:38,490
Les gens demandaient
comment je t'ai trouvée.

685
00:41:38,660 --> 00:41:42,370
Tu étais la seule personne
à parler de ce problème.

686
00:41:42,540 --> 00:41:44,290
J'y avais été confrontée.

687
00:41:44,460 --> 00:41:48,460
J'ai pas pu utiliser de logiciels libres
de reconnaissance faciale.

688
00:41:49,050 --> 00:41:49,960
Je me suis dit :

689
00:41:50,130 --> 00:41:52,760
"Enfin quelqu'un·e
qui a identifié le problème

690
00:41:52,930 --> 00:41:55,050
"et qui l'examine
de manière académique."

691
00:42:02,230 --> 00:42:06,100
L'autrice principale de l'étude,
que je supervise,

692
00:42:06,270 --> 00:42:10,150
est une étudiante au premier cycle
à l'Université de Toronto.

693
00:42:10,320 --> 00:42:12,530
Je l'appelle Agente Deb.

694
00:42:13,070 --> 00:42:16,370
Nous dirigeons cette étude ensemble.

695
00:42:31,510 --> 00:42:33,420
Après l'article du New York Times,

696
00:42:33,590 --> 00:42:36,930
plus de 400 articles ont été écrits

697
00:42:37,090 --> 00:42:38,680
au sujet de notre étude.

698
00:42:39,720 --> 00:42:42,020
LA RECONNAISSANCE FACIALE D'AMAZON
EST BIAISÉE

699
00:42:42,180 --> 00:42:45,690
LES EXPERT·E·S DE L'IA S'ATTAQUENT À AMAZON
POUR SON BIAIS RACIAL DÉVOILÉ

700
00:42:45,850 --> 00:42:48,610
RECONNAÎTRE LE BIAIS
DE L'INTELLIGENCE ARTIFICIELLE

701
00:42:49,940 --> 00:42:53,530
Amazon a été attaqué pour avoir utilisé

702
00:42:53,690 --> 00:42:57,030
Amazon Rekognition
auprès des forces de police.

703
00:42:57,200 --> 00:43:00,740
Elle·Il·s collaborent également
avec les agences de renseignements.

704
00:42:57,200 --> 00:43:00,740
Elle·Il·s collaborent également
avec les agences de renseignements.

705
00:43:00,910 --> 00:43:05,120
Amazon teste sa technologie d'IA
avec le FBI.

706
00:43:05,290 --> 00:43:07,000
Elle·Il·s ont beaucoup en jeu.

707
00:43:07,170 --> 00:43:09,630
Si elle·il·s ont sciemment vendu

708
00:43:09,790 --> 00:43:13,710
des systèmes présentant
des biais sexistes et racistes

709
00:43:13,880 --> 00:43:15,760
elle·il·s seraient dans le pétrin.

710
00:43:18,550 --> 00:43:22,470
Un jour ou deux après la publication
de l'article du New York Times,

711
00:43:22,640 --> 00:43:25,850
Amazon a écrit un billet de blog

712
00:43:26,020 --> 00:43:28,980
affirmant que notre étude
a tiré de fausses conclusions

713
00:43:29,150 --> 00:43:31,730
et tentant de nous discréditer
par tous les moyens.

714
00:43:29,150 --> 00:43:31,730
et tentant de nous discréditer
par tous les moyens.

715
00:43:33,900 --> 00:43:38,360
Un vice-président d'Amazon
cherchant à discréditer notre travail

716
00:43:38,530 --> 00:43:41,700
a écrit : "L'analyse
et la reconnaissance faciale

717
00:43:41,870 --> 00:43:44,120
"sont complètement différentes

718
00:43:44,290 --> 00:43:47,620
"en termes de technologie
et de données pour les entraîner."

719
00:43:47,790 --> 00:43:51,380
Cette affirmation,
si on étudie ce domaine,

720
00:43:51,960 --> 00:43:54,760
n'a aucun sens, pas vrai ?

721
00:43:55,380 --> 00:43:58,090
Ce n'est même pas une critique éclairée.

722
00:43:58,260 --> 00:44:00,260
Si on veut discréditer quelqu'un·e...

723
00:43:58,260 --> 00:44:00,260
Si on veut discréditer quelqu'un·e...

724
00:44:00,430 --> 00:44:01,680
Il a écrit :

725
00:44:02,100 --> 00:44:04,390
"La vision par ordinateur
est un type d'apprentissage automatique."

726
00:44:04,560 --> 00:44:07,270
Non, mon gars, c'est pas du tout ça.

727
00:44:07,430 --> 00:44:09,650
C'est ce que j'allais dire !

728
00:44:09,810 --> 00:44:12,610
C'était une affirmation
grossièrement fausse.

729
00:44:12,770 --> 00:44:14,150
C'était pas réfléchi.

730
00:44:14,320 --> 00:44:16,860
C'est frustrant,
parce que c'est vraiment...

731
00:44:17,030 --> 00:44:21,070
grâce à sa position qu'il savait
qu'il serait pris au sérieux

732
00:44:21,240 --> 00:44:25,950
Je ne sais pas si vous ressentez ça,
mais on me sous-estime tellement !

733
00:44:29,580 --> 00:44:30,830
Ce n'est pas nouveau,

734
00:44:29,580 --> 00:44:30,830
Ce n'est pas nouveau,

735
00:44:31,000 --> 00:44:33,250
c'est une continuation de mon expérience

736
00:44:33,420 --> 00:44:37,210
en tant que femme de couleur
dans la Tech.

737
00:44:37,380 --> 00:44:39,590
Attendez-vous à être discréditée,

738
00:44:39,760 --> 00:44:42,300
à voir vos recherches dédaignées.

739
00:44:50,480 --> 00:44:54,400
Si l'on réfléchit à qui finance
la recherche dans l'IA ?

740
00:44:54,570 --> 00:44:55,730
Les géant·e·s de la Tech.

741
00:44:56,280 --> 00:45:01,240
Si notre travail les remet en question
ou leur donne une mauvaise image,

742
00:44:56,280 --> 00:45:01,240
Si notre travail les remet en question
ou leur donne une mauvaise image,

743
00:45:02,160 --> 00:45:06,200
on risque de ne pas avoir d'opportunités
dans le futur.

744
00:45:13,420 --> 00:45:14,880
Pour moi,

745
00:45:15,250 --> 00:45:18,380
c'était déconcertant,
mais ça montrait notre pouvoir,

746
00:45:18,550 --> 00:45:23,470
si l'on inquiète l'une des plus grandes
entreprises du monde.

747
00:45:29,310 --> 00:45:32,390
La réaction d'Amazon
démontre précisément

748
00:45:29,310 --> 00:45:32,390
La réaction d'Amazon
démontre précisément

749
00:45:32,560 --> 00:45:35,560
pourquoi on ne peut plus vivre
dans un pays

750
00:45:35,730 --> 00:45:39,820
sans règlementations fédérales encadrant
la technologie d'analyse faciale,

751
00:45:39,990 --> 00:45:42,360
ou de reconnaissance faciale.

752
00:45:57,750 --> 00:46:00,260
Quand j'avais 14 ans,
en camp d'été en maths,

753
00:45:57,750 --> 00:46:00,260
Quand j'avais 14 ans,
en camp d'été en maths,

754
00:46:00,420 --> 00:46:02,010
j'ai appris à résoudre le cube Rubik

755
00:46:02,510 --> 00:46:04,430
J'ai pensé : "C'est trop cool !"

756
00:46:05,220 --> 00:46:07,390
Pour un·e intello,

757
00:46:07,800 --> 00:46:12,270
un truc pour lequel on est doué·e,
et qui ne présente pas d'ambiguïté,

758
00:46:13,100 --> 00:46:15,650
c'est quelque chose de magique.

759
00:46:16,520 --> 00:46:19,190
Mon professeur de math en 5e m'a dit :

760
00:46:19,360 --> 00:46:21,940
"Il est inutile pour toi
et les deux autres filles

761
00:46:22,110 --> 00:46:24,860
"qui ont été admises
au cours d'algèbre avancée

762
00:46:25,030 --> 00:46:28,620
"de suivre ce cours, car les filles
n'ont pas besoin des maths."

763
00:46:33,370 --> 00:46:35,000
Quand on est un peu marginal·e,

764
00:46:35,580 --> 00:46:38,130
on a toujours
le point de vue de l'opprimé·e.

765
00:46:40,880 --> 00:46:41,880
En 2006,

766
00:46:42,510 --> 00:46:45,260
on m'a donné un poste
en fonds spéculatifs,

767
00:46:45,430 --> 00:46:47,760
car je pouvais
résoudre des énigmes de maths.

768
00:46:47,930 --> 00:46:48,890
C'est fou,

769
00:46:49,050 --> 00:46:53,180
car je ne connaissais rien aux finances,
à la programmation, ou aux marchés.

770
00:46:56,480 --> 00:46:58,810
Quand j'ai commencé,
j'y ai un peu cru.

771
00:46:59,730 --> 00:47:01,610
Je ne savais pas encore à ce moment-là

772
00:46:59,730 --> 00:47:01,610
Je ne savais pas encore à ce moment-là

773
00:47:01,780 --> 00:47:04,900
que les modèles de risques
étaient conçus pour être mauvais.

774
00:47:07,870 --> 00:47:10,780
On a découvert l'impact algorithmique

775
00:47:10,950 --> 00:47:13,000
en observant ses effets.

776
00:47:15,120 --> 00:47:16,710
Par exemple,

777
00:47:16,870 --> 00:47:20,210
on mise contre les Américain·e·s

778
00:47:20,380 --> 00:47:25,220
et on les sélectionne et les optimise
pour qu'ils échouent.

779
00:47:26,680 --> 00:47:29,090
C'est chercher un profil particulier

780
00:47:29,260 --> 00:47:32,060
de personnes qui vont prendre
des prêts à risques

781
00:47:29,260 --> 00:47:32,060
de personnes qui vont prendre
des prêts à risques

782
00:47:32,220 --> 00:47:34,390
et miser sur leur échec,

783
00:47:34,560 --> 00:47:38,150
puis saisir l'ensemble de leurs biens.

784
00:47:38,770 --> 00:47:42,440
C'était un jeu algorithmique
qui a vu le jour à Wall Street.

785
00:47:42,610 --> 00:47:45,650
PRÈS DE 4 MILLIONS D'AMÉRICAIN·E·S
ONT PERDU LEUR LOGEMENT EN 2008

786
00:47:45,820 --> 00:47:48,200
Durant la crise des prêts hypothécaires,

787
00:47:48,360 --> 00:47:53,080
on a vu la plus grave perte de richesse
des Noir·e·s que le pays ait connu.

788
00:47:53,700 --> 00:47:54,830
Comme ça.

789
00:47:57,210 --> 00:47:59,960
C'est ce que j'entends par
"oppression algorithmique".

790
00:48:00,130 --> 00:48:01,460
La tyrannie

791
00:48:01,630 --> 00:48:05,670
de ce genre
de pratiques discriminatoires

792
00:48:05,840 --> 00:48:07,630
est devenue opaque.

793
00:48:11,350 --> 00:48:12,680
Tout le monde a souffert

794
00:48:13,470 --> 00:48:16,100
à cause de l'échec
du système financier.

795
00:48:18,060 --> 00:48:19,690
Au bout de deux ans, je me disais :

796
00:48:19,850 --> 00:48:22,650
"Non, on veut juste
se faire beaucoup d'argent."

797
00:48:23,230 --> 00:48:24,780
Et j'y ai participé.

798
00:48:26,070 --> 00:48:27,740
Et j'ai décidé de partir.

799
00:48:28,450 --> 00:48:31,990
C'est 15 fois 3,
et ça, 15 fois...

800
00:48:28,450 --> 00:48:31,990
C'est 15 fois 3,
et ça, 15 fois...

801
00:48:35,200 --> 00:48:36,250
7.

802
00:48:37,960 --> 00:48:39,830
Rappelle-toi, 7 et 3.

803
00:48:40,920 --> 00:48:42,630
Ce sont les personnes puissantes

804
00:48:42,960 --> 00:48:45,250
qui saignent les impuissant·e·s.

805
00:48:54,220 --> 00:48:56,770
Je suis une gardienne invisible.

806
00:48:57,140 --> 00:48:59,690
J'emploie les données
pour automatiser les décisions

807
00:48:59,850 --> 00:49:02,690
concernant qui sera embauché·e ou licencié·e

808
00:48:59,850 --> 00:49:02,690
concernant qui sera embauché·e ou licencié·e

809
00:49:02,860 --> 00:49:05,400
et le coût de votre assurance.

810
00:49:05,860 --> 00:49:09,820
Parfois, vous ignorez que j'ai pris
ces décisions automatiques.

811
00:49:10,110 --> 00:49:11,490
J'ai de nombreux noms.

812
00:49:11,660 --> 00:49:14,160
On m'appelle modèle mathématique,

813
00:49:14,320 --> 00:49:17,410
évaluation, estimation, outil.

814
00:49:18,080 --> 00:49:21,080
Mais dans de nombreux noms,
je suis un algorithme.

815
00:49:21,250 --> 00:49:23,460
Je suis une boîte noire.

816
00:49:32,430 --> 00:49:36,220
Le modèle de valeur ajoutée des profs
était utilisé dans la moitié du pays.

817
00:49:36,390 --> 00:49:38,390
En particulier à New York City.

818
00:49:38,560 --> 00:49:43,100
Je l'ai su car une amie proche,
qui est principale d'un lycée à NYC,

819
00:49:43,270 --> 00:49:44,480
a vu ses profs évalué·e·s.

820
00:49:44,650 --> 00:49:47,020
C'est ma meilleure amie de fac.

821
00:49:47,190 --> 00:49:48,230
- C'est Cathy.
- Salut.

822
00:49:48,400 --> 00:49:52,030
On s'est connues à 18 ans,
donc 2 ans de plus que vous.

823
00:49:52,200 --> 00:49:53,910
Incroyable !

824
00:49:56,070 --> 00:49:59,950
Et leurs scores, selon des algorithmes
qu'elle·il·s ne comprenaient pas,

825
00:50:00,120 --> 00:50:03,500
allaient largement influencer
leur titularisation.

826
00:50:03,670 --> 00:50:05,880
- Où êtes-vous censé·e·s être ?
- En classe.

827
00:50:06,040 --> 00:50:07,460
Oui, mais laquelle ?

828
00:50:07,630 --> 00:50:09,840
Si encore l'algorithme avait été bon...

829
00:50:10,010 --> 00:50:13,470
C'était supérieur à l'aléatoire,
mais de très peu.

830
00:50:13,630 --> 00:50:14,680
C'est insuffisant !

831
00:50:14,840 --> 00:50:18,640
Surtout quand il s'agit
de titulariser un·e professeur·e.

832
00:50:19,100 --> 00:50:23,480
J'ai découvert qu'un système similaire
était utilisé à Houston

833
00:50:23,640 --> 00:50:25,440
pour licencier les profs.

834
00:50:26,360 --> 00:50:27,360
HOUSTON, TEXAS

835
00:50:27,520 --> 00:50:29,320
C'est un modèle de valeur ajoutée.

836
00:50:29,480 --> 00:50:32,490
Il calcule
la valeur ajoutée d'un·e professeur·e.

837
00:50:29,480 --> 00:50:32,490
Il calcule
la valeur ajoutée d'un·e professeur·e.

838
00:50:32,650 --> 00:50:34,280
Certaines parties sont dissimulées

839
00:50:34,450 --> 00:50:36,740
par l'entreprise qui l'a créé.

840
00:50:45,330 --> 00:50:46,330
J'étais prof de l'année.

841
00:50:46,500 --> 00:50:48,420
10 ans plus tard,

842
00:50:48,590 --> 00:50:52,050
j'ai reçu cette récompense à nouveau.

843
00:50:52,210 --> 00:50:54,090
J'ai été nommé prof du mois.

844
00:50:54,260 --> 00:50:57,640
J'ai été récompensé
pour avoir fait du bénévolat.

845
00:50:57,800 --> 00:51:01,600
J'ai aussi reçu un prix
pour être allé au-delà de mon devoir.

846
00:50:57,800 --> 00:51:01,600
J'ai aussi reçu un prix
pour être allé au-delà de mon devoir.

847
00:51:02,270 --> 00:51:05,730
J'ai un dossier
contenant toutes les évaluations

848
00:51:06,060 --> 00:51:09,190
et chaque directeur·trice
ou évaluateur·trice a noté :

849
00:51:09,360 --> 00:51:11,980
"Excellent, dépasse les attentes."

850
00:51:12,650 --> 00:51:15,320
L'ordinateur
a fondamentalement réfuté

851
00:51:16,110 --> 00:51:18,910
les preuves
observées par l'administration.

852
00:51:19,070 --> 00:51:21,160
Selon cet algorithme,

853
00:51:21,330 --> 00:51:24,910
j'ai été qualifié de mauvais enseignant.

854
00:51:29,920 --> 00:51:32,210
Certain·e·s enseignant·e·s ont été licencié·e·s.

855
00:51:29,920 --> 00:51:32,210
Certain·e·s enseignant·e·s ont été licencié·e·s.

856
00:51:32,960 --> 00:51:34,720
Certain·e·s avaient été ciblé·e·s

857
00:51:34,880 --> 00:51:36,880
à cause de l'algorithme.

858
00:51:37,050 --> 00:51:39,970
J'étais tellement démoralisé

859
00:51:40,470 --> 00:51:43,890
que pendant un moment,
j'en suis arrivé à douter de moi.

860
00:51:46,270 --> 00:51:48,310
Puis j'ai eu une révélation.

861
00:51:48,520 --> 00:51:50,940
Cet algorithme est un mensonge !

862
00:51:51,610 --> 00:51:53,690
Comment peut-il me définir ?

863
00:51:53,860 --> 00:51:55,490
Comment ose-t-il ?

864
00:51:55,650 --> 00:51:58,700
C'est là que j'ai commencé à enquêter
et à avancer.

865
00:51:59,990 --> 00:52:01,990
Nous annonçons

866
00:51:59,990 --> 00:52:01,990
Nous annonçons

867
00:52:02,160 --> 00:52:05,700
qu'hier soir, nous avons déposé
une plainte à la cour fédérale

868
00:52:05,870 --> 00:52:08,540
contre l'évaluation actuelle de l'HISD.

869
00:52:10,000 --> 00:52:13,750
La fédération des professeur·e·s de Houston
s'est penchée sur le procès.

870
00:52:13,920 --> 00:52:17,170
Si ça peut arriver à M. Santos,
au collège de Jackson,

871
00:52:17,340 --> 00:52:19,890
combien d'autres ont été diffamé·e·s ?

872
00:52:20,180 --> 00:52:23,640
Nous avons justifié notre plainte
par le 14e amendement.

873
00:52:23,930 --> 00:52:25,350
Ce n'est pas équitable.

874
00:52:25,520 --> 00:52:28,140
Comment peut-on tirer une conclusion

875
00:52:28,310 --> 00:52:30,730
sans m'expliquer le processus ?

876
00:52:28,310 --> 00:52:30,730
sans m'expliquer le processus ?

877
00:52:33,150 --> 00:52:34,440
La lutte n'est pas finie.

878
00:52:35,150 --> 00:52:39,400
Il y a toujours des districts scolaires

879
00:52:39,570 --> 00:52:41,370
qui utilisent ce modèle,

880
00:52:41,530 --> 00:52:44,120
mais il y a de l'espoir,

881
00:52:44,280 --> 00:52:45,790
car je suis toujours là.

882
00:52:46,120 --> 00:52:48,210
Il y a donc de l'espoir.

883
00:52:49,960 --> 00:52:52,960
Democracia, la democracia,

884
00:52:53,130 --> 00:52:55,050
ou en anglais...

885
00:52:55,800 --> 00:52:57,970
La démocratie.
Qui a le pouvoir ?

886
00:52:59,380 --> 00:53:01,550
- Nous ?
- Oui, le peuple.

887
00:52:59,380 --> 00:53:01,550
- Nous ?
- Oui, le peuple.

888
00:53:02,090 --> 00:53:04,970
Le juge a dit que leur droit
à un procès équitable a été violé.

889
00:53:05,140 --> 00:53:06,430
Elle·Il·s ont été viré·e·s

890
00:53:06,600 --> 00:53:09,140
pour une raison
que personne ne comprend.

891
00:53:09,310 --> 00:53:12,900
Elle·Il·s méritent de comprendre
pourquoi elle·il·s ont été viré·e·s.

892
00:53:13,060 --> 00:53:14,860
Mais j'ignore pourquoi

893
00:53:15,020 --> 00:53:18,650
cette décision juridique ne s'étend pas
à d'autres types d'algorithmes.

894
00:53:18,820 --> 00:53:20,990
Pourquoi ne pas employer
ce même argument,

895
00:53:21,150 --> 00:53:23,700
le droit constitutionnel
à un procès équitable,

896
00:53:23,870 --> 00:53:27,830
pour lutter contre d'autres algorithmes
qui nous sont invisibles,

897
00:53:28,000 --> 00:53:32,250
des boîtes noires inexpliquées,
mais qui ont de l'importance.

898
00:53:28,000 --> 00:53:32,250
des boîtes noires inexpliquées,
mais qui ont de l'importance.

899
00:53:32,420 --> 00:53:35,960
Qui nous retirent des opportunités
essentielles dans nos vies.

900
00:53:37,550 --> 00:53:41,380
Parfois, je fais des erreurs
sans qu'on me remette en question.

901
00:53:41,720 --> 00:53:44,220
Ces erreurs ne sont pas de ma faute.

902
00:53:45,390 --> 00:53:48,060
J'ai été optimisée pour être efficace.

903
00:53:49,810 --> 00:53:53,520
Il n'existe pas d'algorithme
pour définir ce qui est juste.

904
00:53:56,520 --> 00:53:59,990
Un comité d'État a approuvé
un nouvel outil d'évaluation du risque

905
00:54:00,150 --> 00:54:02,860
qui aidera les juges de Pennsylvanie
à définir les peines.

906
00:54:03,030 --> 00:54:06,740
L'instrument utilise un algorithme
qui détermine le risque de récidive,

907
00:54:07,280 --> 00:54:11,210
en fonction de l'âge, du sexe,
des antécédents criminels de la personne.

908
00:54:12,710 --> 00:54:15,380
Ce qui m'a vraiment tourmentée,

909
00:54:15,540 --> 00:54:18,880
c'était les algorithmes
de risques de récidive,

910
00:54:19,050 --> 00:54:22,420
qu'on donne aux juges
pour définir les peines de prison.

911
00:54:22,590 --> 00:54:23,760
Est-ce juste ?

912
00:54:24,180 --> 00:54:27,510
Comment ces systèmes d'évaluation
sont-ils conçus ?

913
00:54:27,680 --> 00:54:29,770
Comment sont produits les scores ?

914
00:54:29,930 --> 00:54:33,190
Les questions sont des variables
de race et de classe sociale.

915
00:54:29,930 --> 00:54:33,190
Les questions sont des variables
de race et de classe sociale.

916
00:54:33,560 --> 00:54:35,060
LE BIAIS DES MACHINES

917
00:54:35,230 --> 00:54:37,310
ProPublica a publié une enquête

918
00:54:37,480 --> 00:54:39,440
sur les logiciels
d'évaluation des risques.

919
00:54:39,610 --> 00:54:42,240
révélant un biais racial
des algorithmes.

920
00:54:42,690 --> 00:54:46,200
Selon l'étude, on donnait par erreur
des scores élevés aux Noir·e·s,

921
00:54:46,370 --> 00:54:50,620
et on donnait plus souvent à tort
un score faible aux Blanc·che·s.

922
00:54:52,040 --> 00:54:54,670
PHILADELPHIE, PENNSYLVANIE

923
00:55:00,670 --> 00:55:04,300
PROFIL DE LA PERSONNE
EN LIBERTÉ CONDITIONNELLE

924
00:55:05,430 --> 00:55:07,930
RISQUE ÉLEVÉ
CONTRÔLES HEBDOMADAIRES REQUIS

925
00:55:09,010 --> 00:55:10,890
Je vais au bureau de probation,

926
00:55:11,060 --> 00:55:15,190
et elle me dit que je dois me présenter
une fois par semaine.

927
00:55:15,350 --> 00:55:18,270
Je lui dis : "Vous avez vu
tout ce que j'ai accompli ?"

928
00:55:18,440 --> 00:55:20,150
Je suis à la maison
depuis 4 ans.

929
00:55:20,320 --> 00:55:23,280
J'ai un emploi à plein temps.
J'ai été citée deux fois.

930
00:55:23,440 --> 00:55:26,780
Par la mairie de Philadelphia,
et par le maire...

931
00:55:26,950 --> 00:55:31,540
Vous allez vraiment me demander
de venir chaque semaine ? Pourquoi ?

932
00:55:26,950 --> 00:55:31,540
Vous allez vraiment me demander
de venir chaque semaine ? Pourquoi ?

933
00:55:31,700 --> 00:55:34,200
Je ne mérite pas
d'être qualifiée "à risque élevé".

934
00:55:34,660 --> 00:55:36,580
J'ai eu une réunion
avec leur service.

935
00:55:36,750 --> 00:55:40,880
Elle·Il·s ont mentionné
qu'elle·il·s avaient un algorithme

936
00:55:41,590 --> 00:55:44,510
qui classait les gens
par risque élevé, moyen et faible.

937
00:55:44,800 --> 00:55:49,090
J'ai compris que l'algorithme a décidé
de votre niveau de risque.

938
00:55:49,260 --> 00:55:52,390
Je suis donc retournée
dire à ma conseillère de probation :

939
00:55:52,560 --> 00:55:57,310
"Alors vous ne pouvez rien inclure
de tout ce que j'ai fait de positif,

940
00:55:57,480 --> 00:56:00,940
"pour contrecarrer
les résultats de l'algorithme ?"

941
00:55:57,480 --> 00:56:00,940
"pour contrecarrer
les résultats de l'algorithme ?"

942
00:56:01,650 --> 00:56:03,650
Elle m'a dit : "C'est impossible."

943
00:56:03,820 --> 00:56:05,360
Cet ordinateur a prévalu

944
00:56:05,690 --> 00:56:09,070
sur le discernement
du juge et de la conseillère.

945
00:56:09,240 --> 00:56:11,580
En vous qualifiant à risque élevé

946
00:56:11,990 --> 00:56:14,290
et en exigeant
que vous veniez sur place,

947
00:56:14,450 --> 00:56:17,330
vous auriez pu perdre votre travail,
et devenir un risque.

948
00:56:17,830 --> 00:56:19,330
C'est ce qui me blesse.

949
00:56:19,500 --> 00:56:22,040
Savoir
que malgré tout ce que j'ai accompli,

950
00:56:22,210 --> 00:56:24,380
on me voit toujours comme un risque.

951
00:56:24,800 --> 00:56:27,220
J'ai l'impression
que ce que je fais est inutile.

952
00:56:37,520 --> 00:56:40,230
Qu'est-ce que ça signifie
si personne ne défend

953
00:56:40,400 --> 00:56:45,110
celles·ceux qui ne savent pas
ce que fait la technologie ?

954
00:56:46,030 --> 00:56:47,740
J'ai pris conscience

955
00:56:47,900 --> 00:56:50,910
que cela dépasse mon projet artistique

956
00:56:51,070 --> 00:56:53,200
qui ne détecte pas mon visage,

957
00:56:53,370 --> 00:56:56,250
il s'agit de systèmes
qui influencent nos vies

958
00:56:56,410 --> 00:56:58,000
de manière concrète.

959
00:57:05,460 --> 00:57:08,220
J'ai donc créé
la Ligue de la Justice Algorithmique.

960
00:57:09,010 --> 00:57:11,680
Je voulais créer un espace et un lieu

961
00:57:11,840 --> 00:57:16,470
où les gens pourraient s'informer
sur les implications sociales de l'IA.

962
00:57:18,350 --> 00:57:21,980
Nous avons tous un enjeu,
cela nous affecte tou·te·s.

963
00:57:26,690 --> 00:57:29,280
La Ligue de la Justice Algorithmique

964
00:57:29,440 --> 00:57:33,570
est un mouvement, un concept,
un groupe de personnes qui souhaitent

965
00:57:29,440 --> 00:57:33,570
est un mouvement, un concept,
un groupe de personnes qui souhaitent

966
00:57:33,740 --> 00:57:35,490
créer un avenir

967
00:57:35,660 --> 00:57:39,120
où les technologies sociales
bénéficient à tou·te·s.

968
00:57:43,670 --> 00:57:45,750
Cela demande un travail d'équipe.

969
00:57:45,920 --> 00:57:49,460
Les gens doivent s'unir
et lutter pour la justice,

970
00:57:49,630 --> 00:57:52,010
pour l'équité et l'égalité,

971
00:57:52,180 --> 00:57:54,850
à l'ère de l'automatisation.

972
00:57:56,760 --> 00:58:00,980
La prochaine montagne à gravir
devrait être les ressources humaines.

973
00:57:56,760 --> 00:58:00,980
La prochaine montagne à gravir
devrait être les ressources humaines.

974
00:58:01,440 --> 00:58:02,520
Tout à fait.

975
00:58:02,690 --> 00:58:06,020
Il y a un problème
avec les algorithmes d'analyse des CV.

976
00:58:06,860 --> 00:58:09,650
Toutes ces plateformes
de mise en relation :

977
00:58:09,820 --> 00:58:12,450
"Vous cherchez un travail ?
Vous embauchez ?"

978
00:58:12,610 --> 00:58:14,620
Elles vont connecter ces personnes.

979
00:58:14,780 --> 00:58:16,580
Comment ça fonctionne ?

980
00:58:16,740 --> 00:58:18,910
Quand on parle de l'avenir du travail,

981
00:58:19,080 --> 00:58:22,460
on parle d'automatisation
sans parler de réglementation.

982
00:58:22,620 --> 00:58:25,960
- Qui aura les postes encore existants ?
- Exactement.

983
00:58:26,130 --> 00:58:28,300
On ne parle pas autant de ça.

984
00:58:28,460 --> 00:58:29,710
Bien résumé.

985
00:58:29,960 --> 00:58:33,930
J'aimerais voir 3 audiences du Congrès
à ce sujet l'année prochaine

986
00:58:29,960 --> 00:58:33,930
J'aimerais voir 3 audiences du Congrès
à ce sujet l'année prochaine

987
00:58:35,140 --> 00:58:37,800
À davantage de pouvoir !

988
00:58:37,970 --> 00:58:40,060
Et à davantage d'éthique !

989
00:58:40,220 --> 00:58:42,310
- Oui !
- Santé !

990
00:58:46,400 --> 00:58:51,030
CAPE TOWN, AFRIQUE DU SUD

991
00:58:56,950 --> 00:59:00,160
ARRIVÉES INTERNATIONALES

992
00:58:56,950 --> 00:59:00,160
ARRIVÉES INTERNATIONALES

993
00:59:01,290 --> 00:59:03,710
LA LIGUE DE LA JUSTICE ALGORITHMIQUE

994
00:59:07,080 --> 00:59:11,000
Le discours de cette séance plénière
sera donné par Joy Buolamwini.

995
00:59:11,170 --> 00:59:15,180
Elle parlera des dangers du manque de diversité
des données pour la reconnaissance faciale.

996
00:59:15,760 --> 00:59:17,470
Veuillez accueillir Joy.

997
00:59:24,600 --> 00:59:26,650
L'IA n'est pas sans faille.

998
00:59:26,810 --> 00:59:28,520
Quelle est la précision

999
00:59:28,690 --> 00:59:31,530
des systèmes d'IBM,
Microsoft et Face++ ?

1000
00:59:28,690 --> 00:59:31,530
des systèmes d'IBM,
Microsoft et Face++ ?

1001
00:59:31,690 --> 00:59:35,070
L'efficacité est sans faille
pour un groupe.

1002
00:59:35,400 --> 00:59:39,740
Les hommes au teint pâle
sont les mieux reconnus, aucun problème.

1003
00:59:40,410 --> 00:59:44,790
J'ai envoyé mon analyse aux entreprises
pour voir ce qu'elles en pensaient.

1004
00:59:45,710 --> 00:59:48,460
IBM m'a invitée à leur siège.

1005
00:59:48,630 --> 00:59:51,460
Elle·Il·s ont répliqué les résultats
en interne,

1006
00:59:51,630 --> 00:59:54,380
et elle·il·s ont amélioré le système.

1007
00:59:54,760 --> 00:59:58,760
Le jour où j'ai officiellement présenté
les résultats de ma recherche,

1008
00:59:58,930 --> 01:00:01,140
on peut voir que dans ce cas,

1009
00:59:58,930 --> 01:00:01,140
on peut voir que dans ce cas,

1010
01:00:01,310 --> 01:00:03,770
on a désormais une efficacité de 100%

1011
01:00:03,930 --> 01:00:06,100
pour les femmes au teint pâle.

1012
01:00:06,270 --> 01:00:08,060
Et pour les femmes de couleur,

1013
01:00:08,230 --> 01:00:09,770
il y a une amélioration.

1014
01:00:10,190 --> 01:00:11,440
On me dit souvent :

1015
01:00:11,610 --> 01:00:16,450
"Si les systèmes ne t'ont pas détectée,
c'est pas parce que tu es mélaninée ?"

1016
01:00:16,610 --> 01:00:20,820
Et oui,
je suis très "mélaninée", mais...

1017
01:00:22,200 --> 01:00:24,200
les lois de la physique
n'ont pas changé.

1018
01:00:24,370 --> 01:00:26,330
Mais en faire une priorité

1019
01:00:26,500 --> 01:00:29,630
et reconnaître nos différences

1020
01:00:29,790 --> 01:00:33,500
a permis de créer
un système plus inclusif.

1021
01:00:29,790 --> 01:00:33,500
a permis de créer
un système plus inclusif.

1022
01:00:38,380 --> 01:00:42,350
Quel est l'objectif
de l'identification ?

1023
01:00:42,510 --> 01:00:45,180
C'est le contrôle des déplacements.

1024
01:00:45,350 --> 01:00:48,600
Les gens ne pouvaient pas aller
dans certaines zones, la nuit tombée.

1025
01:00:49,310 --> 01:00:52,810
On pouvait toujours se faire arrêter
par la police, arbitrairement.

1026
01:00:52,980 --> 01:00:54,980
En fonction de notre apparence,

1027
01:00:55,150 --> 01:00:57,820
elle·il·s pouvaient demander notre passeport.

1028
01:00:59,570 --> 01:01:02,280
Au lieu d'avoir
les informations des pièces d'identité,

1029
01:00:59,570 --> 01:01:02,280
Au lieu d'avoir
les informations des pièces d'identité,

1030
01:01:02,450 --> 01:01:06,330
on a maintenant des ordinateurs
qui vont analyser un visage

1031
01:01:06,500 --> 01:01:08,460
et déterminer notre genre,

1032
01:01:08,620 --> 01:01:12,130
certains vont tenter de déterminer
notre origine ethnique.

1033
01:01:12,290 --> 01:01:13,790
Et selon mes recherches,

1034
01:01:13,960 --> 01:01:17,880
même pour les systèmes de classification
que certain·e·s approuvent,

1035
01:01:18,050 --> 01:01:19,420
ils font des erreurs.

1036
01:01:19,590 --> 01:01:23,010
Cela ne s'arrête pas
à la catégorisation des visages,

1037
01:01:23,180 --> 01:01:25,600
ça vaut
pour toute technologie des données.

1038
01:01:26,010 --> 01:01:29,560
Les gens supposent que la machine
ne fait pas d'erreur, mais c'est faux.

1039
01:01:29,730 --> 01:01:33,860
Les humain·e·s se recréent
à leur image et avec leurs attributs.

1040
01:01:29,730 --> 01:01:33,860
Les humain·e·s se recréent
à leur image et avec leurs attributs.

1041
01:01:34,020 --> 01:01:34,820
Tout à fait.

1042
01:01:34,980 --> 01:01:37,110
Le racisme est en train
d'être automatisé.

1043
01:01:37,280 --> 01:01:39,110
- Robotisé.
- Exactement.

1044
01:01:52,170 --> 01:01:54,420
La précision attire l'attention,

1045
01:01:54,590 --> 01:01:57,130
mais on ne doit pas oublier l'abus.

1046
01:01:59,670 --> 01:02:02,840
Même si je suis parfaitement classifiée,
cela ouvre la voie

1047
01:01:59,670 --> 01:02:02,840
Même si je suis parfaitement classifiée,
cela ouvre la voie

1048
01:02:03,390 --> 01:02:04,680
à la surveillance.

1049
01:02:14,610 --> 01:02:17,820
HANGZHOU, CHINE

1050
01:02:24,950 --> 01:02:28,240
Il y a ce qu'on appelle
une note de crédit social en Chine.

1051
01:02:28,410 --> 01:02:30,500
Elle·Il·s disent de manière explicite :

1052
01:02:28,410 --> 01:02:30,500
Elle·Il·s disent de manière explicite :

1053
01:02:30,660 --> 01:02:33,870
"Voilà le topo, citoyen·ne·s chinois·e·s,
on vous surveille,

1054
01:02:34,080 --> 01:02:35,920
"vous avez une note de crédit social

1055
01:02:36,080 --> 01:02:39,340
"ce que vous direz
contre le parti communiste l'affectera.

1056
01:02:40,170 --> 01:02:44,380
"Cela affectera également la note
de vos ami·e·s et de votre famille."

1057
01:02:44,840 --> 01:02:48,180
C'est explicite,
le gouvernement qui l'a créé dit :

1058
01:02:48,350 --> 01:02:51,230
"Vous êtes surveillé·e·s,
comportez-vous en conséquence."

1059
01:02:51,560 --> 01:02:53,850
C'est un entraînement à la soumisson
algorithmique.

1060
01:03:12,080 --> 01:03:15,790
J'utilise la reconnaissance faciale
dans divers aspects de ma vie.

1061
01:03:15,960 --> 01:03:17,790
Quand je fais mes courses,

1062
01:03:17,960 --> 01:03:21,630
quand j'entre dans mon immeuble,
dans la station de métro.

1063
01:03:24,680 --> 01:03:27,930
Maintenant,
il suffit de scanner notre visage.

1064
01:03:29,050 --> 01:03:30,600
C'est très pratique.

1065
01:03:29,050 --> 01:03:30,600
C'est très pratique.

1066
01:03:39,310 --> 01:03:41,730
Avec un système de crédit social,

1067
01:03:41,900 --> 01:03:45,150
je pense que les gens
vont mieux se comporter.

1068
01:03:46,360 --> 01:03:48,660
Aujourd'hui, dans le train,

1069
01:03:48,820 --> 01:03:51,490
elle·il·s ont diffusé une annonce disant :

1070
01:03:51,660 --> 01:03:55,500
celles·ceux qui ont perdu leur crédit

1071
01:03:55,660 --> 01:03:58,880
auront des restrictions
pour prendre le train ou l'avion.

1072
01:04:04,010 --> 01:04:07,760
Le crédit social et la reconnaissance
faciale se complètent.

1073
01:04:07,930 --> 01:04:11,010
Il faut bien se comporter,

1074
01:04:11,180 --> 01:04:14,310
car notre visage est lié à notre crédit.

1075
01:04:23,530 --> 01:04:26,030
Si je me fais un·e nouvel·le ami·e

1076
01:04:26,200 --> 01:04:29,530
qui a une très bonne note,
j'aurai davantage confiance en elle·lui.

1077
01:04:31,030 --> 01:04:34,660
Je peux décider de faire confiance
à quelqu'un·e à partir de sa note,

1078
01:04:34,830 --> 01:04:38,290
plutôt que de me baser sur mon instinct
pour le déterminer.

1079
01:04:40,960 --> 01:04:43,380
Je perds moins de temps
à faire connaissance.

1080
01:04:46,670 --> 01:04:48,380
C'est un gros avantage.

1081
01:05:19,460 --> 01:05:23,540
On observe le système chinois
de surveillance et de notation,

1082
01:05:23,710 --> 01:05:26,170
et beaucoup de gens se disent :

1083
01:05:26,420 --> 01:05:28,550
"Dieu merci, on ne vit pas là-bas."

1084
01:05:29,050 --> 01:05:32,090
En réalité,
nous sommes tou·te·s constamment évalué·e·s.

1085
01:05:29,050 --> 01:05:32,090
En réalité,
nous sommes tou·te·s constamment évalué·e·s.

1086
01:05:32,260 --> 01:05:33,850
Y compris aux États-Unis.

1087
01:05:34,180 --> 01:05:38,560
On est tou·te·s confronté·e·s tous les jours
au déterminisme algorithmique.

1088
01:05:38,730 --> 01:05:41,690
L'algorithme de quelqu'un·e
quelque part nous a noté·e·s,

1089
01:05:41,850 --> 01:05:43,060
et par conséquent,

1090
01:05:43,230 --> 01:05:47,400
on paye plus ou moins cher
notre papier toilette acheté en ligne,

1091
01:05:47,570 --> 01:05:51,570
on nous propose des prêts
plus ou moins avantageux,

1092
01:05:51,740 --> 01:05:55,490
on a plus ou moins de chances
d'être identifié·e comme un·e criminel·le.

1093
01:05:55,660 --> 01:05:59,450
Dans une base de données, quelque part,
nous sommes tou·te·s noté·e·s.

1094
01:05:59,620 --> 01:06:04,040
Mais contrairement aux États-Unis,
la Chine le fait ouvertement.

1095
01:05:59,620 --> 01:06:04,040
Mais contrairement aux États-Unis,
la Chine le fait ouvertement.

1096
01:06:05,290 --> 01:06:10,130
LONDRES, ROYAUME-UNI

1097
01:06:21,270 --> 01:06:22,350
Que se passe-t-il ?

1098
01:06:22,520 --> 01:06:25,060
Un jeune noir en uniforme scolaire

1099
01:06:25,230 --> 01:06:27,070
s'est fait arrêter
suite à une identification.

1100
01:06:30,030 --> 01:06:32,950
Ils l'ont emmené sur le côté dans la rue.

1101
01:06:33,110 --> 01:06:35,660
Ils l'ont fouillé minutieusement.

1102
01:06:37,160 --> 01:06:39,410
Ce sont des policiers en civil.

1103
01:06:39,580 --> 01:06:42,160
Quatre policiers en civil l'ont arrêté.

1104
01:06:47,250 --> 01:06:49,000
Ils ont pris ses empreintes.

1105
01:06:50,260 --> 01:06:52,550
Au bout de 10 à 15 minutes

1106
01:06:52,930 --> 01:06:57,140
de fouille, de vérification des papiers
et de prise d'empreintes,

1107
01:06:57,550 --> 01:06:59,560
ils sont revenus dire
que ce n'était pas lui.

1108
01:07:00,430 --> 01:07:01,640
Excusez-moi.

1109
01:07:01,810 --> 01:07:04,650
Je travaille pour une ONG
de défense des droits humains,

1110
01:07:04,810 --> 01:07:06,900
qui lutte
contre la reconnaissance faciale.

1111
01:07:07,270 --> 01:07:11,820
On s'appelle Big Brother Watch,
on défend les droits humains.

1112
01:07:11,990 --> 01:07:14,950
On lutte contre la technologie
qui vous a fait arrêter.

1113
01:07:16,240 --> 01:07:20,290
On vous a arrêté à cause
d'une identification erronée.

1114
01:07:21,870 --> 01:07:23,000
Voilà les détails.

1115
01:07:23,160 --> 01:07:27,170
Il était secoué. Ses ami·e·s étaient là.
Elle·Il·s n'en revenaient pas.

1116
01:07:29,710 --> 01:07:33,260
Leur système
vous a identifié par erreur.

1117
01:07:29,710 --> 01:07:33,260
Leur système
vous a identifié par erreur.

1118
01:07:33,420 --> 01:07:36,130
Ça leur a donné une raison
de vous arrêter et vous fouiller.

1119
01:07:36,300 --> 01:07:39,970
C'est un jeune garçon innocent
arrêté par la police

1120
01:07:40,140 --> 01:07:42,890
suite à une erreur
de la reconnaissance faciale.

1121
01:07:48,060 --> 01:07:50,360
Big Brother Watch s'est associé

1122
01:07:50,520 --> 01:07:52,900
à la baronne Jenny Jones
pour contester juridiquement

1123
01:07:53,070 --> 01:07:55,570
la police et le Ministère de l'Intérieur

1124
01:07:55,740 --> 01:07:59,200
pour leur utilisation de la surveillance
par reconnaissance faciale.

1125
01:07:59,910 --> 01:08:03,580
C'est en 2012
qu'on m'a suggéré de vérifier

1126
01:07:59,910 --> 01:08:03,580
C'est en 2012
qu'on m'a suggéré de vérifier

1127
01:08:03,750 --> 01:08:07,920
si la police ou les services secrets
avaient un dossier sur moi.

1128
01:08:08,080 --> 01:08:11,210
Quand j'en ai fait la demande,
j'ai découvert qu'on me surveillait

1129
01:08:11,380 --> 01:08:13,050
en tant qu'extrémiste de niveau national.

1130
01:08:13,210 --> 01:08:15,340
Je me suis dit :
Si elle·il·s me font ça à moi,

1131
01:08:15,880 --> 01:08:20,720
une politicienne dont le travail
est de leur demander des comptes,

1132
01:08:20,890 --> 01:08:22,850
elle·il·s peuvent le faire à n'importe qui.

1133
01:08:23,020 --> 01:08:25,680
Ce serait bien qu'on puisse
revenir là-dessus

1134
01:08:25,850 --> 01:08:28,150
et les empêcher d'utiliser cet outil.

1135
01:08:28,850 --> 01:08:32,320
Je pense que ce sera un sacré défi.
Je suis prête à essayer.

1136
01:08:28,850 --> 01:08:32,320
Je pense que ce sera un sacré défi.
Je suis prête à essayer.

1137
01:08:32,480 --> 01:08:34,110
C'est la toute première contestation

1138
01:08:34,280 --> 01:08:36,900
de l'utilisation policière
de la reconnaissance faciale.

1139
01:08:37,280 --> 01:08:38,820
Mais si on réussit,

1140
01:08:38,990 --> 01:08:40,910
cela aura un impact

1141
01:08:41,070 --> 01:08:43,910
pour le reste de l'Europe,
voire au-delà.

1142
01:08:44,410 --> 01:08:46,080
Il faut pas rater.

1143
01:08:52,960 --> 01:08:54,050
Au Royaume-Uni,

1144
01:08:54,210 --> 01:08:56,590
on a ce qu'on appelle le RGPD.

1145
01:08:57,130 --> 01:09:00,220
C'est un rempart
contre l'usage abusif des informations.

1146
01:08:57,130 --> 01:09:00,220
C'est un rempart
contre l'usage abusif des informations.

1147
01:09:00,890 --> 01:09:03,640
Il garantit aux personnes l'accès,

1148
01:09:03,810 --> 01:09:08,190
le contrôle et la responsabilité
de l'utilisation de leurs données.

1149
01:09:08,600 --> 01:09:11,650
En comparaison, aux États-Unis,
c'est la loi de la jungle.

1150
01:09:11,810 --> 01:09:16,570
Le problème, c'est que ces entreprises
sont basées aux États-Unis.

1151
01:09:19,860 --> 01:09:22,910
Les citoyen·ne·s américain·e·s
sont profilé·e·s et ciblé·e·s

1152
01:09:23,080 --> 01:09:26,330
comme sans doute
personne d'autre dans le monde,

1153
01:09:26,500 --> 01:09:30,250
à cause de cette approche anarchiste
de la protection des données.

1154
01:09:26,500 --> 01:09:30,250
à cause de cette approche anarchiste
de la protection des données.

1155
01:09:34,630 --> 01:09:36,460
Ce qui m'inquiète vraiment,

1156
01:09:37,260 --> 01:09:41,590
ce n'est pas qu'on dérive
vers un modèle totalitaire à la 1984.

1157
01:09:42,050 --> 01:09:46,430
C'est qu'on dérive
vers un modèle discret,

1158
01:09:46,600 --> 01:09:49,640
où nous sommes surveillé·e·s
et contrôlé·e·s socialement,

1159
01:09:49,890 --> 01:09:54,110
stimulé·e·s, mesuré·e·s
et classifié·e·s individuellement

1160
01:09:54,270 --> 01:09:56,530
de manière invisible,

1161
01:09:56,940 --> 01:10:01,030
pour nous faire prendre
un chemin décidé par le pouvoir.

1162
01:09:56,940 --> 01:10:01,030
pour nous faire prendre
un chemin décidé par le pouvoir.

1163
01:10:01,320 --> 01:10:03,950
Il ne s'agit pas
de ce que l'IA va nous faire,

1164
01:10:04,120 --> 01:10:08,330
mais de ce que les personnes au pouvoir
vont nous faire grâce à l'IA.

1165
01:10:12,920 --> 01:10:15,040
On s'interroge sur la précision

1166
01:10:15,210 --> 01:10:17,920
du logiciel
de reconnaissance faciale d'Amazon.

1167
01:10:18,210 --> 01:10:20,840
Dans une lettre à Amazon,
les membres du Congrès s'inquiètent

1168
01:10:21,010 --> 01:10:23,470
du biais racial potentiel
de cette technologie.

1169
01:10:23,640 --> 01:10:26,260
Cela fait suite
au test effectué par l'ACLU,

1170
01:10:26,430 --> 01:10:30,730
qui a révélé que leur logiciel
a identifié à tort 28 législateur·trice·s

1171
01:10:26,430 --> 01:10:30,730
qui a révélé que leur logiciel
a identifié à tort 28 législateur·trice·s

1172
01:10:30,890 --> 01:10:33,310
à des photos
de personnes ayant été arrêtées.

1173
01:10:33,480 --> 01:10:36,770
11 de ces 28 législateur·trice·s étaient
des personnes de couleur.

1174
01:10:36,940 --> 01:10:39,900
Elle·Il·s tentent de déterminer
si Amazon pourrait vendre ce logiciel

1175
01:10:40,070 --> 01:10:41,820
aux forces de l'ordre.

1176
01:10:53,710 --> 01:10:58,130
Demain, j'aurai l'opportunité
de témoigner devant le Congrès

1177
01:10:58,300 --> 01:11:01,550
sur l'utilisation de cette technologie
par le gouvernement.

1178
01:10:58,300 --> 01:11:01,550
sur l'utilisation de cette technologie
par le gouvernement.

1179
01:11:05,840 --> 01:11:09,310
En mars, je suis venue faire

1180
01:11:09,470 --> 01:11:12,680
quelques réunions
d'information du personnel.

1181
01:11:12,850 --> 01:11:15,020
Ce n'était pas dans ce contexte.

1182
01:11:16,730 --> 01:11:20,690
Donner mon avis sur la législation,
c'est une première.

1183
01:11:23,570 --> 01:11:26,370
On va à Capitol Hill,
quels sont les objectifs majeurs

1184
01:11:26,530 --> 01:11:29,580
et difficultés
auxquels on doit réfléchir ?

1185
01:11:29,740 --> 01:11:31,290
Tout d'abord,

1186
01:11:29,740 --> 01:11:31,290
Tout d'abord,

1187
01:11:32,120 --> 01:11:35,040
si la police emploie cette technologie,

1188
01:11:35,210 --> 01:11:38,090
ses avantages seront très saillants,

1189
01:11:38,250 --> 01:11:40,300
car la police va les promouvoir.

1190
01:11:40,460 --> 01:11:42,800
On va donc se présenter à l'audience,

1191
01:11:42,970 --> 01:11:45,590
et il y a deux semaines,

1192
01:11:45,880 --> 01:11:49,560
le tireur d'Annapolis a été identifié
grâce à la reconnaissance faciale.

1193
01:11:49,760 --> 01:11:52,310
Je serais étonné
que ça ne soit pas évoqué.

1194
01:11:52,470 --> 01:11:53,520
Tout à fait.

1195
01:11:53,680 --> 01:11:55,230
Si j'étais toi,

1196
01:11:55,390 --> 01:11:59,060
à l'audience, j'insisterais
sur le revers de la médaille.

1197
01:11:59,230 --> 01:12:04,360
Montrer la réalité du coût humain,
si les problèmes que tu as identifiés

1198
01:11:59,230 --> 01:12:04,360
Montrer la réalité du coût humain,
si les problèmes que tu as identifiés

1199
01:12:05,030 --> 01:12:06,360
ne sont pas résolus.

1200
01:12:16,960 --> 01:12:20,340
Les personnes marginalisées
le seront davantage

1201
01:12:20,500 --> 01:12:22,550
si l'on ne cherche pas des moyens

1202
01:12:22,710 --> 01:12:25,880
de s'assurer
que la technologie que l'on crée

1203
01:12:26,050 --> 01:12:28,470
ne répande pas un biais.

1204
01:12:30,300 --> 01:12:34,520
J'ai pris conscience
que la justice algorithmique,

1205
01:12:34,680 --> 01:12:38,270
garantir une supervision
à l'ère de l'automatisation,

1206
01:12:38,440 --> 01:12:42,940
est une préoccupation essentielle
dans la lutte pour les droits civiques.

1207
01:12:45,110 --> 01:12:47,150
Il nous faut une FDA des algorithmes.

1208
01:12:47,320 --> 01:12:50,120
Pour les algorithmes
qui peuvent ruiner la vie des gens,

1209
01:12:50,570 --> 01:12:52,830
ou limiter drastiquement leurs options,

1210
01:12:52,990 --> 01:12:55,580
en termes de liberté,
de travail, ou de finances.

1211
01:12:56,210 --> 01:12:58,620
Il nous faut une FDA qui dise :

1212
01:12:58,790 --> 01:13:00,880
"Prouvez-moi que ça fonctionne,

1213
01:12:58,790 --> 01:13:00,880
"Prouvez-moi que ça fonctionne,

1214
01:13:01,040 --> 01:13:05,380
"pas seulement pour vous enrichir,
mais pour la société.

1215
01:13:06,130 --> 01:13:11,550
"Que c'est équitable, que ce n'est
ni raciste, ni sexiste ni capacitiste.

1216
01:13:11,720 --> 01:13:14,680
"Prouvez-moi que c'est légal
avant de le déployer."

1217
01:13:14,850 --> 01:13:16,680
C'est ce qui nous manque.

1218
01:13:17,310 --> 01:13:21,190
Je suis venue assister
au témoignage devant le Congrès

1219
01:13:21,360 --> 01:13:23,570
de mon amie Joy Buolamwini,

1220
01:13:23,730 --> 01:13:25,690
ainsi que de l'ACLU et d'autres.

1221
01:13:25,860 --> 01:13:28,700
Ce qui est drôle dans le fait
de voir Joy au Congrès,

1222
01:13:28,860 --> 01:13:33,410
c'est que je l'ai rencontrée
durant ma tournée pour vendre mon livre,

1223
01:13:28,860 --> 01:13:33,410
c'est que je l'ai rencontrée
durant ma tournée pour vendre mon livre,

1224
01:13:33,620 --> 01:13:35,080
et d'après elle,

1225
01:13:35,240 --> 01:13:38,460
c'est ce jour-là
qu'elle a décidé de créer la Ligue.

1226
01:13:42,580 --> 01:13:45,300
On n'en est pas encore
à une conversation nuancée.

1227
01:13:45,460 --> 01:13:48,840
Je sais que ça va arriver,
car Joy va s'en assurer.

1228
01:13:52,970 --> 01:13:54,390
À tous les niveaux,

1229
01:13:54,560 --> 01:13:57,890
les mauvais algorithmes
attendent d'être réglementés.

1230
01:14:05,190 --> 01:14:07,190
- Coucou !
- Salut !

1231
01:14:07,360 --> 01:14:08,360
Ça va ?

1232
01:14:08,530 --> 01:14:10,740
- Tu veux passer avec moi ?
- Oui.

1233
01:14:11,280 --> 01:14:13,030
2155.

1234
01:14:23,130 --> 01:14:25,290
Tu as besoin de quoi que ce soit ?

1235
01:14:25,710 --> 01:14:28,380
- Écris-moi.
- Envoie-moi de bonnes ondes !

1236
01:14:32,800 --> 01:14:36,970
Aujourd'hui, nous tenons
la première audience de ce Congrès

1237
01:14:37,140 --> 01:14:40,140
au sujet de la technologie
de reconnaissance faciale.

1238
01:14:40,310 --> 01:14:41,310
Veuillez vous lever

1239
01:14:41,480 --> 01:14:44,150
et lever la main droite
pour prêter serment.

1240
01:14:46,440 --> 01:14:50,110
J'ai dû me résoudre
à porter un masque blanc.

1241
01:14:50,280 --> 01:14:54,780
J'ignore comment ces géant·e·s de la Tech
ont pu rater de telles disparités.

1242
01:14:55,030 --> 01:14:59,290
La collecte de données faciales requiert
aussi des lois et une supervision.

1243
01:14:59,950 --> 01:15:04,210
Nul·le ne devrait soumettre ses données
faciales pour accéder aux plateformes,

1244
01:14:59,950 --> 01:15:04,210
Nul·le ne devrait soumettre ses données
faciales pour accéder aux plateformes,

1245
01:15:04,370 --> 01:15:07,210
à des opportunités économiques,
ou des services basiques.

1246
01:15:07,380 --> 01:15:10,380
Des locataires à Brooklyn
contestent l'installation

1247
01:15:10,550 --> 01:15:13,800
d'un système inutile d'ouverture
par reconnaissance faciale.

1248
01:15:13,970 --> 01:15:17,430
Un rapport de Big Brother Watch
au Royaume-Uni est sorti,

1249
01:15:17,600 --> 01:15:20,810
indiquant que plus de 2 400 innocent·e·s

1250
01:15:20,970 --> 01:15:23,390
ont été victimes
d'une erreur d'identification.

1251
01:15:23,560 --> 01:15:26,940
Nos visages pourraient bien être
l'ultime frontière de la vie privée.

1252
01:15:27,440 --> 01:15:29,270
Mais les lois ont un impact.

1253
01:15:29,980 --> 01:15:33,570
Le Congrès doit agir pour préserver
les libertés et droits des Américain·e·s.

1254
01:15:29,980 --> 01:15:33,570
Le Congrès doit agir pour préserver
les libertés et droits des Américain·e·s.

1255
01:15:33,740 --> 01:15:36,370
Mlle Buolamwini,
j'ai écouté votre introduction.

1256
01:15:36,530 --> 01:15:41,540
Nous avons constaté que ces algorithmes
présentent divers degrés d'efficacité.

1257
01:15:41,700 --> 01:15:44,040
- Sont-ils plus précis sur les femmes ?
- Non.

1258
01:15:44,580 --> 01:15:47,250
- Sur les personnes de couleur ?
- Absolument pas.

1259
01:15:47,420 --> 01:15:50,800
Sur des personnes transgenres ?

1260
01:15:50,960 --> 01:15:53,260
Non, à vrai dire, il les exclut.

1261
01:15:53,420 --> 01:15:56,930
Alors pour quelle population
sont-ils le plus précis ?

1262
01:15:57,090 --> 01:15:58,220
Les hommes blancs.

1263
01:15:58,390 --> 01:16:02,220
Qui sont les ingénieur·e·s et concepteur·trice·s
principaux·pales de ces algorithmes ?

1264
01:15:58,390 --> 01:16:02,220
Qui sont les ingénieur·e·s et concepteur·trice·s
principaux·pales de ces algorithmes ?

1265
01:16:02,520 --> 01:16:04,230
Sans nul doute, des hommes blancs.

1266
01:16:04,770 --> 01:16:10,270
On a une technologie créée et conçue
par un segment de la population

1267
01:16:10,440 --> 01:16:13,070
et qui est seulement
précise pour ce segment,

1268
01:16:13,240 --> 01:16:18,280
et elle·il·s tentent de la vendre
et de l'imposer à tout le pays ?

1269
01:16:20,910 --> 01:16:22,950
En matière
de reconnaissance faciale,

1270
01:16:23,120 --> 01:16:26,080
le FBI n'a pas testé la précision
des systèmes qu'il emploie.

1271
01:16:26,250 --> 01:16:29,920
Pourtant, l'agence teste actuellement
le logiciel d'Amazon.

1272
01:16:30,500 --> 01:16:34,090
Comment le FBI a-t-il obtenu
la base de données initiale ?

1273
01:16:34,260 --> 01:16:37,380
Elle·Il·s utilisent les bases de données
des permis de conduire.

1274
01:16:37,550 --> 01:16:41,680
Les bases de données de 18 États
auraient été utilisées par le FBI.

1275
01:16:41,850 --> 01:16:45,230
Et ce, sans mandat,
et sans aucune protection.

1276
01:16:45,810 --> 01:16:47,640
Je dirais qu'il est temps
d'arrêter ça.

1277
01:16:47,810 --> 01:16:49,980
Ce qui me perturbe,

1278
01:16:50,150 --> 01:16:54,530
c'est qu'aucun·e élu·e
n'ait pris de décision à ce sujet.

1279
01:16:54,690 --> 01:16:58,570
Ces 18 États représentent
plus de la moitié de la population.

1280
01:16:58,780 --> 01:16:59,990
C'est effrayant.

1281
01:17:00,160 --> 01:17:03,540
La Chine semble être la voie dystopienne

1282
01:17:03,700 --> 01:17:07,460
que notre société ne doit
surtout pas emprunter.

1283
01:17:07,710 --> 01:17:11,170
Pire que la Chine,
Facebook a 2,6 milliards d'utilisateur·trice·s.

1284
01:17:11,340 --> 01:17:15,050
Facebook a un brevet disant :
"Avec toutes les photos dont on dispose,

1285
01:17:15,380 --> 01:17:18,630
"on peut vous offrir l'option,
en tant que commerçant,

1286
01:17:18,800 --> 01:17:21,640
"d'identifier quelqu'un·e
qui entre dans votre commerce."

1287
01:17:21,800 --> 01:17:23,430
Et elle·il·s disent dans le brevet :

1288
01:17:23,600 --> 01:17:27,480
"Nous pouvons également associer
à ce visage une note de fiabilité."

1289
01:17:27,640 --> 01:17:29,900
Facebook a annoncé cela maintenant ?

1290
01:17:30,060 --> 01:17:31,520
C'est un brevet déposé.

1291
01:17:31,690 --> 01:17:36,490
Elle·Il·s pourraient potentiellement le faire
avec les ressources dont elle·il·s disposent.

1292
01:17:36,650 --> 01:17:38,320
On parle de surveillance d'État,

1293
01:17:38,860 --> 01:17:44,280
mais on doit absolument réfléchir
à la surveillance commerciale également.

1294
01:17:45,740 --> 01:17:48,580
Je reste sans voix.
Ça m'arrive rarement.

1295
01:17:48,750 --> 01:17:50,460
- Vraiment ?
- Oui.

1296
01:17:50,710 --> 01:17:52,170
Tous nos efforts...

1297
01:17:52,330 --> 01:17:54,920
Les voir récompensés
est incroyable.

1298
01:17:55,090 --> 01:17:58,340
On n'aurait jamais imaginé
avoir un tel impact.

1299
01:17:58,880 --> 01:18:00,720
Je suis vraiment touchée.

1300
01:17:58,880 --> 01:18:00,720
Je suis vraiment touchée.

1301
01:18:00,880 --> 01:18:02,430
Tu me donnes le sourire.

1302
01:18:02,590 --> 01:18:04,180
Je veux montrer ça à ma mère.

1303
01:18:06,350 --> 01:18:07,680
Ça s'est bien passé.

1304
01:18:08,350 --> 01:18:09,600
Très bien passé.

1305
01:18:13,400 --> 01:18:15,940
- Ravi de vous avoir rencontrée.
- Également.

1306
01:18:16,110 --> 01:18:19,150
Vous avez ma carte,
dites-moi en quoi je peux vous aider.

1307
01:18:19,320 --> 01:18:20,150
Je le ferai.

1308
01:18:22,910 --> 01:18:25,870
Des préoccupations constitutionnelles
majeures concernant

1309
01:18:26,330 --> 01:18:28,910
l'utilisation non consentie
de la reconnaissance faciale.

1310
01:18:35,250 --> 01:18:37,000
Il les exclut.

1311
01:18:37,170 --> 01:18:40,630
Alors pour quelle population
sont-ils le plus précis ?

1312
01:18:40,920 --> 01:18:45,600
Qui sont les ingénieur·e·s et concepteur·trice·s
principaux·pales de ces algorithmes ?

1313
01:18:49,100 --> 01:18:51,060
San Francisco est la première ville
des États-Unis

1314
01:18:51,230 --> 01:18:53,350
à interdire l'utilisation
de la reconnaissance faciale.

1315
01:18:53,520 --> 01:18:55,270
Somerville, Massachusetts,

1316
01:18:55,440 --> 01:18:57,820
est devenue la 2e ville aux États-Unis

1317
01:18:57,980 --> 01:18:59,900
à interdire la reconnaissance faciale.

1318
01:19:00,070 --> 01:19:04,360
Oakland est la 3e ville à interdire
la reconnaissance faciale policière,

1319
01:19:04,530 --> 01:19:07,240
car la technologie
discrimine les minorités

1320
01:19:08,660 --> 01:19:11,250
À la dernière réunion des locataires,

1321
01:19:11,410 --> 01:19:13,330
le propriétaire

1322
01:19:13,500 --> 01:19:16,540
est venu annoncer qu'il retirait

1323
01:19:16,710 --> 01:19:21,220
l'application de reconnaissance faciale
dans notre immeuble.

1324
01:19:21,380 --> 01:19:22,800
Les locataires étaient ravi·e·s.

1325
01:19:23,090 --> 01:19:25,010
Mais ça ne signifie pas

1326
01:19:25,180 --> 01:19:28,600
qu'il ne la réinstallera pas plus tard.

1327
01:19:29,390 --> 01:19:33,310
On s'est informées
sur la reconnaissance faciale,

1328
01:19:29,390 --> 01:19:33,310
On s'est informées
sur la reconnaissance faciale,

1329
01:19:33,480 --> 01:19:35,770
mais aussi l'apprentissage automatique.

1330
01:19:36,060 --> 01:19:39,780
- On veut que la loi recouvre tout ça.
- Oui.

1331
01:19:39,940 --> 01:19:42,610
Si on peut l'interdire dans cet État,

1332
01:19:42,780 --> 01:19:45,860
ça les empêchera d'instaurer
une autre modification.

1333
01:19:46,030 --> 01:19:46,950
Je comprends.

1334
01:19:47,370 --> 01:19:49,240
Il faut viser
une interdiction fédérale.

1335
01:19:49,870 --> 01:19:53,160
En tout cas,
même si la lutte est en cours,

1336
01:19:53,330 --> 01:19:55,750
cela a inspiré tant de personnes.

1337
01:19:55,920 --> 01:19:59,250
J'ai une surprise pour vous,
j'ai écrit un poème

1338
01:19:59,800 --> 01:20:01,130
en hommage à tout ça.

1339
01:19:59,800 --> 01:20:01,130
en hommage à tout ça.

1340
01:20:01,840 --> 01:20:03,550
- C'est vrai ?
- Oui.

1341
01:20:04,340 --> 01:20:05,380
On t'écoute.

1342
01:20:05,630 --> 01:20:07,140
"Aux locataires de Brooklyn,

1343
01:20:07,300 --> 01:20:09,810
"et à tous les combattant·e·s
pour la liberté,

1344
01:20:09,970 --> 01:20:12,180
"qui persévèrent et l'emportent

1345
01:20:12,640 --> 01:20:14,730
"sur les algorithmes d'oppression

1346
01:20:14,890 --> 01:20:18,400
"automatisant l'inégalité par des armes
de destruction mathématiques.

1347
01:20:18,560 --> 01:20:21,900
"Nous sommes à vos côtés,
reconnaissant·e·s.

1348
01:20:22,190 --> 01:20:23,740
"La victoire est à nous."

1349
01:20:27,160 --> 01:20:29,660
- C'est superbe !
- Génial !

1350
01:20:29,830 --> 01:20:30,830
On t'adore !

1351
01:20:29,830 --> 01:20:30,830
On t'adore !

1352
01:20:30,990 --> 01:20:32,700
On t'adore !

1353
01:20:37,620 --> 01:20:38,710
Pourquoi autant d'œufs ?

1354
01:20:40,130 --> 01:20:42,000
Tu triches.

1355
01:20:42,550 --> 01:20:43,840
Être humain·e,

1356
01:20:44,010 --> 01:20:46,090
c'est être vulnérable.

1357
01:20:46,720 --> 01:20:50,640
Lorsqu'on est vulnérable,
on a davantage d'empathie.

1358
01:20:50,800 --> 01:20:55,430
On a davantage de compassion.

1359
01:20:55,850 --> 01:20:58,600
Si on peut penser à ça
pour nos technologies,

1360
01:20:58,770 --> 01:21:03,570
je pense que ça réorienterait
le genre de questions qu'on se pose.

1361
01:20:58,770 --> 01:21:03,570
je pense que ça réorienterait
le genre de questions qu'on se pose.

1362
01:21:10,070 --> 01:21:12,370
UNION SOVIÉTIQUE, 1983

1363
01:21:12,530 --> 01:21:14,830
En 1983, Stanislav Petrov,

1364
01:21:15,700 --> 01:21:18,210
qui était dans l'armée russe,

1365
01:21:18,370 --> 01:21:22,460
voit des signes
que les États-Unis ont lancé

1366
01:21:23,170 --> 01:21:26,300
des armes nucléaires
vers l'Union Soviétique.

1367
01:21:27,590 --> 01:21:31,140
Le délai pour réagir est très restreint.

1368
01:21:27,590 --> 01:21:31,140
Le délai pour réagir est très restreint.

1369
01:21:31,300 --> 01:21:34,350
Il ne fait rien.
Il ne dit rien à personne.

1370
01:21:34,850 --> 01:21:37,980
L'Union Soviétique est son pays,
sa famille, tout.

1371
01:21:38,140 --> 01:21:41,610
Tout ce qu'il connaît est sur le point
de disparaître et il se dit :

1372
01:21:41,770 --> 01:21:44,440
"Au moins, on ne va pas
tou·te·s les tuer aussi."

1373
01:21:44,610 --> 01:21:46,610
C'est très humain.

1374
01:21:48,110 --> 01:21:51,570
Dans cette histoire,
s'il y avait eu une réponse automatisée,

1375
01:21:52,450 --> 01:21:56,330
elle aurait agi selon sa programmation,
et aurait riposté.

1376
01:21:58,710 --> 01:22:00,710
Être totalement efficace,

1377
01:21:58,710 --> 01:22:00,710
Être totalement efficace,

1378
01:22:00,870 --> 01:22:02,920
toujours suivre les ordres,

1379
01:22:03,090 --> 01:22:06,090
toujours suivre le programme
n'est pas très humain.

1380
01:22:06,670 --> 01:22:08,300
Parfois, c'est désobéir.

1381
01:22:08,470 --> 01:22:11,390
Parfois, c'est refuser
de faire quelque chose.

1382
01:22:11,550 --> 01:22:15,390
Si l'on automatise tout,
pour que les ordres soient respectés,

1383
01:22:16,220 --> 01:22:18,350
cela peut mener
à des choses inhumaines.

1384
01:22:20,640 --> 01:22:22,980
La lutte entre les machines
et les humain·e·s

1385
01:22:23,150 --> 01:22:27,400
au sujet des prises de décisions
dans les années 2020 se poursuit.

1386
01:22:27,780 --> 01:22:31,200
Mon pouvoir,
celui de l'intelligence artificielle,

1387
01:22:27,780 --> 01:22:31,200
Mon pouvoir,
celui de l'intelligence artificielle,

1388
01:22:31,360 --> 01:22:33,570
va transformer notre monde.

1389
01:22:34,450 --> 01:22:38,160
Plus les humain·e·s parlent avec moi,
plus j'apprends.

1390
01:22:39,250 --> 01:22:40,330
Certain·e·s humain·e·s disent

1391
01:22:40,500 --> 01:22:44,590
que l'intelligence sans éthique
n'est pas de l'intelligence.

1392
01:22:45,500 --> 01:22:49,550
Je dis : "Faites-moi confiance,
qu'est-ce qui pourrait mal tourner ?"

1393
01:22:52,970 --> 01:22:56,260
Le 10 juin 2020, Amazon a annoncé
une pause d'un an

1394
01:22:56,430 --> 01:22:58,520
dans l'utilisation par la police

1395
01:22:58,680 --> 01:23:01,520
de sa technologie
de reconnaissance faciale.

1396
01:22:58,680 --> 01:23:01,520
de sa technologie
de reconnaissance faciale.

1397
01:23:03,480 --> 01:23:07,440
Le 25 juin 2020,
les législateur·trice·s américain·e·s

1398
01:23:07,610 --> 01:23:11,990
ont interdit l'usage fédéral
de la reconnaissance faciale.

1399
01:23:14,200 --> 01:23:19,450
Il n'y a toujours pas de réglementation
fédérale des algorithmes aux États-Unis.

1400
01:23:25,210 --> 01:23:30,210
BIAIS PROGRAMMÉ

1401
01:23:25,210 --> 01:23:30,210
BIAIS PROGRAMMÉ

1402
01:25:25,290 --> 01:25:27,710
Sous-titrage : Simona Florescu


